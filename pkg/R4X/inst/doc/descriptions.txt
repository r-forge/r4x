This package contains five functions. read.FASTA reads in a
FASTA-format alignment file and parses it into a data frame. read.CX reads
in a ClustalX .aln-format file and parses it into a data frame. read.Gdoc
reads in a GeneDoc .msf-format file and parses it into a data frame. The
alignment data frame returned by each of these functions has the sequence
IDs as the row names and each site in the alignment is a column in the data
frame. The program aaMI calculates the mutual information between each pair
of sites (columns) in the protein sequence alignment data frame. The
program aaMIn calculates the normalized mutual information between pairs of
sites in the protein sequence alignment data frame. The normalized mutual
information of sites i and j is the mutual information of these sites
divided by their joint entropy.

Combine multi-dimensional arrays.  This is a
generalization of cbind and rbind.  Takes a sequence of
vectors, matrices, or arrays and produces a single array of
the same or higher dimension.

This package provides functionality for creating and
evaluating acceptance sampling plans. Plans can be single,
double or multiple sampling plans

This is a suite of tools designed to test and improve the accuracy of
statistical computation, including: Summarization of the sensitivity of linear
and non-linear models (lm, glm, mle, nls) to
measurement and numerical error; Sensitivity analysis of dozens of models
as run through Zelig;  A generalized cholesky method for
correcting non-invertable Hessians; Tests for
the global optimality of non-linear regression and maximum likelihood results;
Tools for obtaining true random numbers using entropy collected from the system
and/or entropy servers on the internet;
A method for converting floating point numbers to normalized fractions;
Benchmark data for checking the accuracy of basic distribution functions.

ACE and AVAS methods for choosing regression transformations.

Collection of functions and data sets related to
actuarial science applications, mostly loss distributions, risk
theory (including ruin theory), simulation of compound hierarchical
models and credibility theory, for the moment.

This package implements Freund and Schapire's Adaboost.M1 algorithm and Breiman's Bagging algorithm
using classification trees as individual classifiers. Once these classifiers have been trained,
they can be used to predict on new data. Also, cross validation predictions can be done.

Analysis and plotting of array CGH data. Allows usage of
Circular Binary Segementation, wavelet-based smoothing, ACE
method (CGH Explorer), HMM, BioHMM, GLAD, CGHseg, and  Price's modification of Smith &
Waterman's algorith. Most computations are
parallelized. Figures are imagemaps with links to IDClight
(http://idclight.bioinfo.cnio.es).

Performs discrete, real, and gentle boost under both exponential and
logistic loss on a given data set.  The package ada provides a straightforward,
well-documented, and broad boosting routine for classification, ideally suited
for small to moderate-sized data sets.  Please refer to the Url below for more
information.

Based on the function "spm" of the SemiPar package fits semiparametric regression models with spatially adaptive penalized splines.

Adaptive Quadrature in up to 20 dimensions

Multivariate data analysis and graphical display.

a Tcl/Tk GUI for some basic functions in the ade4 package

Classes and functions for genetic data analysis within the multivariate framework.

A collection of tools for the analysis of habitat selection by animals

This package implements the Propagation Separation approach
by Polzehl and Spokoiny (2006) for smoothing digital images.

The Anderson-Darling K-sample test can be used to test whether several
independent random samples of various sizes come from the same but unspecified
continuous distribution. It is a rank test and consistent against all alternatives.
A low to moderate number of tied observations can be tolerated. The combination of
such tests can be used to test whether M groups of samples (with K allowed to vary
from group to group) come from respective common distributions, which may vary from
group to group. This is useful in testing for treatment effects in randomized
(incomplete) block designs or in examining whether several laboratories perform
equally well when asked to measure a sufficient number of test speciments
from different batches or materials.

Adaptive Wavelet transforms for signal denoising

Perform first- and second-order multi-scale analyses derived from Ripley's K-function, for univariate,
multivariate and marked mapped data in rectangular, circular or irregular shaped sampling windows, with test of
statitical significance based on Monte Carlo simulations.

This package contains some simple functions for the analysis of growth curve experiments.

These functions are currently utilized by the International Potato Center Research (CIP), the Statistics and Informatics Instructors and the Students of the Universidad Nacional Agraria La Molina Peru, and the Specialized Master in "Bosques y Gestion de Recursos Forestales" (Forest Resource Management). This package contains functionality for the statistical analysis of experimental designs applied specially for field experiments in  agriculture and plant breeding. Planning of field  experiments: Lattice, factorial, RCBD, CRD, Latin Square, Greaco, BIB, PBIB, Alpha design. Comparison of multi-location trials: AMMI (biplot and triplot), Stability. Comparison  between treatments: LSD, Bonferroni, HSD, Waller, Kruskal, Friedman, Durbin, Van Der Waerden. Resampling and simulation: resampling.model, simulation.model, analysis Mother and baby trials, Ecology: Indices Biodiversity, path analysis, consensus cluster, Uniformity Soil: Index Smith's.

High-featured panel functions for bwplot and xyplot,
various plot management helpers, some other utility functions

This provides tools to inspect data

Linear or cubic spline interpolation for irregular gridded data

Algorithmic experimental designs. Calculates exact and approximate
theory experimental designs for D,A, and I criteria. Very large
designs may be created. Experimental designs may be blocked or blocked
designs created from a candidate list, using several criteria.
The blocking can be done when whole and within plot factors
interact.

This is the implementation in R+C of a new association
test described in "A fast, unbiased and exact allelic exact test
for case-control association studies" (Submitted).
It appears that in most cases the classical chi-square test used
for testing for allelic association on genotype data is biased.
Our test is unbiased, exact but fast throught careful optimization.

This library is a companion to the textbook S. Weisberg (2005),
"Applied Linear Regression," 3rd edition, Wiley. It includes all the
data sets discussed in the book (except one), and several few functions that
are tailored to the methods discussed in the book.  Ver. 1.0.3 corrects bugs in (1) weights for POD models; (2) use of 'subset' arguments with boot.case.  Ver. 1.0.4 corrects a few bugs with overparameterized models. Ver. 1.0.5 changes one default argument. Ver 1.0.6 replaces the pure.error.anova function so it prints nicer and works with interactions. Ver 1.0.7 fixes a bug in residual.plots and
adds an additional argument and adds a new function alrweb to access the
website for the book in a browser. Ver 1.0.8 fixed a bug in delta.method for
generalized linear models. Ver 1.0.9 fixes a bug with Yeo Johnson
transformations. Ver 1.1.0 drops the conf.intervals function in favor of
the confint function in stat, includes several minor bug fixes, and makes
the package fully compatible with version 2.6.0 of R. 1.1.2 corrects a bug
introduced in boot.case.

Tools for Clustering and Principal
Component Analysis (With robusts methods, and
parallelized functions).

Amelia II "multiply imputes" missing data in a single cross-section
(such as a survey), from a time series (like variables collected for
each year in a country), or from a time-series-cross-sectional data
set (such as collected by years for each of several countries).
Amelia II implements our bootstrapping-based algorithm that gives
essentially the same answers as the standard IP or EMis approaches,
is usually considerably faster than existing approaches and can
handle many more variables.  Unlike Amelia I and other statistically
rigorous imputation software, it virtually never crashes (but please
let us know if you find to the contrary!).  The program also
generalizes existing approaches by allowing for trends in time series
across observations within a cross-sectional unit, as well as priors
that allow experts to incorporate beliefs they have about the values
of missing cells in their data.  Amelia II also includes useful
diagnostics of the fit of multiple imputation models.  The program
works from the R command line or via a graphical user interface that
does not require users to know R.

This package was born to release the TAO robust neural network algorithm to the R users. It has grown and I think it can be of interest for the users wanting to implement their own training algorithms as well as for those others whose needs lye only in the "user space".

This package performs simple correspondence analysis (CA) on a two-way frequency table (with missings) by means of SVD. Different scaling methods (standard, centroid, Benzecri, Goodman) as well as various plots including confidence ellipsoids are provided.

Fits Modern Analogue Technique transfer function models for prediction of environmental data from
species data. Also performs analogue matching, a related technique used in palaeo ecological restoration.

Functions for I/O, visualisation and analysis of functional Magnetic Resonance Imaging (fMRI) datasets stored in the ANALYZE format.

This package consists of various functions for animations in statistics, covering many areas such as probability theory, mathematical statistics, multivariate statistics, nonparamatric statstics, sampling survey, linear models, time series, computational statistics, data mining and machine learning. These functions might be of help in teaching statistics and data analysis.

The package contains an analog model for statistical/empirical downscaling.

This package provides a set of functions to analyse overdispersed counts or proportions. Most of the methods are already available elsewhere but are scattered in different packages. The proposed functions should be considered as complements to more sophisticated methods such as generalized estimating equations (GEE) or generalized linear mixed effect models (GLMM).

ape provides functions for reading, writing, plotting,
and manipulating phylogenetic trees, analyses of comparative data
in a phylogenetic framework, analyses of diversification and
macroevolution, computing distances from allelic and nucleotide
data, reading nucleotide sequences, and several tools such as
Mantel's test, computation of minimum spanning tree, the population
parameter theta based on various approaches, nucleotide diversity,
generalized skyline plots, estimation of absolute evolutionary rates
and clock-like trees using mean path lengths, non-parametric rate
smoothing and penalized likelihood, classifying genes in trees using
the Klastorin-Misawa-Tajima approach. Phylogeny estimation can be done
with the NJ, BIONJ, ME, and ML methods.

set of functions for drawing some special plots:
stem.leaf plots a stem and leaf plot
bagplot plots a bagplot
faces plots chernoff faces
spin3R for an inspection of a 3-dim point cloud


apTreeshape is mainly dedicated to simulation and analysis of phylogenetic tree topologies using statistical indices. It is a companion library of the 'ape' package. It provides additional functions for reading, plotting, manipulating phylogenetic trees. It also offers convenient web-access to public databases, and enables testing null models of macroevolution using corrected test statistics.  Trees of class "phylo" (from 'ape' package) can be converted easily.

Package ArDec implements the autoregressive decomposition of a time series based on the constructive approach in West (1997). Particular cases include the extraction of trend and seasonal components from a monthly time series. Uncertainty on the resulting components can be derived from sampling of the autoregressive model which is written as a linear regression model and handled on a Bayesian framework.

Generates an allelic richness accumulation curve. This curve shows
the expected number of unique alleles in a population when taking a sample of
individuals. The function aresCalc takes a binary data matrix as input,
showing the presence of alleles per individual, and gives an accumulation
curve (mean with 95\% confidence bounds) back. The function aresPlot can be
used to plot the output from aresCalc.

Functions to filters animal satellite tracking data obtained from Argos. It is especially indicated for telemetry studies of marine animals, where Argos locations are predominantly of low-quality.

R functions for processing lm, glm, mer and polr outputs.

Missing imputation for microarray data

This package is designed to explore missing pattern for microarray data

Adaptive Rejection Sampling, Original version

Provides the infrastructure for representing,
manipulating and analyzing transaction data and patterns (frequent
itemsets and association rules). Also provides interfaces to
C implementations of the association mining algorithms Apriori and Eclat
by C. Borgelt.

Add-on for arules to handle and mine frequent sequences.
Provides interfaces to the C++ implementation of cSPADE by
Mohammed J. Zaki.

David Scotts ASH routines

A collection of functions for computing centrographic satistics (e.g., standard distance, standard deviation ellipse), and minimum convex polygons (MCP)for observations taken at point locations.  A tool is also provided for converting geometric objects associated with the centrographic statistics, and MCPs into ESRI Shapefiles.

ASSIST, see manual

functions and datasets for Aster modeling (forest graph
exponential family conditional or unconditional canonical statistic models
for life history analysis)

Functions and data sets for a lecture in Advanced Statistics using R. Especially the functions mancontr() and inspect() may be of general interest. With mancontr(), short for manual contrasts, it is possible to specify your own contrasts and give them useful names. Something that is important in most projects of reasonable size. The function inspect() shows a wide range of inspection plots to validate model assumptions (currently for models fitted with lm, glm, and lmer). And do not forget to have a look at norm.test(), it is not only fun!

A set of routines written in the S language
that calculate power and related quantities utilizing asymptotic
likelihood ratio methods.

The library contains R-functions to perform the
adaptive weights smoothing (AWS) procedure described in
Polzehl und Spokoiny (2000), Adaptive weights smoothing with
applications to image restoration. Journal of the Royal Statistical
Society, Ser. B,  62, 2, 335--354
and its generalizations to local polynomial models as
described in Polzehl und Spokoiny (2003), Varying coefficient regression
modeling by adaptive weights smoothing, WIAS-Preprint 818
and to local likelihood models as described in Polzehl und Spokoiny (2002).
Local likelihood modelling by adaptive weights smoothing, WIAS-Preprint 787.
The functions awsuni, awsbi and awstri are superseded by functions aws and laws
and will be removed in later versions.

Bayesian analysis of computer code software.  The
bundle contains routines that evaluate the formulae of Kennedy,
O'Hagan, and Oakley.

The backtest package provides facilities for exploring portfolio-based conjectures about financial instruments (stocks, bonds, swaps, options, et cetera).

This is a package for automated redistricting and heuristic exploration of redistricter revealed preference

A set of functions to apply a (zero-inflated) gamma Poisson (equivalent to a negative binomial), (zero-inflated) simple or independant Poisson, (zero-inflated) lognormal Poisson or (zero-inflated) Weibull Poisson model to a set of count data using JAGS (Just Another Gibbs Sampler).  Returns information on the possible values for mean count, variance and zero inflation present in count data such as faecal egg count data.  Checks for convergence using the Gelman-Rubin statistic.  Likelihoods for the above distributions can also be calculated, and a crude maximum likelihood function is included.  Also allows any user specified model to be run in JAGS from within R, returning the MCMC output as R objects.  Just Another Gibbs Sampler (JAGS) must be installed.  *THIS SOFTWARE IS INTENDED FOR EDUCATIONAL PURPOSES ONLY AND SHOULD NOT BE RELIED UPON FOR REAL WORLD APPLICATIONS*

bayesm covers many important models used
in marketing and micro-econometrics applications.
The package includes:
Bayes Regression (univariate or multivariate dep var),
Bayes Seemingly Unrelated Regression (SUR),
Binary and Ordinal Probit,
Multinomial Logit (MNL) and Multinomial Probit (MNP),
Multivariate Probit,
Negative Binomial (Poisson) Regression,
Multivariate Mixtures of Normals (including clustering),
Hierarchical Linear Models with normal prior and covariates,
Hierarchical Linear Models with a mixture of normals prior and covariates,
Hierarchical Multinomial Logits with a mixture of normals prior
and covariates,
Hierarchical Negative Binomial Regression Models,
Bayesian analysis of choice-based conjoint data,
Bayesian treatment of linear instrumental variables models,
and
Analyis of Multivariate Ordinal survey data with scale
usage heterogeneity (as in Rossi et al, JASA (01)).
For further reference, consult our book, Bayesian Statistics and
Marketing by Rossi, Allenby and McCulloch.

Bayesian mixture models of univariate Gaussian distributions using JAGS

Later

Implementation of BART:Bayesian Additive Regression Trees, Chipman, George, McCulloch (2006)

BayesValidate implements the software validation method
described in the paper "Validation of Software for Bayesian Models
using Posterior Quantiles" (Cook, Gelman, and Rubin, 2005).  It inputs
a function to perform Bayesian inference as well as functions to generate
data from the Bayesian model being fit, and repeatedly generates and
analyzes data to check that the Bayesian inference program works properly.

A suite of R functions for Bayesian estimation of smooth hazard rates via Compound Poisson Process (CPP) and Bayesian Penalized Spline (BPS) priors.

The manuscript introduces the BAYSTAR package, which provides the functionality for Bayesian estimation in autoregressive threshold models.

Methods and functions for fitting maximum likelihood models in R.  This package modifies and extends the mle classes in the stats4 package.

This package follows the work of Parnell and Haslett (2007, submitted to JRSSC; contact the author for a pre-print). It runs MCMC, predictions and plots for radiocarbon (and non radiocarbon) dated sediment cores.

An implementation of the Barry and Hartigan (1993) product partition model for the standard change point problem using Markov Chain Monte Carlo.

Full implementation of the 28 distributions introduced as benchmarks for nonparametric density estimation by Berlinet and Devroye (1994). Includes densities, cdfs, quantile functions and generators for samples as well as additional information on features of the densities.

Beta regression for modeling rates and proportions.

Functions for MLE, MCMC, CIs (originally in Fortran)

Functions and data sets reproducing some examples in Box, Hunter and Hunter II

Statistical models of biased sampling in the form of
univariate and multivariate noncentral hypergeometric distributions,
including Wallenius' noncentral hypergeometric distribution and
Fisher's noncentral hypergeometric distribution
(also called extended hypergeometric distribution).
See vignette("UrnTheory") for explanation of these distributions.

The main function biclust provides several algorithms to find biclusters in two-dimensional data: Cheng and Church, Spectral, Plaid Model, Xmotifs and Bimax. In addition, the package provides methods for data preprocessing (normalization and discretisation), visualisation, and validation of bicluster solutions.

The main function is HMA, the HeightMapAlgorithm. This algorithm
is based on the idea of an 'height map', and is described in the following paper:
"Reduction algorithm for the NPMLE for the distribution function of
bivariate interval-censored data", by Marloes Maathuis, Journal of Computational
and Graphical Statistics, Volume 14, Number 2 (to appear).

This package makes global and multiple inferences for given bi- and trifactorial clinical trial designs using bootstrap methods and a classical approach.

Regression for data too large to fit in memory

Functions to sample and interpret Bayesian QTL using MCMC.

Generation of correlated artificial binary data.

Confidence intervals and tests for a single binomial proportion.
Clopper-Pearson, Blaker, Second-order corrected,  Wilson, Agresti-Coull and Wald CI for standard binomial and binomial group testing.  Exact, Score and Wald Test for standard binomial  and binomial group testing.
Experimental design 1. depending on power of the different CI methods and 2. controlling bias and mse of the estimator
depending on group size and sample size, and calculation of expected interval width.

Asymptotic simultaneous confidence intervals for comparison of many treatments with one control,
for the difference of binomial proportions, allows for Dunnett-like-adjustment, Bonferroni or unadjusted intervals.
Simulation of power of the above interval methods, approximate calculation of any-pair-power, and sample size
iteration based on approximate any-pair power.
Exact conditional maximum test for many-to-one comparisons to a control.

Constructs confidence intervals on the probability of success in a binomial experiment via several parameterizations

The Biodem package provides a number of functions for Biodemographycal analysis.

This package provides a GUI (Graphical User Interface, via the R-Commander) and some utility functions (often based on the vegan package) for statistical analysis of biodiversity and ecological communities, including species accumulation curves, diversity indices, Renyi profiles, GLMs for analysis of species abundance and presence-absence, distance matrices, Mantel tests, and cluster, constrained and unconstrained ordination analysis. A book on biodiversity and community ecology analysis is available for free download from the website.

Imports benthic count data, reformats this data, and computes environmental inferences from this data.

Biopara is a parallel system designed to be used with R.

GUI (using GTK+) for the basic image operations package. At the moment, just provides a function that displays imagedata from biOps package in a new window, keeping original size and with information on the coords and values of the pixels in the image. It requires RGtk2.

This package includes several methods for image processing and analysis. It provides geometric, arithmetic, logic, morphologic (supported on one channel images only), look-up tables, edge detection (including Roberts, Sobel, Kirsch, Marr-Hildreth and Canny, among others) and convolution masks operations (predefined commons masks already defined and user defined applications). Isodata and k-means classification methods are also provided (standard, kd-tree and brute force methods implemented). Fast Fourier Transform methods and filters also available if fftw3 installed. Supports jpeg and tiff images so far (more image support in future versions). libtiff and libjpeg libraries installed required.

See bipartite-package for more details.

Sorry, the description of this package is missing.

Functions for Bitwise operations on integer vectors.

Functions for fitting Bivariate Poisson Models using the EM algorithm. Details can be found in Karlis and Ntzoufras (2003, RSS D & 2004,AUEB Technical Report)

Function for drawing the coastline of the British Isles

Create randomizations for block random clinical trials.  Can also produce a pdf file of randomization cards.

Blocks units into experimental blocks, with one unit per treatment condition, by creating a measure of multivariate distance between all possible pairs of units.  Maximum, minimum, or an allowable range of differences between units on one variable can be set.  Randomly assign units to treatment conditions.  Diagnose potential interference problems between units assigned to different treatment conditions.  Write outputs to .tex and .csv files.

Package for Bayesian model averaging for linear models, generalizable linear models and survival models (cox regression).

Bayesian network structure learning
via constraint-based (also known as 'conditional
independence') and score-based algorithms.
This package implements the Grow-Shrink (GS)
algorithm, the Incremental Association (IAMB)
algorithm, the Interleaved-IAMB (Inter-IAMB)
algorithm, the Fast-IAMB (Fast-IAMB) algorithm
and the Hill-Climbing (HC) greedy search
algorithm for both discrete and gaussian networks,
along with many score functions and conditional
independence tests.
Some utility functions (model comparison and
manipulation, random data generation, arc
orientation testing) are also included.

A menu-driven program and library of functions for carrying out
convergence diagnostics and statistical and graphical analysis of Markov
chain Monte Carlo sampling output.

A set of R functions and data sets for the book Introduction to Bayesian Statistics, Bolstad, W.M. (2007), John Wiley & Sons ISBN 0-471-27020-2

This package implements a partial-observability procedure for testing Boolean hypotheses that generalizes the binary response GLM.

Contains a collection of boosting methods, these are
'BagBoost', 'LogitBoost', 'AdaBoost' and 'L2Boost',
along with feature preselection by the Wilcoxon test
statistic. Moreover, methods for the simulation of
data according to correlation and mean structures of
existing real datasets are included.

Bootstrapping test for chromosomal localization

functions and datasets for bootstrapping from the
book "Bootstrap Methods and Their Applications" by A. C. Davison and
D. V. Hinkley (1997, CUP).

Model selection by bootstrapping the stepAIC() procedure.

Software (bootstrap, cross-validation, jackknife) and
data for the book "An Introduction to the Bootstrap" by B. Efron
and R. Tibshirani, 1993, Chapman and Hall.
_____________________________________________________________
This package is primarily provided for projects already based
on it, and for support of the book.  New projects should
preferentially use the recommended package "boot".


This software is used in two situations. The first is to predict the next outcome based on the previous states of a discrete sequence. The second is to classify a discrete response  based on a number of discreate covariates. In both situations, we use Bayesian logistic regression models that consider the high-order interactions. The time arising from using high-order interactions is reduced greatly by our compression technique that represents a group of original parameters as a single one in MCMC step. In this version, we use log-normal prior for the hyperparameters. When it is used for the second situation --- classification, we consider the full set of interaction patterns up to a specified order.

QTL mapping toolkit for inbred crosses and
recombinant inbred lines. Includes maximum likelihood and Bayesian tools.

Specify and fit the Bradley-Terry model and structured versions

This package computes the correlation matrix for each scale of a
wavelet decomposition, namely the one performed by the R package waveslim
(Whitcher, 2000). An hypothesis test is applied to each entry of one matrix
in order to construct  an adjacency matrix of a graph. The graph obtained is
finally analysed using the small-world theory (Watts and Strogatz, 1998) and
using the computation of efficiency (Latora, 2001), tested using simulated
attacks. The brainwaver project is complementary to the camba project for
brain-data preprocessing. A collection of scripts (with a makefile) is avalaible to download along with the brainwaver package, see information on the webpage mentioned below.

brew implements a templating framework for mixing text and R code for report generation.
brew template syntax is similar to PHP, Ruby's erb module, Java Server Pages, and Python's psp module.

Fit binomial-response GLMs using either a modified-score approach to bias-reduction or maximum penalized likelihood where penalization is by Jeffreys invariant prior. Fitting takes place by iteratively fitting a local GLM on a pseudo-data representation. The interface is essentially the same as 'glm'. More flexibility is provided by the fact that custom pseudo-data representations can be specified and used for model fitting. Functions are provided for the construction of confidence intervals for the bias-reduced estimates.

Fits logistic regression models by maximum penalized likelihood

Handles very large numbers in R.  Real numbers are held
using their natural logarithms, plus a logical flag indicating
sign.  The package includes a vignette that gives a step-by-step
introduction to using S4 methods.

An R / S-PLUS package containing OpenBUGS and its R / S-PLUS interface BRugs.

Data sets for book "Basic Statistics and Data Analysis" by Larry J. Kitchens

A collection of utilities for the Birnbaum-Saunders distribution (BSD).

Bayes screening and model discrimination follow-up designs.

A package for testing, profiling and benchmarking your code.

An R interface to the Stark-Parker algorithm for bounded-variable least squares

This package provides tools for caching statistical analyses in key-value databases which can subsequently be distributed over the web.

Tools for caching Sweave computations and storing them in key-value databases

A package for computation and visualization of simple, multiple and joint correspondence analysis.

Cairo/GTK graphics device driver with output to screen,
file (png, svg, pdf, and ps) or memory (GdkDrawable). The screen device
may be embedded into RGtk2 interfaces. Supports all interactive features
of other graphics devices, including getGraphicsEvent().

This package provides a Cairo graphics device that can be use to create high-quality vector (PDF, PostScript and SVG) and bitmap output (PNG,JPEG,TIFF), and high-quality rendering in displays (X11 and Win32). Since it uses the same back-end for all output, copying across formats is WYSIWYG. Files are created without the dependence on X11 or other external programs. This device supports alpha channel (semi-transparent drawing) and resulting images can contain transparent and semi-transparent regions. It is ideal for use in server environemnts (file output) and as a replacement for other devices that don't have Cairo's capabilities such as alpha support or anti-aliasing. Backends are modular such that any subset of backends is supported.

Provides basic S4 data structures and routines for
calibration of bioassays

Package for drawing calibrated scales with tick marks on (non-orthogonal)
variable vectors in scatterplots and biplots.

Functions for processing and classification of protein mass
spectra (SELDI) data. Also includes support for mzXML Files.

This package hooks a Carbon event loop handler into R. This is useful for enabling UI from a console R (such as using the Quartz device from Terminal or ESS).

Misc functions for training and plotting classification and regression models

Augment some caret functions for parallel processing

Augment some caret functions using parallel processing


This package accompanies J. Fox, An R and S-PLUS Companion to Applied Regression,
Sage, 2002.
The package contains mostly functions for applied regression, linear models, and
generalized linear models, with an emphasis on regression diagnostics, particularly
graphical diagnostic methods. There are also some utility functions.
With some exceptions, I have tried not to duplicate capabilities in the basic distribution of R,
nor in widely used packages. Where relevant, the functions in car are consistent with
na.action = na.omit or na.exclude.

Analysis of categorical-variable with missing values

catmap is an R package that conducts fixed-effects (inverse variance) and random-effects (DerSimonian and Laird, 1986) meta-analyses of case-control or family-based (TDT) genetic data; in addition, it performs meta-analyses combining these two types of study designs.  The fixed-effects model was first described by Kazeem and Farrell (2005); the random-effects model is described in Nicodemus (submitted).

Contains several basic utility functions including:
moving (rolling, running) window statistic functions, read/write for GIF
and ENVI binary files, fast calculation of AUC, LogitBoost classifier, base64
encoder/decoder, round-off error free sum and cumsum, etc.

'sqtab' contains a set of functions for estimating
loglinear models for square tables such as quasi-independence,
symmetry, uniform association.
'mclgen' restructures a dataframe to enable the estimation of a
multinomial logistic model using the conditional logit program
'clogit'. This allows greater flexibility in imposing constraints on
the response variable. One application is to specify aforementioned
models for square tables as multinomial logistic models with
covariates at the respondent level.
'ctab' simplifies the production of (multiway( percentage tables.

Implements clustering techniques such as Proximus and Rock, utility functions for efficient computation of cross distances and data manipulation.

The package provide a set of functions that extend the cancor function with new numerical and graphical outputs. It also include a regularized extension of the cannonical correlation analysis to deal with datasets with more variables than observations.

Convex Clustering methods, including Kmeans algorithm,
On-line Update algorithm (Hard Competitive Learning) and
Neural Gas algorithm (Soft Competitive Learning) and
calculation of several indexes for finding the number of
clusters in a data set.

Components of Canadian Credit Aggregates and Monetary Aggregates with continuity adjustments.

This package is an object-oriented implementation of one-dimensional cellular automata. It supports many of the features offered by Mathematica, including elementary rules, user-defined rules, radii, user-defined seeding, and plotting.

Retrieve Affymetrix microarray measurements and metadata from Celsius web services, see http://genome.ucla.edu/projects/celsius

Analysis of configuration frequencies for simple and repeated measures, more sample CFa, hierarchical CFA, bootstrap-CFA, functional CFA, Kieser-Victor CFA and various plots

Efficient procedures for fitting an entire regression
sequences with different model types.

Spatial smoothing and hot spot detection using the fused lasso regression

Functions to analyze microarray comparative genome hybridization data using the Smith-Waterman algorithm

Facilities for the use of R to write CGI scripts

The package contains the Mack- and Munich-chain-ladder methods which are used in insurance claims reserving exercise.

Change in length of hospital stay (LOS) is frequently used to assess the impact and the costs of hospital-acquired complications. In order to compute the attributable change in LOS, it is crucial to account for the timing of events: A complication can only have an effect on LOS, once it has occured. These temporal dynamics can be adequately handled by multistate models; however, there is few software for such models available. We introduce an R-package "changeLOS" for computing change in LOS based on methods described in Schulgen and Schumacher (1996). We will illustrate the program on data from a prospective cohort study on hospital-acquired infections. Main features of the R-package "changeLOS" are R-methods to: (1) describe the multi-state model. (2) compute the Aalen-Johansen estimator for the matrix of transition probabilities P(u-, u) for all observed transition times u.(3) compute the Aalen-Johansen estimator for the matrix of transition probabilities P(s,t); the estimator is a finite matrix product of matrices P(u-,u) for every observed event time in the interval(s,t]. (4) visualize the temporal dynamics of the data, illustrated by transition probabilities. (5) compute and visualize change in LOS. (6) compute bootstrap variances for change in LOS.

Discrete Linear Chebyshev Approximation

chemCal provides simple functions for plotting linear
calibration functions and estimating standard errors for measurements
according to the Handbook of Chemometrics and Qualimetrics: Part A
by Massart et al. There are also functions estimating the limit
of detection (LOQ) and limit of quantification (LOD).
The functions work on model objects from - optionally weighted - linear
regression (lm) or robust linear regression (rlm from the MASS package).

Choplump Tests are Permutation Tests for Comparing Two Groups with Some Positive but Many Zero Responses

Informative and nice plots for grouped bivariate data.

Chronological objects which can handle dates and times

Circular Statistics, from "Topics in circular Statistics" (2001) S. Rao Jammalamadaka and A. SenGupta, World Scientific.

Circular Statistics, from "Topics in circular Statistics" (2001) S. Rao Jammalamadaka and A. SenGupta, World Scientific.

The 'cir' package provides a documented version of the well-known isotonic regression (IR) algorithm (function 'pava'), and an improvement to IR in case the true function is known to be smooth and strictly monotone. This improvement called Centered Isotonic Regression (CIR) is available via the function 'cir.pava'. Additionally, the function 'cir.upndown' provides percentile estimation for dose-response experiments (e.g., ED50 estimation of a medication) using CIR.

Clust Along Chromosomes, a method to call gains/losses in CGH array data

Construct directed graphs of S4 class hierarchies and
visualize them.  Typically, these graphs are DAGs (directed acyclic
graphs) in general, though often trees.

Given $p$-dimensional training data containing $d$ groups (the design space), a classification algorithm (classifier) predicts which group new data belongs to.  Generally the input to these algorithms is high dimensional, and the boundaries between groups will be high dimensional and perhaps curvilinear or multi-faceted. This package implements methods for understanding the division of space between the groups.  See \url{http://had.co.nz/classifly} for more details.

A package for choosing univariate class intervals for mapping or other graphics purposes

PP Indices using class information

Functions to fill missing data in climatological
(monthly) series and to test their homogeneity, plus functions
to draw wind-rose and Walter&Lieth diagrams.

The package contains R functions for retrieving data, making climate analysis and downscaling of monthly mean and daily mean global climate scenarios. (Windows-users may need to obtain the 'ncdf' package from URL 'http://www.stats.ox.ac.uk/pub/RWin/' to get clim.pact to work).

Calculates Contour Lines

Utilities to make your clinical collaborations easier if not fun.

CLUster Ensembles

Visualise clustering algorithms with GGobi.  Contains both general code for visualising clustering results and specific visualisations for model-based, hierarchical and SOM clustering. See http://had.co.nz/clusterfly for more information.

The package contains functions for generating random
clusters, generating random covariance/correlation matrices,
calculating a separation index (data and population version)
for pairs of clusters or cluster distributions, and
1-D and 2-D projection plots to visualize clusters.
The package also contains a function to generate random
clusters based on factorial designs with factors such as
degree of separation, number of clusters,
number of variables, number of noisy variables.

Cluster Analysis, extended original from
Peter Rousseeuw, Anja Struyf and Mia Hubert.

A function for validating microarry clusters via reproducibility

GDM Distance,
Sokal-Michener Distance,
Bray-Curtis Distance,
Calinski-Harabasz Index,
G2 Index,
G3 Index,
Silhouette Index,
Krzanowski-Lai Index,
Hartigan Index,
Gap Index,
DB Index,
Data Normalization,
HINoV method,
Replication analysis for cluster validation,
Clustering with several algorithms, distances, normalizations and icq indices,
Symbolic interval distances,
Plot functions,
Random cluster generation

This package can be used for clustering data with spatial information. Try function GUIspatClust() to run the clustTool GUI.

The selection method uses either a greedy search or headlong search. The greedy search at each step either checks all variables not currently included in the set of clustering variables singly for inclusion into the set or checks all variables in the set of clustering variables singly for exclusion.The headlong search only checks until a variable is included or excluded (i.e. does not necessarily check all possible variables for inclusion/exclusion at each step) and any variable with evidence of clustering below a certain level at any stage is removed from consideration for the remainder of the algorithm. Each variable's evidence for being useful to the clustering given the currently selected clustering variables is given by the difference between the BIC for the model with clustering (allowed to vary over 2 to a maximum number of groups and any of the different covariance parameterizations allowed in mclust) using the set of clustering variables including the variable being checked and the sum of BICs for the model with clustering (allowed to vary over 2 to a maximum number of groups and any of the different covariance parameterizations allowed in mclust) using the set of clustering variables without the variable being checked and the model for the variable being checked being conditionally independent of the clustering given the other clustering variables (this is modeled as a regression of the variable being checked on the other clustering variables).

Statistical and biological validation of clustering results.


Package contains most of the popular internal and external cluster validation methods ready to use for the most of the outputs
produced by functions coming from package "cluster". Package contains also functions and examples of usage for cluster
stability approach applied to algorithms implemented in "cluster" package.

Estimation, testing and regression modeling of
subdistribution functions in competing risks, as described in Gray
(1988), A class of K-sample tests for comparing the cumulative
incidence of a competing risk, Ann. Stat. 16:1141-1154, and Fine JP and
Gray RJ (1999), A proportional hazards model for the subdistribution
of a competing risk, JASA, 94:496-509.

Qualitatively Constrained (Regression) Smoothing via Linear
Programming. OUTDATED version based on He & Ng (1999), the modified
"non-simplex" algorithm of Bartels and Conn (1980).

Qualitatively Constrained (Regression) Smoothing via Linear
Programming and Sparse Matrices.

Interface to CoCo from R

Fits predictive and symmetric co-correspondence analysis (CoCA) models to relate one data matrix
to another data matrix. More specifically, CoCA maximises the weighted covariance
between the weighted averaged species scores of one community and the weighted averaged species
scores of another community. CoCA attempts to find patterns that are common to both communitities.

Output analysis and diagnostics for Markov Chain Monte
Carlo simulations.

Code analysis tools for R

Conditional inference procedures for the general independence
problem including two-sample, K-sample, correlation, censored, ordered and
multivariate problems.

Builds gradient color maps

Carries out mapping between assorted color spaces.

routines for combinatorics

ComPairWise contains functions to compare DNA/RNA alignments.

The package offers a fitting of smooth varying coefficients
in a competing risks modelling of hazards as well as estimating of the frailties (or unobserved heterogenities)
for clustered observations. Nonparametric penalized spline (p-spline)
fitting of smooth covariates effects is proposed. As a spline basis truncated polynomial functions are chosen.
The frailties are also fitted (via the EM-algoritghm) in a flexible way using a penalizied mixture of gamma distributions.

Performs the complementary hierarchical clustering procedure and returns X' (the expected residual matrix), and a vector of the relative gene importance.

The package provides functions for the consistent analysis of
compositional data (e.g. portions of substances) and positive numbers
(e.g. concentrations) in the way proposed by Aitchison and Pawlowsky-Glahn.

This package contains function to test the difference between two
overlapping (in the sense of having a variable in common) correlation coefficients, using
a Z-test as described by Meng, Rosenthal, and Rubin (Meng et al, 1992).

Measures of concordance and reliability

The four functions svdcp (cp for column partitioned), svdbip or
svdbip2 (bip for bi-partitioned), and svdbips (s for a simultaneous
optimization of one set of r solutions), correspond to a "SVD by
blocks" notion, by supposing each block depending on relative
subspaces, rather than on two whole spaces as usual SVD does. The
other functions, based on this notion, are relative to two column
partitioned data matrices x and y defining two sets of subsets xi and
yj of variables and amount to estimate a link between xi and yj for
the pair (xi, yj) relatively to the links associated to all the other
pairs.

This small library contains a series of simple tools for
constructing and manipulating confounded and fractional
factorial designs. To some extent these complement those
already available in S-PLUS 3.1 and 3.2, (but they
may well be rendered obsolete by S+DOX, to which the author
does not yet have access).

Package provides functions to find and plot disconnected
subsets and hence test for estimability in a two-way cell-means
model without interaction.

This package furnishes R with a suite of object-oriented data structures: stack, queue, deque, max-heap, min-heap, binary search tree, and splay tree.

Contrast methods, in the style of the Design package,
for fit objects produced by the lm, glm, gls, and geese functions.

Classes (S4) of commonly used copulas including elliptical, Archimidean, extreme value and Farlie-Gumbel-Morgenstern families. Methods for density, distribution, random number generators, bivariate association measures, persp, and contour. Functions for fitting copula models. Independence tests among random variables and random vectors based on the empirical copula process. Serial independence tests for univariate and multivariate continuous time series based on the empirical copula process.

This package implements an analytic shrinkage approach
for inferring the covariance matrix.  The estimator is statistically
highly accurate and efficient, applicable to "small n, large p" data,
and always returns a positive definite and well-conditioned matrix.
Nevertheless, this method requires only little a priori modeling and is
computationally cheap. In addition to covariance estimation the
package contains similar functions for inferring variances, correlations,
partial correlations, partial variances, and regression coefficients.
Furthermore, it provides  functions for fast SVD computation, for computing
the pseudoinverse, for checking the rank and positive definiteness of a
matrix, and for the computationally fast inversion of the covariance
and correlation matrix.

Utility functions for the statistical analysis of corpus frequency data

Multivariate correlation estimation and statistical inference. See package vignette.

Calculates correlation of variables and displays the results graphically.

The cov.nnve() function for robust covariance estimation
by the nearest neighbor variance estimation (NNVE) method
of Wang and Raftery (2002,JASA)

This package provides routines for fitting Cox survival models by likelihood based boosting

Cox regression with Firth's penalized likelihood

Fit robustly proportional hazards regression model

Functions to calculate concordance probability estimates in survival analysis

Provides R routine for the so called two-sample Cramer-Test.
This not distribution free, nonparametric two-sample-test can be applied
on multivariate data as well as univariate data. It offers two
possiblities to approximate the critical value both of which are
included in this package.

Functions for completing and recalculating rankings.

A set of functions for computing the CreditMetrics risk model

Contains functions for the construction and randomization of balanced carryover balanced designs. Contains functions to check given designs for balance. Also contains functions for simulation studies on the validity of two randomization procedures.

Functions for identification of probes potentially affected by cross-hybridizations in microarray experiments. Includes functions for diagnostic plots.

Quantile regression for randomly censored data


This package contains functions for likelihood and posterior
analysis of conditionally specified logistic regression models.
All calculus and simulation is done in compiled FORTRAN.

The CTFS Large Plot Forest Dynamics Analyses

Continuous Time Autoregressive Models and the Kalman Filter

Server-side and client-side tools for task views
to CRAN-style repositories

This package carries out level-dependent
cross-validation method for the selection of thresholding
value in wavelet shrinkage. This procedure is implemented
by coupling a conventional cross validation with an
imputation method due to a limitation of data length,
a power of 2. It can be easily applied to classical
leave-one-out and k-fold cross validation.
Since the procedure is computationally fast,
a level-dependent cross validation can be performed for
wavelet shrinkage of various data such as a data
with correlated errors.

Miscellaneous functions for general use

Functions for locating local minima/maxima

Data sets and functions, useful for the display of microarray
and for demonstrations with microarray data

various data sets used in examples and exercises in
the book Maindonald, J.H. and Braun, W.J. (2003, 2007)  "Data
Analysis and Graphics Using R".

various data sets used in additonal exercises for
the book Maindonald, J.H. and Braun, W.J. (2nd edn 2007)
"Data Analysis and Graphics Using R", and for a
'Data Mining' course.  Note especially the datasets
nassCDS (airbag and other influences on US vehicle
accident outcomes: 1997-2002), nswdemo and related datasets
(US Labor Training Evaluation Data) rockArt (multivariate
binary data on Pacific rock art), hotspots (ages and
distance from Kilauea of larval hotspots; c.f. also
hotspots2006) and nihills (record times for Northern
Ireland mountain races).

Functions for handling dates.

The Davies quantile function and the Generalized Lambda distribution

A database interface (DBI) definition for
communication between R and relational database management
systems.  All classes in this package are virtual and need
to be extended by the various R/DBMS implementations.

Use EM algorithm to compute the NPMLE of CDF and also the two
censoring distributions. For doubly censored data
(as described in Chang and Yang (1987) Ann. Stat. 1536-47).
You can also specify a constraint, it will return
the constrained NPMLE and the -2 log empirical likelihood ratio.
This can be used to test the hypothesis about the constraint
and find confidence intervals for F(T) via empirical likelihood
ratio theorem.
Influence function of hat F may also be calculated (but slow).

A set of functions for the detection of spatial
clusters of disease using count data. Bootstrap is used to estimate sampling
distributions of statistics.

This package solves systems of delay differential equations.
by interfacing numerical routines written by Simon N. Wood
&lt;s.wood _at_ bath.ac.uk&gt;, with contributions by Benjamin J. Cairns
&lt;ben.cairns@bristol.ac.uk&gt;. These numerical routines first appeared
in Simon Wood's solv95 program.

The package contains the normalizing and variance stabilizing
Data-Driven Haar-Fisz algorithm. Also contains related algorithms
for simulating from certain microarray gene intensity models and
evaluation of certain transformations.

Performs some basic models of Data Envelopment Analysis, both in multiplier and envelopment form.

Bayesian networks with continuous and/or discrete variables can be learned and compared from data.

Debugger for R functions, with code display, graceful
error recovery, line-numbered conditional breakpoints, access
to exit code, flow control, and full keyboard input.

Set, Get, and Import Global Function Defaults

Likelihood-based inference for skewed count distributions used in network modeling. "degreenet" is a part of the "statnet" suite of packages for network analysis.  For a list of functions type: help(package='degreenet')

Calculates the  Delaunay triangulation and the Dirichlet
or Voronoi tessellation (with respect to the entire plane) of
a planar point set.


The package implements
methods for estimating multivariate densities:
adaptive histograms (greedy histograms and CART-histograms),
stagewise minimization, and
bootstrap aggregation are provided.

Construction and analysis of matrix population models in R.


The package provides tools to
(1) visualize multivariate density functions and density estimates
with level set trees,
(2) visualize level sets with shape trees,
(3) visualize multivariate data with tail trees,
(4) visualize scales of multivariate density estimates with
mode graphs and branching maps, and
(5) visualize anisotropic spread with 2D volume functions and
2D probability conetent functions.
Level set trees visualize mode structure,
shape trees visualize shapes of level sets of unimodal densities,
and tail trees visualize connected data sets.
The kernel estimator is implemented
but the package may be applied for visualizing other density estimates.

This package provides the DEoptim function which performs
Differential Evolution Optimization (evolutionary algorithm).

Fit (multigroup) mixtures of latent Markov models on mixed categorical and continuous (timeseries) data

Produce publication quality graphics from output of GGobi's describe display plugin

Regression modeling, testing, estimation, validation,
graphics, prediction, and typesetting by storing enhanced model design
attributes in the fit.  Design is a collection of about 180 functions
that assist and streamline modeling, especially for biostatistical and
epidemiologic applications.  It also contains new functions for binary
and ordinal logistic regression models and the Buckley-James multiple
regression model for right-censored responses, and implements
penalized maximum likelihood estimation for logistic and ordinary
linear models.  Design works with almost any regression model, but it
was especially written to work with logistic regression, Cox
regression, accelerated failure time models, ordinary linear models,
the Buckley-James model, and generalized least squares for
serially or spatially correlated observations.

S3 classes for multivariate optimization using the desirability function by Derringer and Suich (1980)

Data sets and sample analyses from Jay L. Devore
(2000), "Probability and Statistics for Engineering and the
Sciences (5th ed)", Duxbury.

Data sets and sample analyses from Jay L. Devore
(2003), "Probability and Statistics for Engineering and the
Sciences (6th ed)", Duxbury.

Data sets and sample analyses from Jay L. Devore (2008),
"Probability and Statistics for Engineering and the Sciences
(7th ed)", Thomson.

Fitting double  generalized linear models

Visualises simple graphs (networks) based on a transition matrix, utilities to plot flow diagrams,
visualising webs,...
Support for the book "A guide to ecological modelling" by Karline Soetaert and Peter Herman (in preparation)
Includes demo(flowchart), demo(plotmat), demo(plotweb)

Functions for illustrating aperture-4 diamond partitions in the plane, or on the surface of an octahedron or icosahedron, for use as analysis or sampling grids.

This package provides utilities to calculate the probabilities of various dice-rolling events, such as the probability of rolling six four-sided dice and getting a 4, a 3, and either a 1 or 2 among the six rolls (in any order) or the probabilities of each possible total of rolling five six-sided dice, dropping the lowest two rolls, and summing the remaining dice.

Collapse red-green distinctions to simulate the effects of colour-blindness

This package provides functions to import and manipulate
medical imaging data via the Digital Imaging and Communications in
Medicine (DICOM) Standard.

This package provides a wrapper to the FITPACK routines written by Paul Dierckx. The original Fortran is available from http://www.netlib.org/dierckx

The digest package provides functions for the creation of
'hash' digests of arbitrary R objects using the md5, sha-1 and
crc32 algorithms permitting easy comparison of R language objects.
The md5 algorithm by Ron Rivest is specified in RFC 1321, the SHA-1
algorithm is specified in FIPS-180-1 and the crc32 algorithm is
described in ftp://ftp.rocksoft.com/cliens/rocksoft/papers/crc_v3.txt.
For md5 and sha-1, this packages uses two small standalone C
implementations that were provided by by Christophe Devine. For
crc32, code from the zlib library is used.

Please note that this package is not meant to be used for
cryptographic purposes for which more comprehensive (and widely
tested) libraries such as OpenSSL should be used.

Compute Hartigan's dip test statistic for unimodality

Functions for modelling dispersion in GLM.

provides documentation in form of a common vignette to packages distr, distrEx, distrSim, distrTEst

Extensions of package distr and some additional functionality

Probability distributions (binomial, poisson,
geometric, normal, chi square, Fisher, Student) based on
TI-83 Plus graphic scientific calculator

Object orientated implementation of distributions

Simulation (S4-)classes based on package distr

Evaluation (S4-)classes based on package distr for evaluating
procedures (estimators/tests) at data/simulation in a unified way.

Functions to filter and summarize time-depth recorder (TDR)
data, and miscellaneous functions for handling location data.

Maximum likelihood, Kalman filtering and smoothing, and Bayesian
analysis of Normal linear State Space models, also known as
Dynamic Linear Models

Facilities for groupwise computations of summary statistics and other facilities for working with grouped data (similar to what can be achieved by proc means or proc summary of the SAS system).

This package contains functions for performing some standard tree-ring analyses.


This package contains functions to perform inference via
simulation from the posterior distributions for Bayesian nonparametric and
semiparametric models. Although the name of the package was motivated
by the Dirichlet Process prior, the package considers and will consider
other priors on functional spaces. So far, DPpackage includes models
considering Dirichlet Process, Polya Trees, Mixtures of Triangular distributions,
and Random Bernstein polynomials priors. The package
also includes models considering Penalized B-Splines.
Currently the package includes semiparametric models for density estimation, ROC curve analysis,
interval censored data, binary regression models, generalized linear mixed
models, IRT type models, and generalized additive models. The package also contains functions
to compute Pseudo-Bayes factors for model comparison, and to elicitate the precision
parameter of the Dirichlet Process. To maximize computational efficiency,
the actual sampling for each model is done in compiled FORTRAN. The functions
return objects which can be subsequently analyzed with functions provided in
the coda package.

Functions for normalization, treatment of missing values, discretization,
outlier detection, feature selection,  and visualization

Analysis of one or multiple curves with focus on concentration-response, dose-response and time-response curves used, for example, in biology, environmental sciences, medicine, pharmacology, toxicology.

drfit provides basic and easy-to-use functions for fitting
dose-response curves to dose-response data, calculating some
(eco)toxicological parameters and plotting the results. Functions that are
fitted are the cumulative density function of the lognormal distribution
(probit fit), of the logistic distribution (logit fit), of the weibull
distribution (weibull fit) and a linear-logistic model ("linlogit" fit),
derived from the latter, which is used to describe data showing stimulation
at low doses (hormesis). In addition, functions checking, plotting and
retrieving dose-response data retrieved from a database accessed via RODBC
are included.

Functions, methods, and datasets for fitting dimension
reduction regression, using slicing (methods SAVE and SIR), Principal
Hessian Directions (phd, using residuals and the response), and an
iterative IRE.  Partial methods, that condition on categorical
predictors are also available.  A variety of tests, and stepwise
deletion of predictors, is also included.  Also included is
code for computing permutation tests of dimension.  Adding additional
methods of estimating dimension is straightforward.
For documentation, see the vignette in the package.

Likelihood-based marginal regression and association modelling for repeated, or otherwise clustered, categorical responses using dependence ratio as a measure of the association

Multivariate Time Series Library
For each package there is a section of the Users' Guide
available in doc/*.pdf and these are combined in
dse1/doc/dse-guide.pdf.
The package dse1 is the base system, including multivariate
ARMA and State Space models.
Package dse2 has extensions for evaluating estimation
techniques, forecasting, and for evaluating forecasting models.
These packages require the packages setRNG and tframe.

This package provides functions for 1D and 2D Discrete Cosine Transform (DCT), Discrete Sine Transform (DST) and Discrete Hartley Transform (DHT).

Comprehensive implementation of Dynamic Time Warping
algorithms in R. DTW finds the optimal (least cumulative distance)
mapping between two time series. This package implements all common
DTW variants, including local and global constraints, arbitrary
timeseries lenghts, distance definitions, etc.  Methods provides
cumulative distance, warping functions, plots, etc.

Interactive graphical tool for manipulating graphs

Time series regression.  The dyn class interfaces
ts, irts, its, zoo and zooreg time series classes to
lm, glm, loess, quantreg::rq, MASS::rlm, quantreg::rq,
randomForest::randomForest and other regression
functions allowing those functions to be used with
time series including specifications that may contain
lags, diffs and missing values.

Dynamic linear models and time series regression.

Functions for latent class analysis, short time Fourier
transform, fuzzy clustering, support vector machines,
shortest path computation, bagged clustering, naive Bayes
classifier, ...

Build regression models using the techniques in Friedman's
papers "Fast MARS" and "Multivariate Adaptive Regression Splines".
(The term "MARS" is copyrighted and thus not used in the name of the package.)

Fitting and testing multi-attribute probabilistic choice
models, especially the Bradley-Terry-Luce (BTL) model (Bradley &
Terry, 1952; Luce, 1959), elimination-by-aspects (EBA) models
(Tversky, 1972), and preference tree (Pretree) models (Tversky &
Sattath, 1979).

This package carries out Empirical Bayes thresholding
using the methods developed by I. M. Johnstone and B. W. Silverman.
The basic problem is to estimate a mean vector given a vector of observations of the
mean vector plus white noise, taking advantage of possible sparsity in the mean vector.
Within a Bayesian formulation, the elements of the mean vector are modelled as having,
independently, a distribution that is a mixture of an atom of probability at zero and
a suitable hevay-tailed distribution.
The mixing parameter can be estimated by a marginal maximum likelihood approach.
This leads to an adaptive thresholding approach on the original data.
Extensions of the basic method, in particular to wavelet thresholding, are also
implemented within the package.

Data sets for econometrics

Some wrappers, functions and data sets for for spatial point pattern analysis, used in the book "Introduccion al Analisis Espacial de Datos en Ecologia y Ciencias Ambientales: Metodos y Aplicaciones".

Dissimilarity-based analysis functions including ordination and Mantel test functions, intended for use with spatial and community data.

eco is a publicly available R package that implements the
Bayesian and likelihood methods proposed in Imai, Lu, and Strauss
(Forthcoming) for ecological inference in $2 \times 2$ tables as
well as the method of bounds introduced by Duncan and Davis (1953).
The package fits both parametric and nonparametric models using
either the Expectation-Maximization algorithms (for likelihood
models) or the Markov chain Monte Carlo algorithms (for Bayesian
models).  For all models, the individual-level data can be directly
incorporated into the estimation whenever such data are available.
Along with in-sample and out-of-sample predictions, the package also
provides a functionality which allows one to quantify the effect of data
aggregation on parameter estimation and hypothesis testing under the
parametric likelihood models.

The library contains R-functions to estimate the
effective dimension reduction space in multi-index regression models.


Graphical and tabular effect displays, e.g., of interactions, for linear
and generalised linear models.

A package for survival and event history analysis

This package estimates the parameters of a model for
symmetric relational data (e.g.,  the above-diagonal part of a
square matrix), using a model-based eigenvalue decomposition
and regression. Missing data is accomodated, and a posterior
mean for missing data is calculated under the assumption that the
data are missing at random. The marginal distribution of the
relational data can be arbitrary, and is fit with an ordered
probit specification.

Provides methods for analyzing RxC ecological contingency tables using the extreme case analysis, ecological regression, and Multinomial-Dirichlet ecological inference models.  Also provides tools for manipulating higher-dimension data objects.

Sorry, the description of this package is missing.

Useful when reading the book above mentioned, in the
documentation referred to as 'the book'.

This package contains various routines for drawing
ellipses and ellipse-like confidence regions, implementing the plots
described in Murdoch and Chow (1996), A graphical display of large
correlation matrices, The American Statistician 50, 178-180. There are
also routines implementing the profile plots described in Bates and
Watts (1988), Nonlinear Regression Analysis and its Applications.


A suite of elliptic and related functions including Weierstrass and
Jacobi forms.  Also includes various tools for manipulating and
visualizing complex functions.

elrm implements a Markov Chain Monte Carlo algorithm to approximate exact conditional inference for logistic regression models. Exact conditional inference is based on the distribution of the sufficient statistics for the parameters of interest given the sufficient statistics for the remaining nuisance parameters. Using model formula notation, users specify a logistic model and model terms of interest for exact inference.

evolutionary Monte Carlo methods for clustering, temperature ladder construction and placement

Random walk Metropolis, Metropolis Hasting, parallel tempering, evolutionary Monte Carlo, temperature ladder construction and placement.

Auxiliary functions and data sets for _Ecological Models and Data_, a book presenting maximum likelihood estimation and related topics for ecologists (in preparation for Princeton University Press)

This package includes functions to read and write
to an EMME/2 databank

empirical likelihood ratio tests for means/quantiles/hazards
from possibly censored and/or truncated data. Now does regression too.

Provides an interface to the Emu speech database system and
many special purpose functions for display and analysis of speech
data.

Estimation of missing values in a matrix by a k-th nearest neighboors algorithm

E-statistics (energy) tests for comparing distributions:
multivariate normality, multivariate k-sample test for equal distributions,
hierarchical clustering by e-distances, multivariate independence test,
goodness-of-fit tests. Energy-statistics concept based on a generalization of
Newton's potential energy is due to Gabor J. Szekely.

Bayesian Model Averaging to create probabilistic
forecasts from ensemble forecasts and weather observations.

This package contains elementary tools for analysis of common epidemiological problems, ranging from sample size estimation, through 2x2 contingency table analysis and basic measures of agreement (kappa, sensitivity/specificity).  Appropriate print and summary statements are also written to facilitate interpretation wherever possible.  This package is a work in progress, so any comments or suggestions would be appreciated.  Source code is commented throughout to facilitate modification.  The target audience includes graduate students in various epi/biostatistics courses.

Functions making R easy for epidemiological calculation.

Functions for demographic and epidemiological
analysis in the Lexis diagram, i.e. register and cohort follow-up
data, including interval censored data and representation of
multistate data. Also some useful functions for tabulation and
plotting. Contains some epidemiological datasets.

EpiTools: R Package for Epidemiologic Data and Graphics

Smoothing methods for images which are based on a redescending
M kernel estimator which preserves edges and corners.

This package provides some statistical tests and graphics
for assessing tests of equivalence.  Such tests have similarity
as the alternative hypothesis instead of the null.  Sample
datasets are included.

An integrated set of tools to analyze and simulate networks based on exponential-family random graph models (ERGM). "ergm" is a part of the "statnet" suite of packages for network analysis.  For a list of functions type: help(package='ergm')

eRm fits Rasch models (RM), linear logistic test models (LLTM), rating scale model (RSM), linear rating scale models (LRSM), partial credit models (PCM), and linear partial credit models (LPCM). Missing values are allowed in the data matrix. Additional features are the estimation of the person parameters, LR-Model test, item-specific Wald test, itemfit and personfit statistics, various ICC plots. An eRm platform is provided at http://r-forge.r-project.org/projects/erm/.

The package allows selecting those treatments of a one-way layout
being equivalent to a control. Bonferroni adjusted "two one-sided t-tests"
(TOST) and related simultaneous confidence intervals are given for both
differences or ratios of means of normally distributed data. For the case of
equal variances and balanced sample sizes for the treatment groups, the
single-step procedure of Bofinger and Bofinger (1995) can be chosen. For
non-normal data, the Wilcoxon test is applied.

Provides functions for the bayesian analysis of
extreme value models, using MCMC methods.

Extends simulation, distribution, quantile and
density functions to univariate and multivariate parametric
extreme value distributions, and provides fitting functions
which calculate maximum likelihood estimates for univariate
and bivariate maxima models, and for univariate and
bivariate threshold models.

Functions for extreme value theory, which may be
divided into the following groups; exploratory data analysis,
block maxima, peaks over thresholds (univariate and bivariate),
point processes, gev/gpd distributions.

Monte Carlo and MCMC goodness of fit tests for log-linear models

This package computes the exact distribution
of some maximally selected statistics in the following setting: the 'response'
variable is binary, the splitting variable may be
nominal, ordinal or continuous.
Currently, the package implements the chi-square statistic and the Gini-index.

Computes exact conditional p-values and quantiles using an
implementation of the Shift-Algorithm by Streitberg & Roehmel.

The package provides various statistical methods for
designing and analyzing randomized experiments. One main functionality
of the package is the implementation of randomized-block and
matched-pair designs based on possibly multivariate pre-treatment
covariates. The package also provides the tools to analyze various
randomized experiments including cluster randomized experiments,
randomized experiments with noncompliance, and randomized experiments
with missing data.

Uses Stuart Coles' (Coles, Stewart, "An introduction to
statistical modeling of extreme values", Springer-Verlag, London 2001)
S-plus functions as ported to the R programming language (ismev) by Alec
Stephenson (http://www.maths.lancs.ac.uk/~stephena/).  This toolkit provides
a Graphical User Interface (GUI) to ismev for pedagogical purposes.

an R package for exploratory data analysis

Data and functions for the book "Multivariate Statistical
Modelling Based
on Generalized Linear Models", version 1, by
Ludwig Fahrmeir and Gerhard Tutz

This package estimates factor analysis models using a genetic algorithm, which  makes opens up a number of new ways to pursue old ideas, such as those discussed by Allen  Yates in his 1987 book Multivariate Exploratory Data Analysis. The major sources of value added in this package are new ways to transform factors in exploratory factor analysis, and perhaps more importantly, a new estimator for the factor analysis model called semi-exploratory factor analysis.

Includes FAME storage and retrieval function, as well as functions and S3 classes for time indexes and time indexed series, which are compatible with FAME frequencies.

Books are "Practical Regression and ANOVA in R" on CRAN, "Linear Models with R" published in August 2004 by CRC press and "Extending the Linear Model with R" published by CRC press in December 2005.

Modelizations and previsions functions for
Functional AutoRegressive processes using
nonparametric methods: functional kernel,
estimation of the covariance operator in
a subspace, ...

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Implementation of FastICA algorithm to perform
Independent Component Analysis (ICA) and Projection Pursuit.

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

These functions were developed to support
functional data analysis as described in Ramsay, J. O.
and Silverman, B. W. (2005) Functional Data Analysis.
New York: Springer.  They were ported from earlier
versions in Matlab and S-PLUS.  A manual that describes
the use of these functions is available in this
library. The library also contains a number of the
data sets used in the book along with R code for the
analyses that produced many of the figures in the book.

Functions for calculating fractal dimension.

This package allows to estimate both tail area-based false
discovery rates (Fdr) as well as local false discovery rates (fdr) for a
variety of null models (p-values, z-scores, correlation coefficients,
t-scores).  The proportion of null values and the parameters of the null
distribution are adaptively estimated from the data.  In addition, the package
contains functions for non-parametric density estimation (Grenander estimator),
for monotone regression (isotonic regression and antitonic regression with weights),
for computing the greatest convex minorant (GCM) and the least concave majorant (LCM),
and for the half-normal and correlation distributions.

Feature significance for multivariate kernel density estimation

Environment for teaching "Financial Engineering and Computational Finance"

Plot and summarize results calculated by the modeling environment FEMME (Soetaert, 2002)

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Flat file database designed for large vectors and multi-dimensional arrays

This package performs general linear modeling with multiple
responses (MANCOVA).  An overall p-value for each model term is calculated
by the 50-50 MANOVA method, which handles collinear responses.  Rotation
testing is used to compute adjusted single response p-values according to
familywise error rates and false discovery rates.

Bi-variate data fitting is done by two ingredients: the margins and the dependency structure. The dependency structure is modeled through a copula. An algorithm was implemented considering seven families of copulas  (Generalized Archimedean Copulas), the best fitting can be obtained looking all copula's options. Different dependence's kind was considered (totally positive of order 2 and stochastically increasing).

Environment for teaching "Financial Engineering and Computational Finance"

MLE for H parameter in FGN; MLE for regression with FGN error; simulation of FGN

This package provides routines for simulate random fields.

Fields is for curve, surface and function
fitting with an emphasis on splines, spatial data and spatial statistics. The
major methods include cubic, robust, and thin plate splines,
multivariate Kriging and Kriging for large data sets. One main feature is any
covariance function implemented in R can be used for spatial prediction.
There are also useful functions for plotting and working with
spatial data as images. This package also contains an implementation of a
sparse matrix  methods for large data sets and currently requires the
sparse matrix (spam) package for testing (but not for the standard spatial
functions.) Use help(fields) to get started and for an overview.

Simple key-value database

Simple key-value database using SQLite as the backend

Environment for teaching "Financial Engineering and Computational Finance"

Time value of money, cash flows and other financial functions.

This package contains functions to manipulate binary fingerprints
of arbitrary length. A fingerprint is represented by an object of S4 class 'fingerprint'
which is internally represented a vector of integers, such
that each element represents the position in the fingerprint that is set to 1.
The bitwise logical functions in R are overridden so that they can be used directly
with 'fingerprint' objects. A number of distance metrics are also
available (many contributed by Michael Fadock). Fingerprints
can be converted to Euclidean vectors (i.e., points on the unit hypersphere)i and
can also be folded using XOR.  Arbitrary fingerprint formats can be handled via line
handlers. Currently handlers are provided for CDK, MOE and BCI fingerprint data.

R companion to Tsay (2005)
Analysis of Financial Time Series, 2nd ed. (Wiley).
Includes data sets, functions and script files
required to work some of the examples.  Version 0.2-x
includes R objects for all data files used in the text
and script files to recreate most of the analyses in
chapters 1 and 2 plus parts of chapters 3 and 11.

This is the implementation in R+C of a fuzzy inference engine. It combines the speed of compiled C/C++ code, and the versatility of R in stadistics and graphical representation. suports several inference methods

The main function kcca implements a general framework for
k-centroids cluster analysis supporting arbitrary
distance/similarity measures and centroid computation. Further
cluster methods include hard competitive learning, neural gas and QT
clustering.

FlexMix implements a general framework for finite
mixtures of regression models using the EM algorithm.
FlexMix provides the E-step and all data handling, while the M-step can be
supplied by the user to easily define new models. Existing drivers
implement mixtures of standard linear models, generalized linear
models and model-based clustering.

Contains data sets from Bernard Flury (1997) A First Course in Multivariate Statistics, Springer NY

The library contains R-functions to perform an fmri analysis
as described in
Tabelow, K., Polzehl, J., Voss, H.U., and Spokoiny, V.
Analysing fMRI experiments with structure adaptive smoothing procedures,
NeuroImage, 33:55-62 (2006).

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Functions and datasets for forecasting.

Functions for reading and writing data stored by statistical
packages such as Minitab, S, SAS, SPSS, Stata, Systat, ...,
and for reading and writing .dbf (dBase) files.

The statistical evaluation of DNA mixtures, DNA profile
match probability

These functions  provides simple wrappers around the Unix process
management API calls: fork, signal, wait, waitpid, kill, and _exit.  This
enables construction of programs that utilize and mange multiple concurrent
processes.  Commercial support for this package available from
Random Technologies, LLC &lt;http://www.random-technologies-llc.com&gt;

R Fortunes

Forward search approach to robust analysis in linear and generalized linear regression models.

Fuzzy and crisp fixed point cluster analysis
based on Mahalanobis distance and linear regression fixed point
clusters. Semi-explorative, semi-model-based clustering methods,
operating on n*p data, do not need prespecification of number of
clusters, produce overlapping clusters. Symmetric and asymmetric
discriminant projections separate
groups optimally, used to visualize the separation of
groupings, visual cluster validation. Clusterwise linear
regression by normal mixture modeling. Cluster validation statistics
for distance based clustering. Clusterwise cluster stability assessment.
DBSCAN clustering. Interface functions for many clustering methods
implemented in R.

Environment for teaching "Financial Engineering and Computational Finance"

Maximum likelihood estimation of the parameters of a fractionally
differenced ARIMA(p,d,q) model (Haslett and Raftery, Appl.Statistics, 1989).

The functions provided in FracSim perform simulation of one- and two-dimensional fractional and multifractional Lvy motions

Software to book in development entitled Fractal Time Series Analysis in S-PLUS and R, William Constantine and Donald B. Percival, Springer.

Fit a shared gamma frailty model and Cox proportional hazards model using a Penalized Likelihood
on the hazard function. Left truncated, censored data and strata (max=2) are allowed. Clustered and
recurrent survival times can be studied  (the  Andersen-Gill (1982) approach has been implemented
for recurrent events). An automatic choice of the smoothing parameter is possible using an approximated
cross-validation procedure.

Environment for teaching "Financial Engineering and Computational Finance"

This package analyses Fractional Factorial designs with 2-level factors; it is meant for completely aliased designs only, i.e. e.g. not for analysing Plackett-Burman designs with interactions. The package enables convenient main effects and interaction plots for all factors simultaneously and offers a cube plot for looking at the simultaneous effects of three factors. An enhanced DanielPlot function (modified from BsMD) is provided. Furthermore, the alias structure for Fractional Factorial 2-level designs is output in a more readable format than with the built-in function alias.

Environment for teaching "Financial Engineering and Computational Finance"

Fuzzy set ordination is a multivariate analysis
used in ecology to relate the composition of samples to
possible explanatory variables.  While differing in theory
and method, in practice, the use is similar to "constrained
ordination."
The package contains plotting and summary
functions as well as the analyses

The package contains R-functions to perform the
methods in nonparametric regression and
density estimation, described in
Davies, P. L. and Kovac, A. (2001) Local Extremes, Runs, Strings and Multiresolution (with discussion) Annals of Statistics. 29. p1-65
Davies, P. L. and Kovac, A. (2004) Densities, Spectral Densities and Modality Annals of Statistics. Annals of Statistics. 32. p1093-1136
Kovac, A. (2006) Smooth functions and local extreme values. Computational Statistics and Data Analysis (to appear)
D\"umbgen, L. and Kovac, A. (2006) Extensions of smoothing via taut strings
Davies, P. L. (1995) Data features. Statistica Neerlandica 49,185-245.

Environment for teaching "Financial Engineering and Computational Finance"

FunCluster performs a functional analysis of microarray expression
data based on Gene Ontology & KEGG functional annotations. From
expression data and functional annotations FunCluster builds classes
of putatively co-regulated biological processes through a specially
designed clustering procedure.

Environment for teaching "Financial Engineering and Computational Finance"

Environment for teaching "Financial Engineering and Computational Finance"

Exact calculation of fuzzy decision rules for multiple
testing. Choose to control FDR (false discovery rate) using the
Benjamini and Hochberg method, or FWER (family wise error rate)
using the Bonferroni method. Kulinsakaya and Lewin (2007).

see title

Frankel-Wei regression and structural change tools
for estimating, testing, dating and monitoring
(de facto) exchange rate regimes.

G1DBN performs DBN inference using 1st order conditional dependencies.

A group of sample points are evaluted against a user-defined expression,
the sample points are lists of parameters with values that may be substituted into
that expression. The genetic algorithm attmepts to make the result of the expression
as low as possible (usually this would be the sum of residuals squared).

Data sets and scripts used in the book "Generalized
Additive Models: An Introduction with R", Wood (2006) CRC.

This package provides routines for fitting generalized additive models by likelihood based boosting, using penalized B-splines

Functions for fitting and working with generalized
additive models, as described in chapter 7 of "Statistical Models in
S" (Chambers and Hastie (eds), 1991), and "Generalized Additive
Models" (Hastie and Tibshirani, 1990).

This is an add on package to GAMLSS. The purpose of this package is to allow users to fir censored data in
GAMLSS models. The main function gen.cens() generates censored version of an existing GAMLSS family distribution.

This package contains few extra distributions for GAMLSS modelling.

The main GAMLSS library and datasets.

The main purpose of this package is to allow fitting of mixture distributions with GAMLSS models.

This is an add on package to GAMLSS. It allows one extra method for fitting
GAMLSS models. The main function nlgamlss() can fit any parametric
(up to four parameter) GAMLSS distribution.

This is an add on package to GAMLSS. The purpose of this package is to allow users to defined truncated distributions in  GAMLSS models. The main function gen.trun() generates truncated version of an existing GAMLSS family distribution.

This is an integrated package for genetic data analysis
of both population and family data. Currently it contains
functions for sample size calculations of both population-based
and family-based designs, classic twin ACE/ADE/AE/CE models,
probability of familial disease aggregation, kinship calculation,
some statistics in linkage analysis, and association analysis
involving one or more genetic markers including haplotype
analysis with or without environmental covariates.

This package performs non-parametric regression
when covariates are measured with error. The models
are estimated using gradient boosted regression trees.
Regression is performed using squared error loss, while binary
response regression can be performed using negative log-likelihood loss.

This package implements extensions
to Freund and Schapire's AdaBoost algorithm and Friedman's
gradient boosting machine. Includes regression methods for least
squares, absolute loss, quantile regression, logistic, Poisson,
Cox proportional hazards partial likelihood, and AdaBoost
exponential loss.

gcl  is a function that computes a fuzzy rules
classifier [Vinterbo, S.A.; Kim, E. & Ohno-Machado, L. Small, fuzzy and
interpretable  gene expression based classifiers. Bioinformatics, 2005,
21, 1964-1970] given input data as the data  frame  or  matrix  mydata.
gcl  returns  another  R function that implements the found classifier.

Orders panels in scatterplot matrices and parallel coordinate
displays by some merit index. Package contains various indices of merit,
ordering functions, and enhanced versions of pairs and parcoord which
color panels according to their merit level.

Parameters estimation of the general semiparametric model for recurrent event
data proposed by Pea and Hollander.

Create and maintain delayed-data packages (DDP's).  Data stored in
a DDP are available on demand, but do not take up memory until requested.
You attach a DDP with g.data.attach(), then read from it and assign to it in
a manner similar to S-Plus, except that you must run g.data.save() to
actually commit to disk.

Various R programming tools for data manipulation

Platform and X11 independent device for creating bitmaps (png, gif and jpeg).

Generalized Estimation Equation solver

Generalized estimating equations solver for parameters in mean, scale, and correlation structures, through mean link, scale link, and correlation link. Can also handle clustered categorical responses.

Running macroevolutionary simulation, and estimating parameters related to diversification from comparative phylogenetic data.

a package for genome-wide association analysis between
quantitative or binary traits and single-nucleiotide
polymorphisms (SNPs).

R based genetic algorithm for binary and floating point
chromosomes.

The GeneCycle package implements the approaches of Wichert et al.
(2004) and Ahdesmaki et al. (2005) for detecting periodically
expressed genes from gene expression time series data.

This package implements several generalized F-statistics. The current version includes a generalized F-statistic based on the flexible isotonic/monotonic regression or order restricted hypothesis testing.

Detection of spatial structure from geo-referenced genetic data.

GeneNet is a package for analyzing gene expression
(time series) data with focus on the inference of gene networks.
In particular, GeneNet implements the methods of Schaefer and
Strimmer (2005a,b,c) and Opgen-Rhein and Strimmer (2006, 2007)
for learning large-scale gene association networks (including
assignment of putative directions).

The package implements a two-stage algorithm to screen co-expressed
gene pairs with controlled False Discovery Rate (FDR) and Minimum Acceptable Strength
(MAS). The packages also constructs relevance networks and clusters co-expressed
genes (both similarly co-expressed and transitively co-expressed). In addtion, it
includes new correlation estimators and non-parametric statistical inference procedure
for replicated gene microarray data.

Classes and methods for handling genetic data. Includes
classes to represent genotypes and haplotypes at single markers up
to multiple markers on multiple chromosomes. Function include
allele frequencies, flagging homo/heterozygotes, flagging carriers
of certain alleles, estimating and testing for Hardy-Weinberg
disequilibrium, estimating and testing for linkage disequilibrium,
...

With the present version, GeneTS has become a meta-package that
exists mainly for reasons of backward compatibility. Its only purpose
is to load the GeneCycle and GeneNet packages.You may load
either of these packages directly.

Computes generalised KDEs

Set of data for use in package GEOmap.  Includes world map, USA map, Coso map, Japan Map, ETOPO5, ETOPO2.

Set of routines for making Map Projections (forward and inverse), Topographic Maps, Perspective plots, Geological Maps, geological map symbols, geological databases, interactive plotting and selection of focus regions.

This package makes the qhull library (www.qhull.org) available in R,
in a similar manner as in Octave and MATLAB. Qhull computes convex hulls,
Delaunay triangulations, halfspace intersections about a point, Voronoi diagrams,
furthest-site Delaunay triangulations, and furthest-site Voronoi diagrams.
It runs in 2-d, 3-d, 4-d, and higher dimensions. It implements the Quickhull
algorithm for computing the convex hull. Qhull does not support constrained
Delaunay triangulations, or mesh generation of non-convex objects, but the
package does include some R functions that allow for this. Currently the package
only gives access to Delaunay triangulation and convex hull computation.

Functions for inference in generalised linear spatial models. The posterior and predictive inference is based on Markov chain Monte Carlo methods. Package geoRglm is an extension to the package geoR, which must be installed first.

Geostatistical analysis including traditional, likelihood-based and Bayesian methods.

GeoXp is a tool for researchers in spatial statistics, spatial econometrics, geography, ecology etc allowing to link dynamically statistical plots with elementary maps. This coupling consists in the fact that the selection of a zone on the map results in the automatic highlighting of the corresponding points on the statistical graph or reversely the selection of a portion of the graph results in the automatic highlighting of the corresponding points on the map. GeoXp includes tools from different areas of spatial statistics including geostatistics as well as spatial econometrics and point processes. Besides elementary plots like boxplots, histograms or simple scatterplos, GeoXp also couples with maps Moran scatterplots, variogram cloud, Lorentz Curves,...In order to make the most of the multidimensionality of the data, GeoXp includes some dimension reduction techniques such as PCA.

The package allows geometric objects defined in geozoo to be displayed in GGobi through the use of rggobi.

Functions for fitting Gaussian Markov models.

An implementation of the grammar of graphics in R. It combines the advantages of both base and lattice graphics: conditioning and shared axes are handled automatically, and you can still build up a plot step by step from multiple data sources. It also implements a sophisticated multidimensional conditioning system and a consistent interface to map data to aesthetic attributes. See the ggplot2 website for more information, documentation and examples.

An implementation of the grammar of graphics in R. It combines the advantages of both base and lattice graphics: conditioning and shared axes are handled automatically, and you can still build up a plot step by step from multiple data sources. It also implements a more sophisticated multidimensional conditioning system and a consistent interface to map data to aesthetic attributes. See \url{http://had.co.nz/ggplot/} for more information, documentation and examples.

This package provides all about univariate and multivariate generalized hyperbolic distributions and its special cases (Hyperbolic, Normal Inverse Gaussian, Variance Gamma and skewed Student-t distribution). Especially fitting procedures, computation of the density, quantile, probability, random variates, expected shortfall and some portfolio optimization and plotting routines. In addition the generalized inverse gaussian distribution is contained in this package.

GillespieSSA provides a simple to use, intuitive, and extensible interface to several stochastic simulation algorithms for generating simulated trajectories of finite population continuous-time model. Currently it implements Gillespie's exact stochastic simulation algorithm (Direct method) and several approximate methods (Explicit tau-leap, Binomial tau-leap, and Optimized tau-leap). The package also contains a library of template models that can be run as demo models and can easily be customized and extended. Currently the following models are included, decaying-dimerization reaction set, linear chain system, logistic growth model, Lotka predator-prey model, Rosenzweig-MacArthur predator-prey model, Kermack-McKendrick SIR model, and a metapopulation SIRS model.}

Supply classes and methods to represent and manipulate graphs

Graphical lasso

The fitting algorithms considered in this package have two major objectives. One is to provide a smoothing device to fit distributions to data using the weight and unweighted discretised approach based on the bin width of the histogram. The other is to provide a definitive fit to the data set using the maximum likelihood estimation. Diagnostics on goodness of fit can be done via qqplots, KS-resample tests and comparing mean, variance, skewness and kurtosis of the data with the fitted distribution.

The generalised lambda distribution, or Tukey lambda distribution,
provides a wide variety of shapes with one functional form.  This
package provides random numbers, quantiles, probabilities, densities
and plots.  It also includes an implementation of the starship
estimation method for the distribution.

Routines for log-linear models of incomplete contingency tables,
including some latent class models, via EM and Fisher scoring
approaches.  Allows bootstrapping.

Fits generalized linear models where the parameters are subject to linear constraints. The model is specified by giving a symbolic description of the linear predictor, a description of the error distribution, and a matrix of constraints on the parameters.

Later

A Maximum Likelihood and bootstrap approach to mixed models

A path-following algorithm for L1 regularized generalized linear models and Cox proportional hazards model

The GNU Linear Programming Kit (GLPK) version 4.8.  This interface mirrors the GLPK C API.  Almost all GLPK lpx routines are supported.

Various R programming tools for model fitting

Multiple Precision Arithmetic (big integers and rationals, prime number tests, matrix computation),
"arithmetic without limitations" using the C library gmp.

Interface between the GMT map-making software and R,
enabling the user to manipulate geographic data within R and call GMT commands
to draw and annotate maps in postscript format. The 'gmt' package is about
interactive data analysis, rapidly visualizing subsets and summaries of
geographic data, while performing statistical analysis in the R console.

Simulate data sets given a dependence model,
validate graphical models using the bootstrap,
find the best prediction model using cross validation.

Functions to specify and fit generalized nonlinear models, including models with multiplicative interaction terms such as the UNIDIFF model from sociology and the AMMI model from crop science, and many others.  Over-parameterized representations of models are used throughout; functions are provided for inference on estimable parameter combinations, as well as standard methods for diagnostics etc.

Provides a console for R under Unix-alikes based on GNOME

A collection of functions to solve weighted and lexicographical
goal programming problems as specified by Lee (1972) and Ignizio (1976).

This package implements several functions useful for
computing similarities between GO terms and gene products based on their GO annotation

Gradient Projection Algorithm Rotation for Factor Analysis. See ?GPArotation.Intro for more details.

General polygon clipping routines for R based on Alan Murta's C library

Various R programming tools for plotting data

Classification using generalized partial least squares
for two-group and multi-group (more than 2 group)
classification.

A package for probability propagation in graphical independence networks.

This small collection of functions provides distinctive graphics for display of anova
results. The two principal functions are granova.1w (a graphic for one way anova) and
granova.2w (a corresponding graphic for two way anova). These functions were written to display
data for any number of groups, regardless of their sizes (however, very large data sets or numbers of
groups are likely to be problematic). For these two functions a specialized approach is used
to construct data-based contrast vectors with respect to which anova data are displayed.
The result is that the graphics use straight lines, and flat surfaces, to help ensure clear
interpretations while being faithful to the standard effect tests in anova; the graphic
results are complementary to standard summary tables for these two basic kinds of analysis of variance.
Two additional functions are granova.ds (for comparing two dependent samples of data), and granova.contr
(which provides graphic displays for a priori contrasts). All functions provide relevant
numerical results to supplement the graphic displays of anova data.
The graphics based on these functions are aimed at students and non-statistician analysts; but they can
be generally helpful for identifying outliers, clusters, trends or the role of non-linear
transformations of data. In the case of granova.1w and granova.ds especially, several arguments have been
provided to facilitate construction of graphics that accommodate diverse features of data, and their
corresponding display requirements.

A package that implements some simple graph handling
capabilities.

GRASP is a general method for making spatial predictions of response variables (RV) using point surveys of the RV and spatial coverages of Predictor variables (PV). Originally, GRASP was developed to analyse, model and predict vegetation distribution over New Zealand. It has been used in all sorts of applications since then. (A. Lehmann, J.R. Leathwick & J.McC. Overton, 2002. GRASP. Ecological Modelling, 157: 189-207)

Interface between GRASS 5.0 geographical information system and R,
based on starting R from within the GRASS environment using values
of environment variables set in the GISRC file. Interface examples
should be run outside GRASS, others may be run within. Wrapper and
helper functions are provided for a range of R functions to match the
interface metadata structures.

This package defines S3-classes gmData (graphical meta data), gModel (graphical model). Several other graphical modelling packages rely on these fundamental constructs. gRbase illustrates how hierarchical log-linear models (hllm) may be implemented.

Estimation, model selection and other aspects of statistical inference in Graphical Gaussian models with edge and vertex symmetries (Graphical Gaussian models with colours)

The former gregmisc bundle is a repository for a variety of
useful functions.  The gregmisc package was recently split into
a set of more focused packages: gdata, gmodels, gplots, gtools.
The purpose of this 'new' gregmisc is to provide an easy
way to access the original combined functionality.  To this
end, it simply depends on all of the new packages so that
these will installed/loaded when this package is installed/loaded.

Integration of base and grid graphics

Functions for converting, importing, and drawing PostScript
pictures in R plots.

grnnR synthesizes a generalized regression neural network
from the supplied training data, P(atterns) and T(argets)

Regression models for grouped and coarse data, under the Coarsened At Random assumption.

The computations are done via the alpha spending approach i.e. interim
analyses need not to be equally spaced, and their number need not to be specified in advance.

Fits user specified models with Group Lasso penalty

Gene set analysis


An R wrapper for the special functions and quasi random number
generators of the Gnu Scientific Library
(http://www.gnu.org/software/gsl/).  See gsl-package.Rd for details of
overall package organization, and Misc.Rd for some functions that are
widely used in the package.

This package implements a Bayesian approach for estimation of a mixture of gamma distributions in which the mixing occurs over the shape parameter. This family provides a flexible and novel approach for modeling heavy-tailed distributions, it is computationally efficient, and it only requires to specify a prior distribution for a single parameter.

A comprehensive package for structural multivariate
function estimation using smoothing splines.

variogram modelling; simple, ordinary and universal point
or block (co)kriging, sequential Gaussian or indicator (co)simulation;
variogram and variogram map plotting utility functions.

gsubfn is like gsub but can take a replacement function
or proto object rather than a replacement string.  Matches and
back references are input to the function and replaced by the
function output.  In the case of a proto object the object is
used for sharing information from one match to the next.
gsubfn can be used to split strings based on content rather than
delimiters and for quasi-perl-style string interpolation. The
package also has facilities for translating formulas to functions
and allowing such formulas in function calls instead of functions.
This can reduce code size and improve clarity when using apply,
lapply, optim, integrate, xyplot and any other function that
expects another function as an input argument.  gsubfn also
provides for quasi-perl string interpolation in arguments
of any existing R function.

GTK graphics device driver that may be used independently of the R-GNOME interface
and can be used to create R devices as embedded components in a GUI using a Gtk drawing area widget, e.g. using RGtk.

Various R programming tools

Methods from the paper: Pena, EA and Slate, EH, "Global Validation of Linear Model Assumptions," J. American Statistical Association, 101(473):341-354, 2006.

gWidgets provides a toolkit-independent API for building interactive GUIs. Atleast one of the gWidgetsXXX packages, such as gWidgetstcltk, needs to be installed. Some icons are on loan from the scigraphica project http://scigraphica.sourceforge.net.

Port of gWidgets API to RGtk2

Port of gWidgets API to the rJava interface between R and Java.

Port of gWidgets API to the tcltk package

The following R functions are used for likelihood inference of trait associations with haplotypes and other covariates in generalized linear models.  The functions accommodate uncertain haplotype phase and can handle missing genotypes at some SNPs.

'haplo.ccs' estimates haplotype and covariate relative risks in case-control data by weighted logistic regression. Diplotype probabilities, which are estimated by EM computation with progressive insertion of loci, are utilized as weights.

A suite of S-PLUS/R routines for the analysis of indirectly measured haplotypes. The statistical methods assume that all subjects are unrelated and that haplotypes are ambiguous (due to unknown linkage phase of the genetic markers). The main functions are:  haplo.em, haplo.glm and haplo.score.

Package for haplotype data simulation. Haplotypes are generated such that their
allele frequencies and linkage disequilibrium coefficients match those estimated from an input data set.

Package HardyWeinberg is a package for exploring
bi-allelic marker data. It focuses on the graphical
representation of the results of tests for Hardy-Weinberg equlibrium
in a ternary plot. Routines for several tests for Hardy-Weinberg equilibrium
are included in the package.

Calculate expected relative risk and proportion protected assuming normally distributed log10 transformed antibody dose for several component vaccine. Uses Hill models for each component which are combined under Bliss independence.

Cross-validated linear discriminant calculations determine
the optimum number of features. Test and training scores from
successive cross-validation steps determine, via a principal
components calculation, a low-dimensional global space onto which test
scores are projected, in order to plot them. Further functions are
included that serve didactic purposes.

A flexible and hierarchical framework for comparing categorical map composition and configuration (spatial pattern) along spatial, thematic, or external grouping variables.  Comparisons are based on measures of mutual information between thematic classes (colours) and location (spatial partitioning).  Results are returned in textual, tabular, and graphical forms.

Interface to the NCSA HDF5 library

Computation of highest density regions in one and two dimensions,
kernel estimation of univariate density functions conditional on one covariate,
and multimodal regression.

Allows heatmap matrix to have non-identical X- and Y-dimensions.  Allows multiple tracks of annotation for RowSideColors and ColSideColors.

Represents sums-of-squares-and-products matrices for linear hypotheses and for error using ellipses (in two dimensions) and ellipsoids (in three dimensions).

Functions for the fitting and summarizing of heteroscedastic t-regression.

Functions to view files in raw binary form like in a hex editor.  Additional functions to specify and read arbitrary binary formats.

package containing functions for excel connections and string matching
and passing by reference

Support software for Statistical Analysis and Data
Display (Springer, ISBN 0-387-40270-5).  This contemporary
presentation of statistical methods features extensive use of
graphical displays for exploring data and for displaying the
analysis. The authors demonstrate how to analyze data---showing
code, graphics, and accompanying computer listings---for all the
methods they cover. They emphasize how to construct and
interpret graphs, discuss principles of graphical design, and
show how accompanying traditional tabular results are used to
confirm the visual impressions derived directly from the
graphs. Many of the graphical formats are novel and appear
here for the first time in print. All chapters have exercises.

Contains functions for the analysis of Discrete Time Hidden Markov Models, Markov Modulated GLMs and the Markov Modulated Poisson Process. It includes functions for simulation, parameter estimation, and the Viterbi algorithm. See the topic \code{\link{Overview}} for an introduction to the package, and \code{\link{changes}} for a list of recent changes. The algorithms are based of those of Walter Zucchini.

This R package allows the estimation of hierarchical F-statistics from haploid or diploid genetic data with any numbers of levels in the hierarchy, following the algorithm of Yang (Evolution, 1998, 52(4):950-956). Functions are also given to test via randomisations the significance of each F and variance components, using the likelihood-ratio statistics G -see Goudet etal (Genetics, 1996, 144(4): 1933-1940)

Variance partition of a multivariate data set

HighProbability provides a simple, fast, reliable solution to the multiple testing problem. Given a vector of p-values or achieved significance levels computed using standard frequentist inference, HighProbability determines which ones are low enough that their alternative hypotheses can be considered highly probable. The p-value vector may be determined using existing R functions such as t.test, wilcox.test, cor.test, or sample. HighProbability can be used to detect differential gene expression and to solve other problems involving a large number of hypothesis tests.

Simulation from distributions supported by nested hyperplanes,
using the algorithm described in Petris & Tardella, "A geometric approach to
transdimensional Markov chain Monte Carlo", Canadian Journal of Statistics,
v.31, n.4, (2003).
Also random direction multivariate Adaptive Rejection Metropolis Sampling.

Gives hints on what functions you might want to apply to an object you have created.

The Hmisc library contains many functions useful for data
analysis, high-level graphics, utility operations, functions for
computing sample size and power, importing datasets,
imputing missing values, advanced table making, variable clustering,
character string manipulation, conversion of S objects to LaTeX code,
and recoding variables.  Please submit bug reports to
'http://biostat.mc.vanderbilt.edu/trac/Hmisc'.

Fits hidden Markov models with discrete non-parametric
observation distributions to data sets.  Simulates data
from such models.

Various functions for higher order likelihood-based
inference

This package performs a homogeneity analysis, aka a multiple correspondence
analysis, but with many additional options. Variables can be grouped into
sets, in order to emulate regression analysis and canonical analysis. For
each variable rank constraints on the category and quantifications (or transformations) and level constraints (which allows one to treat a variable as nominal, ordinal, or numerical) can be imposed.

A collection of homogeneity tests described in:
Viglione A., Laio F., Claps P. (2007) "A comparison of homogeneity tests for regional frequency analysis", Water Resources Research, 43, W03428, doi:10.1029/2006WR005095. More on Regional Frequency Analysis can be found in package \code{nsRFA}.

The HOPACH clustering algorithm builds a hierarchical tree
of clusters by recursively partitioning a data set, while ordering
and possibly collapsing clusters at each level. The algorithm uses
the Mean/Median Split Silhouette (MSS) criteria to identify the
level of the tree with maximally homogeneous clusters. It also runs
the tree down to produce a final ordered list of the elements. The
non-parametric bootstrap allows one to estimate the probability that
each element belongs to each cluster (fuzzy clustering).

Computation on micro-arrays

When testing multiple hypotheses simultaneously,
this package provides functionality to calculate a lower bound for the
number of correct rejections (as a function of the number of rejected
hypotheses), which holds simultaneously -with high probability- for all
possible number of rejections.
As a special case, a lower bound for the total number of false null
hypotheses can be inferred.
Dependent test statistics can be handled for multiple tests of
associations. For independent test statistics, it is sufficient to
provide a list of p-values.

Functions, data sets, analyses and examples from the book
'A Handbook of Statistical Analyses Using R' (Brian S. Everitt and Torsten
Hothorn, Chapman & Hall/CRC, 2006). The first chapter
of the book, which is entitled 'An Introduction to R',
is completely included in this package, for all other chapters,
a vignette containing all data analyses is available.

A package for computation of hidden semi markov models

Functions inserting dynamic scatterplots and grids in documents generated by R2HTML. To view the applets you must install a Java plugin (http://java.sun.com/products/plugin/). The plot applet is a modified version of PtPlot 5.3 (http://ptolemy.eecs.berkeley.edu/).

HTTP Request protocols. Implements the GET, POST and multipart POST request.

Fits models for genotypic disequilibria, as described in
Huttley and Wilson (2000), Weir (1996) and Weir and Wilson (1986).
Contrast terms are available that account for first order interactions
between loci.
Also implements, for a single locus in a single population, a conditional
exact test for Hardy-Weinberg equilibrium

hybrid hierarchical clustering via mutual clusters

Contains one function for drawing Piper (also called Piper-Henn) digrammes from water analysis for major ions.

This package estimates the parameters in infiltration and
water retention models by curve-fitting method. The models
considered are those that are commonly used in soil science.

Hydrosanity provides a graphical user interface for exploring hydrological
time series. It is designed to work with catchment surface hydrology data
(mainly rainfall and streamflow time series at a set of locations).
There are functions to import from a database or files;
summarise and visualise the dataset in various ways;
estimate areal rainfall;
fill gaps in rainfall data;
and estimate the rainfall-runoff relationship.
Probably the most useful features are the interactive graphical displays of
a spatial set of time series.
WARNING: this package is under development and should not be considered stable.
An introductory paper is included, but there is not much detailed documentation.
Hydrosanity's Graphical User Interface was based on Rattle by Graham Williams.

This package provides functions for the hyperbolic and related
distributions. Density, distribution and quantile functions and
random number generation are provided for the hyperbolic distribution,
the generalized hyperbolic distribution, the generalized inverse
Gaussian distribution and the skew-Laplace distribution. Additional
functionality is provided for the hyperbolic distribution, including
fitting of the hyperbolic to data.

A method to test genetic linkage with covariates by regression methods with response IBD sharing for relative pairs.  Account for correlations of IBD statistics and covariates for relative pairs within the same pedigree.

Provides native Raccess to Interactive Brokers Trader Workstation API.

Kernel Estimators for Interval-Censored Data

Given two unbiased samples of patient level data on cost and effectiveness
for a pair of treatments, make head-to-head treatment comparisons by (i) generating the
bivariate bootstrap resampling distribution of ICE uncertainty for a specified value of
the shadow price of health, lambda, (ii) form the wedge-shaped ICE confidence region with
specified confidence fraction within [0.50, 0.99] that is equivariant with respect to
changes in lambda, (iii) color the bootstrap outcomes within the above confidence wedge
with economic preferences from an ICE map with specified values of lambda, beta and gamma
parameters, (iv) display VAGR and ALICE acceptability curves, and (v) display indifference
(iso-preference) curves from an ICE map with specified values of lambda, beta and gamma or
eta parameters.

Many functions for computing the NPMLE for censored and
truncated data.

The package implements Tyler et al.'s and Oja et al.'s method of two different scatter
matrices to obtain an invariant coordinate system or independent
components, depending on the underlying assumptions.

Tools for multivariate nonparametrics, as location tests based on marginal ranks, spatial median and spatial signs computation, Hotelling's T-test, estimates of shape

Calculate identity coefficients, based on Mark Abney's C code.

The IDPmisc package contains different high-level graphics functions for displaying large datasets, displaying circular data in a very flexible way, finding local maxima, brewing color ramps, drawing nice arrows, zooming 2D-plots, creating figures with differently colored margin and plot region.  In addition, the package contains auxiliary functions for data manipulation like omitting observations with irregular values or selecting data by logical vectors, which include NAs. Other functions are especially useful in spectroscopy and analyses of environmental data: robust baseline fitting, finding peaks in spectra.

The package performes Independent Factor Analysis

Iterated Function Systems

Insightful Research Tools.

A collection of utilities for robust and classical versions of the inverse Gaussian distribution known as inverse Gaussian type distribution (IGTD).

Routines for simple graphs and network analysis. igraph can
handle large graphs very well and provides functions for generating random
and regular graphs, graph visualization, centrality indices and much more.

Testing whether data is independent and identically distributed

Imputation for microarray data (currently KNN only)

Inequality, concentration and poverty measures
Lorenz curves (empirical and theoretical)

implements a network partitioning algorithm to identify communities (or modules) in a network. A network plotting function then utilizes the identified community structure to position the vertices for plotting. The package also contains functions to calculate the assortativity and transitivity of a vertex.

Functionality to dynamically define R functions and S4 methods with in-lined C, C++ or Fortran code supporting .C and .Call calling conventions.

Implementation of ICM-Algorithm by Wei Pan, J. Comp. & Gr. Stat. 78: 109-120, 1999
Algorithm for the Cox proportional hazard model for interval censored data

Interactive plots for R

Improved predictive models by indirect classification and
bagging for classification, regression and survival problems
as well as resampling based estimators of prediction error.

Coefficients of Interrater Reliability and Agreement for
quantitative, ordinal and nominal data: ICC, Finn-Coefficient,
Robinson'A, Kendall's W, Cohen's Kappa, ...

Provides a simple common interface to the estimation of item parameters
in IRT models for binary responses with three different programs (ICL,
BILOG-MG, and ltm, and a variety of functions useful with IRT models.

Insieme di funzioni di supporto al volume "INTRODUZIONE
ALLA STATISTICA APPLICATA con esempi in R", Federico M. Stefanini,
PEARSON Education Milano, 2007.

Functions to support the computations carried out in
'An Introduction to Statistical Modeling of Extreme Values' by
Stuart Coles. The functions may be divided into the following
groups; maxima/minima, order statistics, peaks over thresholds
and point processes.

Data sets and scripts for text examples and exercises in
P. Dalgaard (2002), 'Introductory Statistics with R', Springer Verlag.

The its package contains an S4 class for handling irregular time series

The package ports JF Cardoso's JADE algorithm as well as his function for joint diagonalization. There are also several criteria for performance evaluation of ICA algorithms.

Graphics device routing all graphics commands to a Java
program. The actual functionality of the JavaGD depends
on the Java-side implementation. Simple AWT and Swing
implementations are included.

Java GUI for R - cross-platform, universal and unified Graphical User Interface for R. For full functionality on Windows and Mac OS X JGR requires a start application which depends on your OS. This can be downloaded from JGR's website: http://www.rosuda.org/JGR

This package implements the Lewbel and Linton (2003), Lewbel and Linton (2005) and Jacho-Chvez, Lewbel and Linton (2005) nonparametric estimators of Homothetic and Generalized Homothetic production functions.

Shared parameter models for the joint modelling of longitudinal and time-to-event data.

Some functions usefull to perform joint modelling of
Mean and Dispersion through two interlinked GLM's. This
package is outdated. We recommend the use of the
JointModeling package which generalizes this one.

Some functions usefull to perform joint modelling of Mean
and Dispersion through two interlinked GLM's or GAM's.

Calculates bias, responsiveness, and other characteristics of
two-party electoral systems, with district-level electoral and
other data.

Kappalab, which stands for "laboratory for capacities", is an S4 tool box for capacity (or non-additive measure, fuzzy measure) and integral manipulation on a finite setting. It contains routines for handling various types of set functions such as games or capacities. It can be used to compute several non-additive integrals: the Choquet integral, the Sugeno integral, and the symmetric and asymmetric Choquet integrals. An analysis of capacities in terms of decision behavior can be performed through the computation of various indices such as the Shapley value, the interaction index, the orness degree, etc. The well-known Mbius transform, as well as other equivalent representations of set functions can also be computed. Kappalab further contains seven capacity identification routines: three least squares based approaches, a method based on linear programming, a maximum entropy like method based on variance minimization, a minimum distance approach and an unsupervised approach grounded on parametric entropies. The functions contained in Kappalab can for instance be used in the framework of multicriteria decision making or cooperative game theory.

Computes the Kendall rank correlation

semi-parametric kernel-based approaches to local fdr estimations useful for the testing of multiple hypothesis (in large-scale genetic, genomic and post-genomic studies for instance).

This package creates a individual-based population genetic simulation.  Individuals have spatial coordiantes, dispersal governed by mixtures of Weibull and normal pdfs.  It is discrete-time.  It can be arbitrarily complex given enough known or assumed demographic characteristics.  Simulates diploid and haploid inheritance. Allows the development of null distributions of genotypes for complex demographic scenarios.

Kernel-based machine learning methods for classification, regression,
clustering, novelty detection, quantile regression and dimensionality reduction.
Among other methods kernlab includes Support Vector Machines,
Spectral Clustering, Kernel PCA and a QP solver.

functions for kernel smoothing (and density estimation)
corresponding to the book:
Wand, M.P. and Jones, M.C. (1995) "Kernel Smoothing".

Analysis of kin-cohort studies. kin.cohort provides estimates of age-specific
cumulative risk of a disease for carriers and noncarriers of a mutation. The cohorts are
retrospectively built from relatives of probands for whom the genotype is known. Currently
the method of moments and marginal maximum likelihood are implemented. Confidence intervals
are calculated from bootstrap samples.
Most of the code is a translation from previous matlab code by Chatterjee.

coxme: general mixed-effects Cox models; kinship: routines
to create and manipulate n by n matrices that describe the
genetic relationships between n persons; pedigree: create and
plot pedigrees; bdsmatrix: a class of objects for sparse
block-diagonal matrices (which is how kinship matrices are
stored); gchol: generalized cholesky decompositions

Weighted k-Nearest Neighbors Classification and Regression

Miscellaneous functions for classification and visualization
developed at the Fakultaet Statistik, Technische Universitaet Dortmund

The package implements efficient ways to evaluate and
solve equations of the form Ax=b, where A is a kronecker product of
matrices.  Functions to solve least squares problems of this type
are also included.

Computes various confidence intervals for the Kaplan-Meier estimator, namely: Petos CI, Rothman CI, CI's based on Greenwoods variance, Thomas and Grunkemeier CI and the simultaneous confidence bands by Nair and Hall and Wellner.

Data sets and functions for Klein and Moeschberger (1997),
"Survival Analysis, Techniques for Censored and Truncated Data", Springer.

This program scales categorical variables in such a way as to
make NN classification as accurate as possible. It also handles continuous
variables and prior probabilities, and does intelligent variable selection
and estimation of error rates and the right number of NN's.

Finds the p number of near neighbours for every point in a given dataset in O(M log M) time.

A KNN implementaion which allows continuous responses, the specification of the distance used to calculate nearest neighbors (euclidean, binary, etc.), the aggregation method used to summarize repsonse (majority class, mean, etc.) and the method of handling ties (all, random selection, etc.).

Construct or predict with k-nearest-neighbor classifiers,
using cross-validation to select k, choose variables (by forward
or backwards selection), and choose scaling (from among no scaling,
scaling each column by its SD, or scaling each column by its MAD).
The finished classifier will consist of a classification tree with
one such k-nn classifier in each leaf.

Supervised and unsupervised self-organising maps

Kernel density estimators and kernel discriminant analysis for multivariate data

Image Detection and Time Series breaks.

A colletion of functions to implement Kolmogorov-Zurbenko
Fourier transform based periodograms and smoothing methods

A collection of functions utilizng splines to construct a smooth estimate
of a signal buried in noise.

A variety of ordination and vegetation analyses
useful in analysis of datasets in community ecology.  Includes
many of the common ordination methods, with graphical
routines to facilitate their interpretation, as well as
several novel analyses.

Insieme di funzioni di supporto al volume
"Laboratorio di Statistica con R", Iacus-Masarotto,
MacGraw-Hill Italia, 2003.
This package contains sets of functions defined in
"Laboratorio di Statistica con R", Iacus-Masarotto,
MacGraw-Hill Italia, 2003. Function names and
docs are in italian as well.

The Lancet has published two surveys (Roberts et al (2004) and Burnham et al (2006)) on Iraq mortality before and after the US-led invasion. This package serves three purposes. First, it includes a portion of the summary data related to the first study, both the raw .xls file distributed by the authors and a cleaned up R dataframe. Second, it provides simple functions for working with data from the second study. The authors have distributed this data only to selected researchers. Those researchers can use this package to work with that data more easily. Third, for researchers without such access, we provide a vignette which serves as a guided tour of some of the more interesting aspects of the data.

Data sets exemplifying statistical methods, and some
facilitatory utility functions used in my forthcoming book
"Analyzing Linguistic Data: A practical introduction to
statistics using R", Cambridge University Press, in press.

Efficient procedures for fitting an entire lasso
sequence with the cost of a single least squares
fit. Least angle regression and infinitessimal forward
stagewise regression are related to the lasso, as
described in the paper below.

laser implements maximum likelihood methods based on the birth-death
process to test whether diversification rates have changed over time.  The package
permits batch processing of phylogenies to generate null distributions of test statistics
and posterior distributions of parameter estimates.  Additional functions for manipulating
branching times from molecular phylogenies and for simulating branching times under constant-rate models
of diversification are provided.

Routines and documentation for solving regression problems
while imposing an L1 constraint on the estimates, based on
the algorithm of Osborne et al. (1998)

This package implements the original specification in Handcock, Raftery and Tantrum (2007). This corresponds to version 0.7 of the original latentnet. The current package latentnet implements the new specification in Krivitsky and Handcock (2008), and represents a substantial rewrite of the original package.  This package is part of the "statnet" suite of packages.

A package to fit and simulate latent position and cluster models for statistical networks.

Extra graphical utilities based on lattice

Implementation of Trellis Graphics. See ?Lattice for a
brief introduction

A R software package on statistical tests widely
utilized in biostatistics, public policy and law.
Along with the well known tests for equality of means and variances,
randomness, measures of relative variability etc, the package contains new
robust tests of symmetry, omnibus and directional
tests of normality, and their graphical counterparts such as Robust QQ plot;
a robust trend tests for variances etc. All implemented tests and methods are
illustrated by simulations and real-life examples from legal statistics,
economics and biostatistics.

By combining constant, linear, and quadratic local models,
lazy estimates the value of an unknown multivariate function on
the basis of a set of possibly noisy samples of the function itself.
This implementation of lazy learning automatically adjusts the
bandwidth on a query-by-query basis through a leave-one-out
cross-validation.

Computations related to group sequential boundaries.
Includes calculation of bounds using the Lan-DeMets
alpha spending function approach.

R package for design of experiments for association
studies for detection of linkage disequilibrium. Uses an existing
deterministic power calculation for detection of linkage
disequilibrium between a bi-allelic QTL and a bi-allelic marker,
together with the Spiegelhalter and Smith Bayes factor to generate
designs with power to detect effects with a given Bayes factor.

Produces a graphical display, as a heat map, of measures of pairwise linkage disequilibria between SNPs. Users may optionally include the physical locations or genetic map distances of each SNP on the plot.

Regression subset selection including exhaustive search

LearnBayes contains a collection of functions helpful in learning the basic tenets of Bayesian statistical inference.  It contains functions for summarizing basic one and two parameter posterior distributions and predictive distributions.  It contains MCMC algorithms for summarizing posterior distributions defined by the user.  It also contains functions for regression models, hierarchical models, Bayesian tests, and illustrations of Gibbs sampling.

Tools for linear grouping analysis. Three user-level functions: gap, rlga and lga.

A very simple implementation of a class for longitudinal data.  See my paper in the DSC-2001 proceedings.

This package provides a number of methods for creating and augmenting Latin Hypercube Samples

This package can be used to solve Linear Programming / Linear
Optimization problems by using the simplex algorithm.

Fits and tests logistic joinpoint models.

The likelihood linkage analysis is a general agglomerative
hierarchical clustering method developed in France by Lerman in
a long series of research articles and books. Initially
proposed in the framework of variable clustering, it has been
progressively extended to allow the clustering of very general
object descriptions. The approach mainly consists in replacing
the value of the estimated similarity coefficient by the
probability of finding a lower value under the hypothesis of
'absence of link'. The package LLAhclust contains routines for
computing various types of probablistic similarity coefficients
between variables or object descriptions. Once the similarity
values between variables/objects are computed, a hierarchical
clustering can be performed using several probabilistic and
non-probabilistic aggregation criteria, and indices measuring
the quality of the partitions compatible with the resulting
hierarchy can be computed.

This is a new framework in which graph-structured data are used to train a classifier in a latent space, and then classify new nodes. During the learning phase, a latent representation of the network is first learned and a supervised classifier is then built in the learned latent space. In order to classify new nodes, the positions of these nodes in the learned latent space are estimated using the existing links between the new nodes and the learning set nodes.  It is then possible to apply the supervised classifier to assign each new node to one of the classes.

Fit linear and generalized linear mixed-effects models.

Add smoothing spline modelling capability to nlme. Fit smoothing
spline terms in Gaussian linear and nonlinear mixed-effects models

LMGene package for analysis of microarray data using a
linear model and glog data transformation in the R statistical
package.

Some improved procedures for linear mixed models

The package implements the statistical theory of L-moments including
L-moment estimation, probability-weighted moment estimation, parameter estimation
for numerous familiar and not-so-familiar distributions, and L-moment estimation
for the same distributions from the parameters. L-moments are derived from the
expectations of order statistics and are linear with respect to the probability-
weighted moments. L-moments are directly analogous to the well-known product
moments; however, L-moments have many advantages including unbiasedness,
robustness, and consistency with respect to the product moments. This package
is oriented around the FORTRAN algorithms of J.R.M. Hosking, and the
nomenclature for many of the functions parallels that of the Hosking library.
However, numerous extensions are made to aid in expand of the breadth and ease
of L-moment application. Much theoretical extension of L-moment theory has
occurred in recent years. E.A.H. Elamir and A.H. Seheult have developed the
trimmed L-moments, which are implemented in this package. Further, recent
developments by Robert Serfling and Peng Xiao have extended L-moments into
multivariate space; the so-called sample L-comoments are implemented here. The
supported distributions with moment type shown as L (L-moments) or TL (trimmed
L-moments) and additional support for right-tail censoring ([RC]) include:
Cauchy(TL), Exponential(L), Gamma(L), Generalized Extreme Value(L),
Generalized Lambda(L & TL), Generalized Logistic (L), Generalized Normal(L),
Generalized Pareto(L[RC] & TL), Gumbel(L), Normal(L), Kappa(L),
Pearson Type III(L), Reverse Gumbel(L[RC]), Wakeby(L), and Weibull(L).

The Lmoments package contains functions to estimate L-moments and trimmed L-moments from the data.
The package also contains functions to estimate the parameters of the normal polynomial quantile mixture and
the Cauchy polynomial quantile mixture from L-moments and trimmed L-moments.

A collection of tests, data sets and examples
for diagnostic checking in linear regression models.

Maximum likelihood estimation for clustered binary data

Computation of local false discovery rates

Local Regression, Likelihood and density estimation.

Computes local polynomial estimators.

Assorted plots of location score versus genetic map position

Computes the maximum likelihood estimator from an i.i.d. sample of data from a log-concave density in any number of dimensions.  Plots are available for 1- and 2-d data.

Given independent and identically distributed observations X(1), ..., X(n), this package allows to
compute a concave, piecewise linear function phi on [X(1), X(n)] with knots only in {X(1), X(2), ..., X(n)}
such that L(phi) = sum_{i=1}^n W(i)*phi(X(i)) - int_{X(1)}^{X(n)} exp(phi(x)) dx is maximal, for some
weights W(1), ..., W(n) s.t. sum_{i=1}^n W(i) = 1. According to the results in Duembgen and Rufibach (2006),
this function phi maximizes the ordinary log-likelihood sum_{i=1}^n W(i)*phi(X(i))
under the constraint that phi is concave. The corresponding  function exp(phi) is a log-concave probability density.
Two algorithms are offered: An active set algorithm and one based on the pool-adjacent-violaters algorithm.

Routines for Logic Regression

Analysis of sparse contingency tables with penalization approaches

Firth's bias reduced logistic regression approach with
penalized profile likelihood based confidence intervals for
parameter estimates.

Routines for the logspline density estimation. oldlogspline
uses the same algorithm as the logspline 1.0.x package - the Kooperberg
and Stone (1992) algorithm (with an improved interface). The recomended
routine logspline uses an algorithm from Stone et al (1997).

Kernel regression smoothing with adaptive local or global plug-in
bandwidth selection.

This package contains general data structures and
functions for longitudinal data with multiple variables,
repeated measurements, and irregularly spaced time points.
It also implements a shrinkage estimator of dynamical correlation
and dynamical covariance.

Datasets and Functionality from the textbook
Jan Beran (1994). Statistics for Long-Memory Processes; Chapman \& Hall.

Loop analysis makes qualitative predictions of variable change in a system of causally interdependent variabless, where "qualitative" means sign only (i.e. increases, decreases, non change, and ambiguous). This implementation includes output support for graphs in .dot file format for use with visualization software such as graphviz (graphviz.org). Loop Analyst provides tools for the construction and output of community matrices, computation and output of community effect matrices, tables of correlations, adjoint, absolute feedback, weighted feedback and weighted prediction matrices, change in life expectancy matrices, and feedback, path and loop enumeration tools.

This package contains routines and documentation for
solving quadratic programming problems where the hessian
is represented as the product of two matrices.

Local Polynomial Regression with Ridging.

Lp_solve is freely available (under LGPL 2) software for
solving linear, integer and mixed integer programs. In this implementation
we supply a "wrapper" function in C and some R functions that solve
general linear/integer problems, assignment problems, and transportation
problems. This version calls lp_solve version 5.5.

The basic idea of latent semantic analysis (LSA) is,
that text do have a higher order (=latent semantic) structure which,
however, is obscured by word usage (e.g. through the use of synonyms
or polysemy). By using conceptual indices that are derived statistically
via a truncated singular value decomposition (a two-mode factor analysis)
over a given document-term matrix, this variability problem can be overcome.

Implements the LS-PLS (least squares - partial least squares)
method described in for instance Jrgensen, K., Segtnan, V. H., Thyholt, K.,
Ns, T. (2004)  A Comparison of Methods for Analysing Regression Models with
Both Spectral and Designed Variables.
Journal of Chemometrics, 18(10), 451--464.

Due to lack of proper inference procedure and software, the ordinary linear regression model is seldom used in practice for the analysis of right censored data. This paper presents an S-Plus/R program that implements a recently developed inference procedure (Jin, Lin and Ying, 2006)\cite{Jin} for the accelerated failure time model based on the least-squares principle.

Analysis of multivariate dichotomous and polytomous data using latent trait models under the Item Response Theory approach. It includes the Rasch, the Two-Parameter Logistic, the Birnbaum's Three-Parameter, and the Graded Response Models.

Methods of developing linear time series modelling.  Methods are given for loglikelihood computation, forecasting and simulation.

Likelihood inference in case-control studies of a rare disease under independence or simple dependence of genetic and non-genetic covariates

This package implements letter value boxplots which extend the standard boxplot to deal with larger data.

Analysis of N-dye Micro Array experiment using mixed model effect. Containing anlysis of variance, permutation and bootstrap, cluster and consensus tree.

a collection of efficient, vectorized algorithms for the
creation and investigation of magic squares and hypercubes, including
a variety of functions for the manipulation and analysis of
arbitrarily dimensioned arrays.  The package includes methods for
creating normal magic squares of any order greater than 2.  The
ultimate intention is for the package to be a computerized embodiment
all magic square knowledge, including direct numerical verification
of properties of magic squares (such as recent results on the
determinant of odd-ordered semimagic squares).  The package also
serves as a rebuttal to the often-heard comment "I thought R
was just for statistics".

Supplement to maps package, providing the larger and/or
higher-resolution databases.

mapLD measures linkage disequilibrium and constructs
haplotype blocks using the method described
in Gabriel et al (2002) and Wall & Prichard (2003).

Converts latitude/longitude into projected coordinates.

Display of maps.  Projection code and larger maps are in
separate packages (mapproj and mapdata).

Set of tools for manipulating and reading geographic data, in particular ESRI shapefiles; C code used from shapelib. It includes binary access to GSHHS 1.5 shoreline files. The package also provides interface wrappers for exchanging spatial objects with packages such as PBSmapping, spatstat, maps, RArcInfo, Stata tmap, WinBUGS, Mondrian, and others.

Functions with example data for graphing, pruning, and mapping models
from hierarchical clustering, and classification and regression trees.

Some functions for high-dimensional classification, useful for more than 2 classes

R functions for multivariate autoregressive analysis

Non-parametric Analysis of the Marks of Marked Point Processes

The primary aim of MasterBayes is to use MCMC techniques to integrate over uncertainty in pedigree configurations estimated from molecular markers and phenotypic data.  Emphasis is put on the marginal distribution of parameters that relate the phenotypic data to the pedigree. All simulation is done in compiled C++ for efficency.

Provides functions for multivariate and propensity score matching
and for finding optimal balance based on a genetic search algorithm.
A variety of univariate and multivariate metrics to
determine if balance has been obtained are also provided.

MatchIt preprocesses data by selecting approximate matched samples
of the treated and control groups with similar covariate
distributions, drawing on a large variety of matching methods.
After preprocessing data with MatchIt, whatever standard parametric
technique one might have used without preprocessing can be used, but
the results will be far less model dependent.

Simple tools for constructing and manipulating objects
of class mathgraph from the book "S Poetry", available at
http://www.burns-stat.com/pages/spoetry.html

Emulate MATLAB code using R

A collection of functions to support matrix differential calculus
as presented in Magnus and Neudecker (1999)
Matrix Differential Calculus with Applications in Statistics and Econometrics,
Second Edition, John Wiley, New York.  Some of the functions are comparable
to APL and J functions which are useful for actuarial models and calculations.
This package is used for teaching and research purposes at the Department of
Finance and Risk Engineering, Polytechnic University, Brooklyn, NY 11201.

Classes and methods for dense and sparse matrices and
operations on them using Lapack, CSparse and CHOLMOD

Maximally selected rank statistics with
several p-value approximations.

Scattered data interpolation with Multilevel B-Splines

MBESS implements methods that are especially useful to researchers working within the behavioral, educational, and social sciences (both substantive researchers and methodologists). Many of the methods contained within MBESS are applicable to quantitative research in general.

This package provides linear models based
on Theil-Sen single median and Siegel repeated
medians. They are very robust (29 or 50 percent breakdown
point, respectively), and if no outliers are present,
the estimators are very similar to OLS.

Functional gradient descent algorithms
(boosting) for optimizing general loss functions utilizing
componentwise least squares, either of parametric linear form or
smoothing splines, or regression trees as base learners for fitting
generalized linear, additive and interaction models to potentially
high-dimensional data.


mcgibbsit provides an implementation of Warnes & Raftery's MCGibbsit
run-length diagnostic for a set of (not-necessarily independent) MCMC
sampers.  It combines the estimate error-bounding approach of Raftery
and Lewis with evaulate between verses within chain approach
of Gelman and Rubin.

The package performs Monte Carlo hypothesis tests. It allows a couple of different sequential stopping boundaries (a truncated sequential probability ratio test boundary and a boundary proposed by Besag and Clifford, 1991). Gives valid p-values and confidence intervals on p-values.

Model-based cluster analysis: the 2002 version of MCLUST

Model-based clustering and normal mixture modeling
including Bayesian regularization

functions for Markov chain Monte Carlo (MCMC).

This package contains functions to perform Bayesian
inference using posterior simulation for a number of statistical
models. Most simulation is done in compiled C++ written in the Scythe
Statistical Library Version 1.0.2. All models return coda mcmc objects
that can then be summarized using the coda package.  MCMCpack
also contains some useful utility functions, including some
additional density functions and pseudo-random number generators
for statistical distributions, a general purpose Metropolis
sampling algorithm, and tools for visualization.

Multiple contrast tests and simultaneous confidence
intervals based on normal approximation. With implementations for
binomial proportions in a 2xk setting (risk difference and odds ratio)
and for poly-3-adjusted tumour rates.
Approximative power calculation for multiple contrast tests of binomial
proportions.

Mixture and flexible discriminant analysis, multivariate
additive regression splines (MARS), BRUTO, ...

This package performs maximum entropy density based
dependent data bootstrap. An algorithm is provided to create
a population of time series (ensemble) without assuming stationarity.
The reference paper (Vinod, H.D., 2004) explains how the algorithm
satisfies the ergodic theorem and the central limit theorem.

R package for faunistic count data handling and reporting. The name 'mefa' stands for the term 'metafaunistics' indicating that handling of basic data is only the first, but the most critical and sometimes most time consuming part of data analysis. It contains functions to create and manage objects combining basic faunistic (sample/species/count or crosstabulated) count data and sample/species attribute tables. Segments within the count data and samples with zero count can be indicated and used in subsequent operations. Reports can be generated in plain text or LaTeX format.

Sorry, the description of this package is missing.

A couple of tools
for preparing (especially social science) survey data,
conducting simulation studies, and presentation
of results of statistical analyses.

Data sets and sample analyses from Pinheiro and Bates,
"Mixed-effects Models in S and S-PLUS" (Springer, 2000).

N methods are used to measure each of n items.
This data is used to estimate the accuracy and precision of
the methods. Maximum likelihood estimation is used for the
precision estimates.

Fixed and random effects meta-analysis. Functions for tests of bias, forest and funnel plot.

This is the package for doing model based fucntional clustering.

The package implements several time series filters useful
for smoothing and extracting trend and cyclical components of a time
series. The routines are commonly used in economics and finance,
however they should also be interest to other areas. Currently,
Christiano-Fitzgerald, Baxter-King, Hodrick-Prescott, Butterworth,
and trigonometric regression filters are included in the package.

Fractional polynomials are used to represent curvature in regression models. A key reference is Royston and Altman, 1994.

Routines for GAMs and other generalized ridge regression
with multiple smoothing parameter selection by GCV or
UBRE/AIC. Also GAMMs by REML or PQL. Includes a gam()
function.

Tools for microeconomic analysis and microeconomic modelling

Multivariate Imputation by Chained Equations

An R interface to MIM for graphical modelling in R

This package implements various algorithms for inferring networks from data.
All the algorithms compute the mutual information matrix in order to infer a network.
Several mutual information estimators are implemented.

Provides R interface for two functions from MINPACK library,
solving nonlinear least squares problem by modification of the
Levenberg-Marquardt algorithm.

A collection of miscellaneous 3d plots, including
isosurfaces.

Computes statistical analyses useful for applied psychometricians

Tools to perform analyses and combine results from multiple-imputation datasets.

Estimation/multiple imputation programs for mixed categorical
and continuous data

This package fits multiple variable mixtures of various parametric proportional hazard models using the EM-Algorithm. Proportionality restrictions can be imposed on the latent groups and/or on the variables. Several survival distributions can be specified. Missing values are allowed. Independence is assumed over the single variables.

Fits mixtures of (possibly multivariate) regressions
(which has been described as doing ANCOVA when you don't
know the levels).

mixed stock analysis functions

A collection of R functions for analyzing mixture models.

Package for fast computation of the maximum kernel likelihood estimator (mkle)

A collection of artificial and real-world machine learning
benchmark problems, including, e.g., several
data sets from the UCI repository.

Use numerical maximum likelihood to choose and fit a bivariate copula model (from a library of 40 models) to the data.

Difference scaling is a method for scaling perceived
supra-threshold differences.  The package contains functions that
allow the user to design and run a difference scaling experiment,
to fit the resulting data by maximum likelihood and test the
internal validity of the estimated scale.

This package contains functions to compute the nonparametric
maximum likelihood estimator (MLE) for
the bivariate distribution of (X,Y), when
realizations of (X,Y) cannot be observed directly.
To be more precise, we consider the situation
where we observe a set of rectangles
(that we call 'observation rectangles') that are known
to contain the unobservable realizations of (X,Y). We
compute the MLE based on such a set of rectangles.
The methods can also be used for univariate censored data (see data set
'cosmesis'), and for
censored data with competing risks (see data set 'menopause').
We also provide functions to visualize the observed data and the MLE.
(This package contains the functionality
of the R-package 'bicreduc', which will no longer be maintained.)

Maximum likelihood Gaussian process modeling for univariate and multi-dimensional outputs with diagnostic plots and sensitivity analysis.

An R code implementation of the maximum likelihood (fixed point) algorithm of Hyvaerinen, Karhuna and Oja for independent component analysis.

Computational strategies for multivariate linear mixed-effects
models with missing values, Schafer and Yucel (2002), Journal of Computational and Graphical Statistics, 11, 421-442.

Data and examples from a multilevel modelling software review
as well as other well-known data sets from the multilevel modelling
literature.

Mixed-mode latent class regression (also known as mixed-mode
mixture model regression or mixed-mode mixture regression models) which
can handle both longitudinal and one-time responses, although it is created
with longitudinal data in mind.

This package provides functions for computing the density and
the distribution function of, and for generating random vectors from the
multivariate normal and multivariate t distributions.  It provides functions
similar in scope to those of the package 'mvtnorm', but with some
differences; one of these is that probabilities are computed via a
non-Monte Carlo method.

MNP is a publicly available R package that fits the Bayesian
multinomial probit model via Markov chain Monte Carlo. The
multinomial probit model is often used to analyze the discrete
choices made by individuals recorded in survey data. Examples where
the multinomial probit model may be useful include the analysis of
product choice by consumers in market research and the analysis of
candidate or party choice by voters in electoral studies.  The MNP
software can also fit the model with different choice sets for each
individual, and complete or partial individual choice orderings of
the available alternatives from the choice set. The estimation
is based on the efficient marginal data augmentation algorithm that
is developed by Imai and van Dyk (2005). "A Bayesian Analysis of
the Multinomial Probit Model Using the Data Augmentation," Journal
of Econometrics, Vol. 124, No. 2 (February), pp. 311-334. Detailed
examples are given in Imai and van Dyk (2005). "MNP: R Package for
Fitting the Multinomial Probit Model." Journal of Statistical Software,
Vol. 14, No. 3 (May), pp. 1-32.

Fits a variety of mixtures models for multivariate observations
with user-defined distributions and profiles.

This package provides estimators of the mode of univariate unimodal data or univariate unimodal distributions. It also enables to compute the Chernoff distribution.

Given independent and identically distributed observations X(1), ..., X(n) from a density f,
this package provides five methods to perform a multiscale analysis about f as well as the necessary critical
values. The first method, introduced in Duembgen and Walther (2006), provides simultaneous confidence statements
for the existence and location of local increases (or decreases) of f, based on all intervals I(all) spanned by
any two observations X(j), X(k). The second method approximates the latter approach by using only a subset of
I(all) and is therefore computationally much more efficient, but asymptotically equivalent. Omitting the additive
correction term Gamma in either method offers another two approaches which are more powerful on small scales and
less powerful on large scales, however, not asymptotically minimax optimal anymore. Finally, the block procedure is a
compromise between adding Gamma or not, having intermediate power properties. The latter is again asymptotically
equivalent to the first and was introduced in Rufibach and Walther (2007).

A collection of tools to deal with statistical models.
The functionality is experimental and the user interface is likely to
change in the future. The documentation is rather terse, but packages 'coin'
and 'party' have some working examples. However, if you find the
implemented ideas interesting we would be very interested in a discussion
of this proposal. Contributions are more than welcome!

mokken contains functions for performing mokken
scale analysis on test and questionnaire data. It includes an automated
item selection algorithm, and various checks of model assumptions.

Functions to calculate: moments, Pearson's kurtosis,
Geary's kurtosis and skewness; tests related to them (Anscombe-Glynn,
D'Agostino, Bonett-Seier).

Estimation of multivariate normal data of arbitrary dimension
where the pattern of missing data is monotone. Through the use of
parsimonious/shrinkage regressions (plsr, pcr, lasso, ridge, etc.),
where standard regressions fail, the package can handle an (almost)
arbitrary amount of missing data.  The current version supports
maximum likelihood inference.  Future versions will provide a means
of sampling from a Bayesian posterior.

monotonizes a given fit in one or two variables.

Estimates monotone regression and varinace functions in a nonparametric model.

A collection of basic astronomica}l routines for R based on "Practical astronomy with your calculator" by Peter Duffet-Smith.

Detect nonlinear functional relations. This package was originally
designed for the identifiability analysis of nonlinear dynamical models. However,
the concept is very general and allows to detect groups of functionally related
variables whenever there are multiple estimates for each variable, which can be written
in matrix K. The essence of the algorithm is the calculation of mean optimal
transformations estimated from bootstrap samples drawn from K.

Multivariate normal rectangle probabilities (positive
exchangeable, general, approximations);
MLE of regression and correlation
parameters in the multivariate binary/ordinal probit models:
exchangeable, AR(1), and unstructured correlation matrix.

These data sets are taken from the book Introduction
to Linear Regression Analysis (3rd ed), by the above authors.

Analysis of mark-recapture (capture-recapture) data using individual, time, and individual-time varying covariates. Version 1.X contains functions to estimate live-capture Cormack-Jolly-Seber open population models.

With this package, it is possible to perform (simultaneous) inferences for ratios of linear combinations of coefficients in the
general linear model. In particular, tests and confidence interval estimations for ratios of
treatment means in the normal one-way layout and confidence interval estimations like in (multiple) slope ratio
and parallel line assays can be carried out. Moreover, it is possible to calculate the sample sizes required in comparisons
with a control based on relative margins. For the simple two-sample problem, functions for a t-test for ratio-formatted
hypotheses and the corresponding Fieller confidence interval are provided assuming homogeneous or heterogeneous group variances.

Provides methods for estimating frequentist and
Bayesian Vector Autoregression (VAR) models.  Functions for reduced
form and structural VAR models are also available. Includes
methods for the generating posterior inferences for VAR
forecasts, impulse responses (using likelihood-based error bands),
and forecast error decompositions.  Also includes utility functions
for plotting forecasts and impulse responses, and generating draws
from Wishart and singular multivariate normal densities.  Future
versions will include some models with Markov switching.

Functions for fitting general continuous-time Markov and hidden
Markov multi-state models to longitudinal data.  Both Markov
transition rates and the hidden Markov output process can be modelled
in terms of covariates.  A variety of observation schemes are
supported, including processes observed at arbitrary times,
completely-observed processes, and censored states.

A package for producing a smooth estimate of the hazard function for censored data.

Simultaneous tests and confidence intervals
for general linear hypotheses in parametric models, including
linear, generalized linear, linear mixed effects, and survival models.

Convert a logical vector or a vector of
p-values or a correlation, difference, or distance
matrix into a display identifying the pairs for
which the differences were not significantly
different.  Designed for use in conjunction with
the output of functions like TukeyHSD, dist{stats},
simint, simtest, csimint, csimtest{multcomp},
friedmanmc, kruskalmc{pgirmess}.

The package provides tests and confidence intervals for
comparing two treatments when there is more than one primary response
variable (endpoint) given. The step-up procedure of Quan et al. (2001)
is both applied for differences and extended to ratios of means of
normally distributed data. A related single-step procedure is also
available.


Calculate the polygenic and major gene models for quantitative trait
linkage analysis using variance components approach.  The 0.2.2
release includes bug fixes that allow multic to run properly
on 64-bit systems.  The 0.3.0 release includes a fully implemented
sw2mloci function.  Note to Splus users:  sw2mloci requires Splus 8
or greater.

The functions in this package are designed to be used in the analysis of multilevel data by organizational and social psychologists.  The package includes a number of estimates of within-group agreement, and a series of routines using random group resampling (RGR) to identify the group-level properties of data.  Finally, the package contains some basic routines for estimating reliability and manipulating data.

MNL and overdispersed multinomial regression using robust (LQD and tanh) estimation

Non-parametric bootstrap and permutation resampling-based
multiple testing procedures for controlling the family-wise
error rate (FWER), generalized family-wise error rate (gFWER),
tail probability of the proportion of false positives (TPPFP),
and false discovery rate (FDR). Single-step and step-wise
methods are implemented. Tests based on a variety of t- and
F-statistics (including t-statistics based on regression
parameters from linear and survival models) are included.
Results are reported in terms of adjusted p-values,
confindence regions and test statistic cutoffs. The procedures
are directly applicable to identifying differentially
expressed genes in DNA microarray experiments.

muR2SC contains functions that are needed by package muStat
and either have different definitions in R and Splus,
or defined in Splus while not in R.

Performs Wilcox rank sum test, Kruskal rank sum test,
Friedman rank sum test and McNemar test.

This package contains a collection of utility functions for
package muStat.

Utilities for project organization, per-object lazy-loading,
code editing and backup, sourcing,
documentation (formal and informal), package preparation,
macro functions, and miscellaneous utilities.
Needed by debug package.

This package computes the Nelson-Aalen estimator of the cumulative transition hazard for multistate models.

Finds the maximum likelihood estimate of the mean vector
and variance-covariance matrix for multivariate normal data with
missing values.

Generalization of shapiro-wilk test for multivariate variables.

This packages was made for multivariate outlier detection.

Multivariate regression trees

Computes multivariate normal and t probabilities, quantiles and
densities.

Computes multivariate student and multivariate normal integrals,
given a correlation matrix structure defined by a vector bpd, s.t.
rho(i,j) = bpd(i) * bpd(j)   (product correlation structure)

Contains methods described by Dennis R. Helsel in
his book "Nondetects And Data Analysis: Statistics
for Censored Environmental Data"

This package provides a high-level R interface to Unidata's
netCDF data files, which are portable across platforms and include
metadata information in addition to the data sets.
Using this package netCDF files can be opened and data sets read in easily.
It is also easy to create new netCDF dimensions, variables, and files,
or manipulate existing netCDF files.
This interface provides considerably more functionality than the old "netCDF"
package for R, and is not compatible with the old "netCDF" package for R.
Release 1.2 (2005-01-24) adds better support for character variables, and miscellaneous
bug fixes.  Release 1.3 (2005-03-27) is for miscellaneous bug fixes, and improves the
documentation.  Release 1.4 (2005-06-27) improves the efficiency, and adds small bug
fixes.  Release 1.5 (2006-02-27) adds support for byte variables, plus small bug fixes.
Release 1.6 (2006-06-19) adds various bug fixes, plus support for making dimensions
WITHOUT dimvars (coordinate variables), although I think this is a bad
idea in general.  ALSO, the default behavior for put.var.ncdf with unlimited variables
and NO specified start and count parameters has changed!  Before, the default was
to append to the end of the existing variable.  Now, the default is to assume
a start of 1 along each dimension, and a count of the current length of each
dimension.  This really can be ambiguous when using an unlimited dimension.
I always specify both start and count when writing to a variable with an unlimited
dimension, and suggest you do as well.  I make require this in a future release,
as it seems to cause people problems.

The library contains R-functions to perform the
regression depth method (RDM) to binary regression to approximate
the minimum number of observations that can be removed such that
the reduced data set has complete separation.

This package provides a high-level R interface to Unidata's NetCDF
data files. Using this package netCDF datasets, and all their
associated metadata, can be read and written in one go. It is also
easy to create datasets including lots of metadata.
This package supports both the CF and default NetCDF metadata conventions.
This package
supports more general NetCDF files and conventions than the ncdf
package by David Pierce. It requires the low-level NetCDF package
RNetCDF by Pavel Michna.

Estimating the number of essential genes in a genome on the basis of data from a random transposon mutagenesis experiment, through the use of a Gibbs sampler.

Estimate hazard ratios, standardized survival and attributable risks for cohorts with missing covariates, for Cox models or Kaplan-Meier.

Tools to create and modify network objects.  The network class can represent a range of relational data types, and supports arbitrary vertex/edge/graph attributes.

RBF and MLP neural networks with graphical user interface

Two numerical indices are proposed for the Cattell Scree test:
1. Acceleration factor (with or without Parallel Analysis)
2. Optimal Coordinates (with or without Parallel Analysis)

nFDR is package that implements the nonparametric estimate of FDR based on Bernstein polynomials. It calculates the proportion of true null hypotheses, FDR, FNR, and q-values.

get or set UNIX priority (niceness) of running R process

Datasets for testing nonlinear regression routines.

Fit and compare Gaussian linear and nonlinear mixed-effects models.

This package combines the odesolve and nlme packages for mixed-effects modelling using differential equations.

Adds algorithm="brute-force" and multiple starting values to nls.

Several tools for assessing the quality of fit of a gaussian nonlinear model are provided.

Uses a modified lifting algorithm on which it builds the nondecimated lifting transform. It has applications in wavelet shrinkage.

Fits a non-linear transformation model (nltm) for analyzing survival data, see Tsodikov (2003) "Semiparametricmodels: a generalized self-consistency approach". J.R. Statistical Society B, 65, Part 3, 759-774. The class of nltm includes the following currently supported models: Cox proportional hazard, proportional hazard cure, proportional odds, proportional hazard - proportional hazard cure, proportional hazard - proportional odds cure, Gamma frailty, and proportional hazard - proportional odds.

Provides remote access to daily mortality, weather, and air pollution data from the National Morbidity, Mortality, and Air Pollution Study for 108 U.S. cities (1987--2000); data are obtained from the Internet-based Health and Air Pollution Surveillance System (iHAPSS)

An R interface to the Lawson-Hanson algorithm for non-negative least squares (NNLS).  Also allows the combination of non-negative and non-positive constraints.

Estimate and compare the accuracies of diagnostic tests in situations where the gold standard is continuous, ordinal or nominal.

Onedimensional Normal Mixture Models Classes, for, e.g.,
density estimation or clustering algorithms research and teaching;
providing the widely used Marron-Wand densities, see ?MarronWand.

A collection of utilities refered to
exponential power distributions, also known as General Error Distribution

Analysis of multivariate normal datasets with missing values

Evaluates the probability density function of the sum of the
Gaussian and Student's t density on 3 degrees of freedom.
Evaluates the p.d.f. of the sphered Student's t density function.
Also evaluates the erf and erfc functions on complex-valued arguments.

Includes Omnibus Univariate and Multivariate Normality Tests. One variation allows for the possiblity of weak dependence rather than independence in the variable(s). Also included is an univariate white noise test where the null hypothesis is for "white noise" rather than "strict white noise". The package deals with similar approaches to testing as the "nortest","moments",and "mvnormtest" packages in R.

Five omnibus tests for the composite hypothesis of normality

The library contains R-functions to perform the
regression depth method (RDM) to binary regression to approximate
the amount of overlap, i.e. the minimal number of observations
that need to be removed such that the reduced data set has no longer overlap.

This package provides a variety of nonparametric (and semiparametric) kernel methods that seamlessly handle a mix of continuous, unordered, and ordered factor datatypes. We would like to gratefully acknowledge support from  the Natural Sciences and Engineering Research Council of Canada (NSERC:www.nserc.ca), the Social Sciences and Humanities Research Council of Canada (SSHRC:www.sshrc.ca), and the Shared Hierarchical Academic Research Computing Network (SHARCNET:www.sharcnet.ca).

Provides simultaneous rank test procedures for the one-way layout
without presuming a certain distribution.

Nonparametric maximum likelihood estimation or Gaussian
quadrature for overdispersed generalized linear models and
variance component models

Datasets from Bates and Watts (1988) "Nonlinear Regression
Analysis and Its Applications" with sample code.

A collection of statistical tools for objective (non-supervised) applications
of the Regional Frequency Analysis methods in hydrology.

Accurate Numerical Derivatives. See ?numDeriv.Intro for more details.

Provides coordination and parallel execution
facilities, as well as limited cross-language data exchange,
using the netWorkSpaces server developed by REvolution
Computing

Generalise the starting point of the array index

Observational studies are limited in that there could be an unmeasured variable related to both the response variable and the primary predictor.  If this unmeasured variable were included in the analysis it would change the relationship (possibly changing the conclusions).  Sensitivity analysis is a way to see how much of a relationship needs to exist with the unmeasured variable before the conclusions change.  This package provides tools for doing a sensitivity analysis for regression (linear, logistic, and cox) style models.

Supports the analysis of Oceanographic data,
including CTD measurements, sea-level timeseries, coastline
files, etc.  Also includes functions for calculating
seawater properties such as density, and derived properties
such as buoyancy frequency.

Estimates Optimal Classification scores from roll call votes supplied though a 'rollcall' object from package 'pscl'.

This package provides an interface for the ODE solver lsoda.
ODEs are expressed as R functions or as compiled code.

Sweave processing of Open Document Format (ODF) files

This package implements the stochastic meta algorithm called Optimal Feature Weighting for two multiclass classifiers, CART and SVM


A collection of routines to manipulate and visualize quaternions and
octonions.

An interface to openNLP (http://opennlp.sourceforge.net/).

English and Spanish models for openNLP.

Functions to perform optimal matching, particularly full matching

Representations, conversions and display of orientation SO(3) data.
See the orientlib help topic for details.

Odds ratio based multifactor-dimensionality reduction method for detecting gene-gene interactions

A collection of functions to construct sets of orthogonal
polynomials and their recurrence relations. Additional functions
are provided to calculate the derivative, integral, value and roots
of lists of polynomial objects.

Fit and compare Ornstein-Uhlenbeck models for evolution
along a phylogenetic tree.

A collection of some tests commonly used for identifying outliers.

Functions for plotting Australia's coastline and state
boundaries.

Calculation of the parametric, nonparametric confidence intervals
for the difference or ratio of location parameters and for the difference, ratio
and odds-ratio of binomial proportion for comparison of independent samples.
CI are not adjusted for multiplicity. A by statement allows calculation of CI
separately for the levels of further factors.

This package facilitates analysis of paleontological sequences of trait values from an evolving lineage.  Functions are provided to fit, using maximum likelihood, evolutionary models including unbiased random walks, directional evolution, stasis and Orstein-Uhlenbeck (OU) models.  This package performs many of the same functions as the package paleoTS, but does so using a different parameterization of the evolutionary models.

This package facilitates analysis of paleontological sequences of trait values from an evolving lineage.  Functions are provided to fit, using maximum likelihood, evolutionary models including unbiased random walks, directional evolution and stasis models.

Some functions for sample classification in microarrays

Functions and datasets for fitting models to Panel data.

Multiple imputation for multivariate panel or clustered data

Similar to apply and lapply, applies a function to all items
of a list, and returns a list with the results.  Uses Rmpi to distribute
the processing evenly across a cluster.  If Rmpi is not available,
implements this as a non-parallel algorithm.  Includes some debugging
support

paran is an implementation of Horn's technique for evaluating the components retained in a principle components analysis (PCA). Horn's method contrasts eigenvalues produced through a PCA on a number of random data sets of uncorrelated variables with the same number of variables and observations as the experimental or observational data set to produce eigenvalues for components that are adjusted for the sample error-induced inflation. Components with adjusted eigenvalues greater than one are retained. paran may also be used to conduct parallel analysis following Glorfeld's (1995) suggestions to reduce the likelihood of over-retention.

pARtial implements (partial) attributable risk estimates, corresponding variance estimates and confidence intervals.

Additive partitions of integers.  Enumerates the
partitions, unequal partitions, and restricted partitions of an
integer; the three corresponding partition functions are also
given.  Set partitions are now included.

This package performs basic functions to fit and predict periodic autoregressive time series models. These models are discussed in the book P.H. Franses (1996) "Periodicity and Stochastic Trends in Economic Time Series", Oxford University Press. Data set analyzed in that book is also provided.

A computational toolbox for recursive partitioning.
The core of the package is ctree(), an implementation of
conditional inference trees which embed tree-structured
regression models into a well defined theory of conditional
inference procedures. This non-parametric class of regression
trees is applicable to all kinds of regression problems, including
nominal, ordinal, numeric, censored as well as multivariate response
variables and arbitrary measurement scales of the covariates.
Based on conditional inference trees, cforest() provides an
implementation of Breiman's random forests. The function mob()
implements an algorithm for recursive partitioning based on
parametric models (e.g. linear models, GLMs or survival
regression) employing parameter instability tests for split
selection. Extensible functionality for visualizing tree-structured
regression models is available.

Regulation, decomposition and analysis of space-time series. The pastecs library is a PNEC-Art4 and IFREMER (Benoit Beliaeff &lt;Benoit.Beliaeff@ifremer.fr&gt;) initiative to bring PASSTEC 2000 (http://www.obs-vlfr.fr/~enseigne/anado/passtec/passtec.htm) functionnalities to R.

Data and functions for the book Probability and Statistics with R.

This package provides data analysis via the pbat program, and an alternative internal implementation of the power calculations via simulation only.  For analysis, this package provides a frontend to the PBAT software, automatically reading in the output from the pbat program and displaying the corresponding figure when appropriate (i.e. PBAT-logrank). It includes support for multiple processes and clusters. For analysis, users must download PBAT and accept it's license, available on the PBAT webpage. Both the data analysis and power calculations have command line and graphical interfaces using tcltk.

This software has evolved from fisheries research
conducted at the Pacific Biological Station (PBS) in Nanaimo,
British Columbia, Canada. It extends the R language to include
two-dimensional plotting features similar to those commonly
available in a Geographic Information System (GIS).  Embedded
C code speeds algorithms from computational geometry, such as
finding polygons that contain specified point events or
converting between longitude-latitude and Universal Transverse
Mercator (UTM) coordinates.  It includes data for a global
shoreline and other data sets in the public domain. For a
complete user guide, see the file "PBSmapping-UG.pdf" in the
directory \library\PBSmapping of the local R installation.

PBS Modelling provides software to facilitate the design,
testing, and operation of computer models. It focuses particularly on
tools that make it easy to construct and edit a customized graphical
user interface (GUI). Although it depends heavily on the R interface
to the Tcl/Tk package, a user does not need to know Tcl/Tk. The
package contains examples that illustrate models built with other R
packages, including PBS Mapping, odesolve, ddesolvem, and BRugs.
It also serves as a convenient prototype for building new R packages,
along with instructions and batch files to facilitate that process.
The root library directory of PBSmodelling includes a complete user guide
PBSmodelling-UG.pdf. To use this package effectively, please consult
the guide.

Standard and robust estimation of the skeleton (ugraph)
and the equivalence class of a Directed Acyclic Graph (DAG) via the
PC-Algorithm. The equivalence class is represented by its (unique)
Completetd Partially Directed Acyclic Graph (CPDAG).

Robust PCA by Projection Pursuit

This package contains a function to estimate panel-corrected standard errors. Data may contain balanced or unbalanced panels.

Fits a principal curve to a numeric multivariate
dataset in arbitrary dimensions. Produces diagnostic plots.

Package for estimating periodic autoregressive models.
Package also includes methods for plotting periodic time
series data.

The Pearson-ICA algorithm is a mutual information-based method for blind separation of statistically independent source signals.
It has been shown that the minimization of mutual information leads to iterative use of score functions, i.e. derivatives of log densities.
The Pearson system allows adaptive modeling of score functions. The flexibility of the Pearson system makes it possible to model a wide range
of source distributions including asymmetric distributions. The algorithm is designed especially for problems with asymmetric sources
but it works for symmetric sources as well.

A package for fitting possibly high dimensional penalized regression models. The penalty structure can be any combination of an L1 penalty (lasso), an L2 penalty (ridge) and a positivity constraint on the regression coefficients. The supported regression models are linear and logistic regression and the Cox Proportional Hazards model. Cross-validation routines allow optimization of the tuning parameters.

Library of econometric functions for performance and risk analysis. This library aims to aid practitioners and researchers in utilizing the latest research in analysis of non-normal return streams.  In general, this library is most tested on return (rather than price) data on a monthly scale, but most functions will work with daily or irregular return data as well.

The permax library consists of 7 functions, intended to facilitate
certain basic analyses of DNA array data, especially with regard to
comparing expression levels between two types of tissue.

The permtest uses permutations in order to compare the variability within and distance between two groups within a set of microarray data.

"perturb" evaluates collinearity by adding random noise to selected variables. "colldiag" calculates condition numbers and variance decomposition proportions to test for collinearity and uncover its sources.

This package implements different analytic/direct and
iterative reconstruction methods of Peter Toft. It also
offer the possibility to simulate PET data.

This work is an extension of the state space model for Poisson count data, Poisson-Gamma model, towards a semiparametric specification. Just like the generalized additive models (GAM), cubic splines are used for covariate smoothing. The semiparametric models are fitted by an iterative process that combines maximization of likelihood and backfitting algorithm.

Miscellaneous functions for analysis and display of ecological and spatial data

Provides some easy-to-use functions for time series
analyses of (plant-) phenological data sets. These functions
mainly deal with the estimation of combined phenological time
series and are usually wrappers for functions that are already
implemented in other R packages adapted to the special
structure of phenological data and the needs of phenologists.
Some date conversion functions to handle Julian dates are also
provided.

Serializes R objects for import by PHP into an associative array. Can be used to build interactive web pages with R.

Manipulation and analysis of phylogenetically simulated data sets
and phylogenetically based analyses using GLS.

PhySim contains functions to simulate phylogenetic trees under a virth death model. Functions are provided to model a lag-time to speciation and extract sister species ages.

Package can either (i) graphically traverse trees (rpart)
objects. If a new case needs to be classified then the PT function
uses a GUI to ask questions to determine how a new case satisfies
the decisions in a classification tree and comes up with its classified
value (also for regression trees) (ii) alternatively, the pinktoe function
can produce HTML/PERL code which can be used with a CGI-enabled web
server so that a tree can be traversed on a web site.

Functions for import, export, plotting and other
manipulations of bitmapped images.

PKfit is a nonlinear regression (including a genetic algorithm) program which was designed to perform model/curve fitting and model simulations for pharmacokinetics.

Estimation of pharmacokinetic parameters

computations for WinBUGS, NONMEM, NLME

A graphical user interface for viewing and interacting with R plots.
It tries to work out the structure of a plot, in order to interact with it.
The built-in features include: navigating the data space, identifying data points,
editing and annotating the plot, and saving to a file. New tools can be defined.
Note: the interaction features do not work with multiple-plot layouts.
This package is based on RGtk2, and so requires the GTK+ libraries.

This package uses item response theory methods to compute linking constants and conduct
chain linking of tests for multiple groups under a nonequivalent groups common item design.
It allows for mean/mean, mean/sigma, Haebara, and Stocking-Lord calibrations of dichotomous
(1PL, 2PL, and 3PL) and/or polytomous (graded response, partial credit/generalized partial
credit, nominal, and multiple-choice model) common items.

A set of estimators and tests for panel data

A graphical user interface for R plots. Wrap it around your plot calls,
like playwith(plot(mydata)).
NOTE: This package has been superseded by package "playwith"
and will eventually be removed.

Lots of plots, various labeling, axis and color scaling functions.

Fit Log Linear by Linear Association models

This package provides routines for PLS-based genomic analyses.
It implements PLS methods for classification with microarray
data  and  prediction of transcription factor activities from combined
ChIP-chip analysis. The &gt;=1.2-1 versions include two new classification
methods for microarray data: GSIM and Ridge PLS.

Multivariate regression by partial least squares
regression (PLSR) and principal component regression (PCR).

Kernel density estimation with global bandwidth selection
via "plug-in".

Simple GUI for R using gWidgets.

The Predictive Modelling Markup Language (PMML) is a language
for representing models, in an application independent way. Such models
can then be loaded into other applications supporting PMML, such as
Teradata Warehouse Miner and IBM's DB2.
The package provides a generic pmml function to generate pmml for an
object. Using a S3 generic function the approriate method for the
class of the supplied object is dispatched.
The package currently supports the export of PMML for linear
regression (but not interaction terms), rpart classification trees,
randomSurvivalForest forest models, and kmeans clusters.
This package is part of the Rattle toolkit.

Functions for obtaining the density, random deviates
and maximum likelihood estimates of the Poisson lognormal
distribution and the bivariate Poisson lognormal distribution.

Latent class analysis and latent class regression models
for polytomous outcome variables.  Also known as latent structure analysis.

Routines for the polynomial spline fitting routines
hazard regression, hazard estimation with flexible tails, logspline,
lspec, polyclass, and polymars, by C. Kooperberg and co-authors

Generate dependent samples from a non-full dimensional polytope
via a Markov Chain sampler

Computes polychoric and polyserial correlations by quick "two-step" methods or ML,
optionally with standard errors; tetrachoric and biserial correlations are special cases.

A collection of functions to implement a class for univariate
polynomial manipulations.

Inference methods for partially-observed Markov processes

Construct and analyze projection matrix models from a demography study of marked individuals classified by age or stage. The package covers methods described in Matrix Population Models by Caswell (2001) and Quantitative Conservation Biology by Morris and Doak (2002).

A package that implements a variety of statistical and population genetic methodology.

Population Lab is a tool for constructing a virtual electronic
population of related individuals evolving over calendar time, by using
vital statistics, such as mortality and fertility, and disease incidence
rates.

Classes for analysing and implementing equity portfolios.

Classes that serve as a framework for designing equity portfolio simulations.

Some functions useful to perform a Peak Over Threshold analysis in univariate and bivariate cases. A user's guide is avalaible within the doc folder of the package or at my webpage.

Optimizes a function using Powell's UObyQA algorithm.

(1) To estimate the power of testing for linkage using an
affected sib pair design, as a function of the recurrence risk ratios. We
will use analytical power formulae as implemented in R. These are based
on a Mathematica notebook created by Martin Farrall. (2) To examine how
the power of the transmission disequilibrium test (TDT) depends on the
disease allele frequency, the marker allele frequency, the strength of
the linkage disequilibrium, and the magnitude of the genetic effect. We
will use an R program that implements the power formulae of Abel and
Muller-Myhsok (1998). These formulae allow one to quickly compute power
of the TDT approach under a variety of different conditions. This R
program was modeled on Martin Farrall's Mathematica notebook.

Sample classification of protein mass spectra by peak probabilty contrasts

This package contains linear and nonlinear regression methods based on Partial Least Squares and Penalization Techniques.

The pps package contains functions to select samples using PPS
(probability proportional to size) sampling. It also includes a function for
stratified simple random sampling, a function to compute joint
inclusion probabilities for Sampford's method of PPS sampling, and a few
utility functions. The user's guide pps-ug.pdf is included.

Distance-based parametric bootstrap tests for clustering,
mainly intended for presence-absence data and abundance data
(clustering of species
distribution ranges). Jaccard, Kulczynski, quantitative Kulczynski
and geco distance measures,
clustering of presence-absence and abundance data, and nearest neighbor
based noise detection (R port of Byers
and Raftery's (1998) "NNclean"). Main functions are prabtest, abundtest (for
testing), prabclust (for clustering), prabinit (for preparing the data)
and NNclean (for noise detection). The help-pages for prabtest, abundtest and
prabclust contain simple standard executions. Note that the use of the
package mclust (called by function prabclust) is protected by a special
license, see http://www.stat.washington.edu/mclust/license.txt, particularly
point 6.

This software is used to predict the binary response based on high dimensional features, for example gene expression data. The data are modelled with Bayesian naive Bayes models. When a large number of features are available, one may like to select only a subset of features to use, typically those features strongly correlated with the response in training cases. Such a feature selection procedure is however invalid since the relationship between the response and the features will appear stronger. This package provides a way to avoid this bias and yields well-calibrated prediction for the test cases.

"train_predict_mix" predicts the binary response with binary features

This package provides a set of functions useful when evaluating the results of presence-absence models. Package includes functions for calculating threshold dependant measures such as confusion matrices, pcc, sensitivity, specificity, and Kappa, and produces plots of each measure as the threshold is varied. It will calculate optimal threshold choice according to a choice of optimization criteria. It also includes functions to plot the threshold independent ROC curves along with the associated AUC (area under the curve).

Functions for conventionally formatted descriptive stats, and
to format R output as HTML.

PRIM for bump hunting in high-dimensional data

fits a principal curve to a data matrix in arbitrary
dimensions

Most statistical methods for finding interesting genes are focusing on the summary value with large fold change or large variations. Very few methods are considered the probe level data. This package is to calculate the reliability of the gene expression data from Affymetrix chip using the probe-level data.

It produces probabilistic weather field forecasts using the Geostatistical Output Perturbation method introduced by Y. Gel, A. E. Raftery and T. Gneiting (JASA 99, 2004)


This package provides a framework for performing elementary probability calculations on finite sample spaces, which may represented by data frames or lists.  Functionality includes setting up sample spaces, counting tools, defining probability spaces, performing set algebra, calculating probability and conditional probability, tools for simulation and checking the law of large numbers, adding random variables, and finding marginal distributions.

Programs to determine student grades and create
examinations from Question banks.  Programs will create numerous
multiple choice exams, randomly shuffled, for different versions of same question list.

profileModel provides tools that can be used to calculate, evaluate, plot and use for inference the profiles of *arbitrary* inference functions for *arbitrary* 'glm'-like fitted models with linear predictors.

Tools for examining Rprof profile output.

A simple interface to lat/long projection and datum
transformation of the PROJ.4 cartographic projections library. It
allows transformation of geographic coordinates from one projection
and/or datum to another.

Tests of the proportional hazards assumption in the Cox
model: data-driven Neyman type smooth tests and score process based
tests for identifying nonproportional covariates and for global
checks.

An object oriented system using object-based, also
called prototype-based, rather than class-based object
oriented ideas.

Provides an extensible framework for the effcient calculation of auto- and cross-proximities, along with implementations of the most popular ones.

A collection of functions that primarily produce graphics to aid in a
Propensity Score Analysis (PSA).  Functions include: cat.psa and box.psa to test
balance within strata of categorical and quantitative covariates,
circ.psa for a representation of the estimated effect size by stratum,
loess.psa that provides a graphic and loess based effect size estimate,
and various balance functions that provide measures of the balance achieved
via a PSA in a categorical covariate.

Bayesian analysis of item-response theory (IRT) models, roll call analysis;
computing highest density regions; maximum likelihood estimation of
zero-inflated and hurdle models for count data; goodness-of-fit measures for
GLMs; data sets used in writing
and teaching at the Political Science Computational Laboratory; seats-votes curves.

Various functions for computing pseudo-observations for censored data regression

Smoothing splines with penalties on order m derivatives.

A number of routines for personality, psychometrics and experimental
psychology.   Functions are primarily for scale construction, cluster analysis and
reliability analysis, although others are basic descriptive
stats.  For more information, see the personality-project.org/r.

Contains functions useful for correlation theory,
meta-analysis (validity-generalization),
reliability, item analysis, inter-rater reliability,
and classical utility

Kappa, ICC, Cronbach alpha, screeplot, PCA, biplots, non linear mapping and related methods

An assortment of functions that could be useful in analyzing data from pyschophysical experiments. It includes functions for calculating d' from several different experimental designs, links for mafc to be used with the binomial family in glm (and possibly other contexts) and self-Start functions for estimating gamma values for CRT screen calibrations.

A multiway method to decompose a tensor (array) of any order,
as a generalisation of SVD also supporting non-identity metrics and penalisations.
2-way SVD with these extensions is also available. The package includes also some other multiway
methods: PCAn (Tucker-n) and PARAFAC/CANDECOMP with these extensions.

pvclust is a package for assessing the uncertainty in hierarchical cluster analysis. It provides AU (approximately unbiased) p-values as well as BP (boostrap probability) values computed via multiscale bootstrap resampling.

Tools to compute power in a group sequential design.  SimPwrGSD C-kernel is a simulation routine that is similar in spirit to dssp2.f by Gu and Lai, but with major improvements.  AsyPwrGSD has exactly the same range of application as SimPwrGSD but uses asymptotic methods and runs _much_ faster.

Power analysis functions along the lines of Cohen (1988)

The Penn World Table provides purchasing power parity and
national income accounts converted to international prices for
168 countries for some or all of the years 1950-2000.

qAnalyst makes control charts for variables and attributes according to Douglas C. Montgomery Introduction to Statistical Quality Control book. Moreover, capability analysis for normal and non - normal distributions is implemented.

QCAGUI is a graphical user interface (GUI) for the QCA package, derived from R Commander. Because QCA has little to do with statistics, the menus from Rcmdr were stripped down to the very basics. In crisp sets QCA, data is binary therefore it is fairly decent to treat it as categorical (1 - presence; 0 - absence). In order to ease the primary analysis (e.g. tables of frequencies) and the creation of basic graphs, this package activates some menus that are not available in Rcmdr but for factors. Users should be aware, however, that QCAGUI is not a package for statistics; Rcmdr is better for this purpose

Performs the Quine-McCluskey algorithm for Qualitative Comparative Analysis, as described in Ragin, Charles C. 1987 "The Comparative Method. Moving beyond qualitative and quantitative strategies", Berkeley: University of California Press. In the classical approach it currently handles about 15 conditions and one outcome, but since version 0.4-5 the package has an enhanced, faster function that obtains the same exact solutions with a substantially lower memory consumption, and it handles even more causal conditions. The next versions of this algorithm will be oriented towards multi-value and fuzzy-set functions. Also, in the near future the current functions will deal with missing values in the data in order to perform simplifying assumptions

Shewhart quality control charts for continuous, attribute and
count data. Cusum and EWMA charts. Operating characteristic
curves.  Process capability analysis. Pareto chart and
cause-and-effect chart.

The package qgen is a collection of functions to analyse quantitative genetic data. It is especially helpful to perform parametric resampling of quantitative genetic data sets. Resampling allows first to determine a priori the expected variance of an estimator, second for a given empirical data set to calculate bootstrap confidence intervals, and third to evaluate different estimators and confidence intervals.

Model fitting, optimal model selection and calculation of various features that are essential in the analysis of quantitative real-time polymerase chain reaction (qPCR).

the q-order partial correlation graph search algorithm, q-partial,
or qp, algorithm for short, is a robust procedure for structure
learning of undirected Gaussian graphical Markov models from
"small n, large p" data, that is, multivariate normal data coming
from a number of random variables p larger than the number of
multidimensional data points n as in the case of, e.g., microarray
data.

This is a free R-language library designed to accompany the book
Quantitative Risk Management: Concepts, Techniques and Tools by Alexander
J. McNeil, Rudiger Frey and Paul Embrechts.  A separate SPlus version can be
downloaded from  Alexander McNeil's URL at
http://www.ma.hw.ac.uk/~mcneil/book/QRMlib.html

Functions for model selection for genetic architecture.

Tools for the design of QTL experiments

Analysis of experimental crosses to identify genes (called quantitative trait loci, QTLs) contributing to variation in quantitative traits.

This package contains routines and documentation for
solving quadratic programming problems.

Qualitative methods for the validation of models.

Statistical evaluation of calibration curves
by different regression techniques: ordinary, weighted,
robust (up to 4th order polynomial).
Log-log and Box-Cox transform, estimation of
optimal power and weighting scheme. Tests for heteroscedascity
and normality of residuals. Different kinds of plots
commonly used in illustrating calibrations. Easy "inverse
prediction" of concentration by given responses
and statistical evaluation of results (comparison of
precision and accuracy by common tests).

Specify, build, trade, and analyse quantitative financial trading strategies

Contains functions useful for data screening,
testing moderation, mediation and estimating power.

Quantile Regression Forests is a tree-based ensemble method
for estimation of conditional quantiles. It is particularly well suited for
high-dimensional data. Predictor variables of mixed classes can be
handled. The package is dependent on the package randomForests,
written by Andy Liaw.

Quantile regression and related methods.

This package takes a list of p-values resulting from the
simultaneous testing of many hypotheses and estimates their
q-values. The q-value of a test measures the proportion of
false positives incurred (called the false discovery rate)
when that particular test is called significant. Various plots
are automatically generated, allowing one to make sensible
significance cut-offs. Several mathematical results have recently
been shown on the conservative accuracy of the estimated
q-values from this software. The software can be applied
to problems in genomics, brain imaging, astrophysics, and
data mining.

Functions to compute quasi variances and associated measures of approximation error

Includes HTML function and methods to write in an HTML file. Thus, making HTML reports is easy. Includes a function that allows redirection on the fly, which appears to be very usefull for teaching purpose, as the student can keep a copy of the produced output to keep all that he did during the course. Package comes with a vignette describing how to write HTML reports for statistical analysis. Finally, a driver for Sweave allows to parse HTML flat files containing R code and to automatically write the corresponding outputs (tables and graphs).

r2lUniv perform some basic analysis and generate the corresponding LaTeX code.
The basic analysis depends of the variable type.
Four types are considered:
- Nominal: modaliy, size, barplot
- Ordinal: modaliy, size, quartile, barplot
- Discrete: modaliy, size, mean, var, quartile, boxplot, barplot
- Continuous: mean, var, quartile, boxplot, barplot

Using this package,
it is possible to call a BUGS model, summarize inferences and
convergence in a table and graph, and save the simulations in arrays for easy access
in R / S-PLUS. In S-PLUS, the openbugs functionality and the windows emulation
functionality is not yet available.

Implementation of some racing methods for the empirical
selection of the best. If the R package 'rpvm' is installed
(and if PVM is available, properly configured, and initialized),
the evaluation of the candidates are performed in parallel on
different hosts.

RadioSonde is a collection of programs for reading and
plotting SKEW-T,log p diagrams and wind profiles for data collected by
radiosondes (the typical weather balloon-borne instrument), which we
will call "flights", "sondes", or "profiles" throughout the associated
documentation.  The raw data files are in a common format that has a
header followed by specific variables.  Use "help(ExampleSonde)" for
the full explanation of the data files.

Raking a survey dataset entails re-weighting a sample by making the sample marginal totals agree with the population marginal totals for two survey response variables. Raking is a robust technique that is often useful for dealing with nonresponse. The 'rake' package streamlines the process of Raking by creating the special 'rake' class, which is essentially a summary of the sample weights.

Bayesian geostatistical modeling of Gaussian processes using a reparameterized and marginalized posterior sampling (RAMPS) algorithm designed to lower autocorrelation in MCMC samples.  Package performance is tuned for large spatial datasets.

The deterministic part of the Fortuna cryptographic pseudorandom number generator, described by Scheier & Ferguson "Practical  Cryptography"

Simulation of Gaussian and extreme value random fields;
conditional simulation; kriging

Classification and regression based on a forest of trees using random inputs.

This package provides an interface to the true random number
service provided by the random.org website created by Mads Haahr.
The random.org web service samples atmospheric noise via radio
tuned to an unused broadcasting frequency together with a skew
correction algorithm due to John von Neumann.  More background is
available in the included vignette based on an essay by Mads Haahr.
In its current form, the package offers functions to retrieve
random integer number (with duplicates), randomized sequences
(without duplicates) and raw random bytes.

Ensemble survival analysis based on a random forest of trees using random inputs.

Implementation of random variables by means
of S4 classes and methods

This package performs aggregation of ordered lists based on the ranks using three different algorithms: Cross-Entropy Monte Carlo algorithm, Genetic algorithm, and a brute force algorithm (for small problems)

Obtain rank regression estimator for the AFT model
with right censored data. Testing a given value of
the regression coefficient and Re-sampling variance
estimator can also be computed.

This package uses the functions written by Daniel
Morissette &lt;danmo@videotron.ca&gt; to read geographical information in Arc/Info
V 7.x format and E00 files to import the coverages into R variables.

Sampling binary matrices with fixed margins

A function which performs exact rate ratio tests and returns an object of class htest.

Rattle provides a Gnome (RGtk2) based interface to
R functionality for data mining. The aim is to provide
a simple and intuitive interface that allows a user to quickly
load data from a CSV file (or via ODBC), transform and explore the data,
and build and evaluate models, and export models as PMML (predictive
modelling markup language). All of this with knowing little about R.
All R commands are logged and available for the user, as a tool to
then begin interacting directly with R itself, if so desired. Rattle also
exports a number of utility functions and the graphical user interface does
not need to be run to deploy these.

demo of interface with full copy of all hpp defining boost

Fetch data from Bloomberg

Functions to prepare files needed for running BUGS
in batch-mode, and running BUGS from R. Support for Linux systems
with OpenBugs is emphasized.

Methods for memoization, that is, caching arbitrary R objects in persistent memory.  Objects can be loaded and saved stratified on a set of hashing objects.

Estimation of abundance and other demographic parameters
for closed populations, open populations and the robust design
in capture-recapture experiments using loglinear models.

R interface to (some of) cddlib,
see http://www.ifor.math.ethz.ch/~fukuda/cdd_home/cdd.html
converts back and forth between two representations of a convex polytope:
as solution of a set of linear equalities and inequalities and as
convex hull of set of points and rays,
also does linear programming with exact arithmetic

This package allows the user to access functionality in the
CDK, a Java framework for cheminformatics. This allows the user to load
molecules, evaluate fingerprints, calculate molecular descriptors and so on.
In addition the CDK API allows the user to view structures in 2D. The package
can also use JChemPaint, to draw 2D structure and use them within R and Jmol to
view structures in 3D


A platform-independent basic-statistics GUI (graphical user interface) for R, based on the tcltk package.

This package provides an Rcmdr "plug-in" based on the
time series functions

Plugin Rcmdr Plugin for the FactoMineR package

Rcmdr menu support for many of the functions in the HH package.
The focus is on menu items for functions we use in our introductory courses.


This package accompanies G. Andy Chang and G. Jay Kerns, Introduction to Probability and Statistics Using R (in progress).
The package contributes functions unique to the book as well as specific configuration and selected functionality to the R Commander by John Fox.

This package provides an Rcmdr "plug-in" based on the TeachingDemos package, and is primarily for illustrative purposes.

The packages provides palettes for drawing nice maps
shaded according to a variable.

R functions to interface with COM objects, R exposed to COM Clients

Generates potential completions for a given command
snippet based on circumstantial evidence

R interface to CPLEX solvers for linear, quadratic, and (linear and quadratic) mixed integer programs. A working installation of CPLEX is required for usage of the Rcplex package. See the file "INSTALL" for details on how to install the Rcplex package in Linux/Unix-like systems. No support for Windows platforms is currently available.

Shrunken Centroids Regularized Discriminant Analysis
for the classification purpose in high dimensional data.

The RDieHarder packages provides an R interface to the
dieharder suite of random number generators and tests that was
developed by Robert G. Brown, extending earlier work by George
Marsaglia and others.

Realized Variance Toolkit

small package with functions for creating references, reading from and writing ro references and a memory efficient refdata type that transparently encapsulates matrixes and data.frames

Functions to fit Gaussian linear model by maximising the residual log likelihood where the covariance structure can be written as a linear combination of known matrices.  Can be used for multivariate models and random effects models.  Easy straight forward manner to specify random effects models, including random interactions.

For a sequence of event occurence times, we are interested in
finding subsequences in it that are too "regular". We define regular as being
significantly different from a homogeneous Poisson process. The departure from
the Poisson process is measured using a L1 distance. See Di and Perlman 2007
for more details.

Functions for unary and binary regression tests

relaimpo provides several metrics for assessing relative importance in linear models. These can be printed, plotted and bootstrapped. The recommended metric is lmg, which provides a decomposition of the model explained variance into non-negative contributions. There is a version of this package available that additionally provides a new and also recommended metric called pmvd. If you are a non-US user, you can download this extended version from Ulrike Groempings web site.

Data structures and algorithms for k-ary relations with
arbitrary domains, featuring relational algebra, predicate functions,
and fitters for consensus relations.

package relax contains some functions for report
writing, presentation, and programming: relax(), tangleR(),
weaveR(), slider(). "relax" is written in R and Tcl/Tk.
relax creates a new window (top level Tcl/Tk widget) that consists
of two text fields and some buttons and menus.
Text (chunks) and code (chunks) are inserted in the upper text field (report field).
Code chunks are evaluated by clicking on EvalRCode.
Results are shown in the lower text field (output field) and
will be transfered to the report field by pressing on Insert.
In this way you get correct reports. These reports can be
loaded again in cause of presentation, modification and result checking.
tangleR() and weaveR() implement a plain kind of tangling
and weaving. slider() is designed to define sliders for interactive
experiments in a simple way.
The windows version uses some ingredients of the noweb system of Norman Ramsey
(http://www.eecs.harvard.edu/~nr/noweb/intro.html),
of the Img package of Jan Nijtmans (http://www.xs4all.nl/~nijtmans/img.html),
-- notice the copyright declarations of these products.

Relaxed Lasso is a generalisation of the Lasso shrinkage technique for linear regression. Both variable selection and parameter estimation is achieved by regular Lasso, yet both steps do not necessarily use the same penalty parameter. The results include all standard Lasso solutions but allow often for sparser models while having similar or even slightly better predictive performance if many predictor variables are present. The package depends on the LARS package.

R functions for the comparison of distributions.
This includes nonparametric estimation of the relative distribution
PDF and CDF and numerical summaries as described in
"Relative Distribution Methods in the Social Sciences"
by Mark S. Handcock and Martina Morris,
Springer-Verlag, 1999, Springer-Verlag, ISBN 0387987789

Functions for estimating parameters in software reliability models.
Only infinite failure models are implemented so far.

Functions to facilitate inference on the relative importance of predictors in a linear or generalized linear model, and a couple of useful Tcl/Tk widgets

Various functions for
regression in relative survival.

Reshape lets you flexibly restructure and aggregate data using just two functions: melt and cast.  See http://had.co.nz/reshape for more information.

electrical properties of resistor networks.

Two accept-and-reject algorithms to sample from permutations

Adjusts the weights of survey repondents so that the marginal distributions of certain variables fit more closely to those from a more precise source (e.g. Census Bureau's data).

Several function to perform a Regional Frequency Analysis

Graphics for statistics on a sphere, as applied to geological fault data, crystalogaphy, earthquake focal mechanisms, radiation patterns, ternary plots and geographical/geological maps.

An R interface to a modified version of the Free Evolutionary Algorithm Kit FrEAK. FrEAK is a toolkit written in Java to design and analyze evolutionary algorithms. Both the R interface an extended version of FrEAK are contained in the RFreak package. For more information on FrEAK see http://sourceforge.net/projects/freak427/.

Explore Multivariate Data with the Forward Search

Thin plate spline fitting and prediction

Provides bindings to Frank Warmerdam's Geospatial Data Abstraction Library (GDAL) (&gt;= 1.3.1) and access to projection/transformation operations from the PROJ.4 library. The GDAL and PROJ.4 libraries are external to the package, and, when installing the package from source, must be correctly installed first. Both GDAL raster and OGR vector map data can be imported into R, and GDAL raster data and OGR vector data exported. Use is made of classes defined in the sp package.

A genetic algorithm plus derivative optimizer

Functions for fitting Gaussian Graphical Models with Robustified Methods

The rggobi package provides a command-line interface to GGobi, an  interactive and dynamic graphics package. Rggobi complements GGobi's graphical user interface, providing a way to fluidly transition between analysis and exploration, as well as automating common tasks.

3D visualization device (OpenGL)

R interface to the GNU Linear Programing Kit (GLPK version 4.25).
GLPK is open source software for solving large-scale linear programming (LP),
mixed integer linear programming (MILP) and other related problems.

Interactive charting application.

Data and Functions from the book R Graphics.  There is a
function to produce each figure in the book, plus several functions,
classes, and methods defined in Chapter 7.

Interfaces R with the AT&T GraphViz library to provide
the ability for plotting R graph objects from the graph
package

Geological Survey of Canada (GSC) R functions for exploratory data analysis with applied geochemical data, with special application to the estimation of background ranges to support both environmental studies and mineral exploration.

Facilities in the R language for programming
graphical interfaces using Gtk, the Gimp Tool Kit.

Discrete, univariate or multivariate gaussian,
mixture of univariate or multivariate gaussian HMM functions for simulation and estimation.

Evaluating risk (that a patient arises a side effect) during hospitalization is the main purpose of this package. Several methods (Parametric, non parametric and De Vielder estimation) to estimate the risk constant (R) are implemented in this package. There are also functions to simulate the different models of this issue in order to quantify the previous estimators. It is necessary to read the first six pages of the report to understand the problem.

The package currently provides a class representing a matrix where the actual data is stored in a binary format on the local file system.  This way the size limit of the data is set by the file system and not the memory.  Please consider this package to in an alpha/early-beta version.

RiboSort is an R package for rapid classification of (TRFLP &
ARISA) microbial community profiles. Its function is to eliminate the
laborious task of manually classifying community fingerprints in
microbial studies.
By automatically assigning detected fragments and their respective
relative abundances to appropriate ribotypes, RiboSort saves time and
greatly simplifies the preparation of DNA fingerprint datasets for
statistical analysis.

A package to provide small integer group functions

Estimation of the relative index of inequality for interval-censored data using natural cubic splines

This package provides functions for image processing,
including sobel filter, rank filters, fft, histogram equalization, and
reading JPEG file. This package requires fftw-2 &lt;http://www.fftw.org/&gt;
and libjpeg &lt;http://www.ijg.org&gt;. This version doesn't require
pixmap package, which the older version of rimage (private only) required.
This package can be used on Unixes / MacOS X / Windows.

Index structures allow quickly accessing elements from large collections.
While btree are optimized for disk databases and ttree for ram databases we use hybrid static indexing which is quite optimal for R.

Compute time-dependent Incident/dynamic accuracy measures
(ROC curve, AUC, integrated AUC )from censored survival
data under proportional or non-proportional hazard
assumption  of  Heagerty & Zheng (Biometrics, Vol 61 No 1,
2005, PP 92-105).

Finds a robust instrumental variables estimator using a high breakdown point S-estimator of multivariate location and scatter matrix.

Bayesian analysis of CGH microarrays fitting Hidden Markov Chain models.
The selection of the number of states is made via their posterior probability
computed by Reversible Jump Markov Chain Monte Carlo Methods. Also
returns probabilistic minimal common regions for gains/losses.

A library that computes Jacobi polynomials and Gauss-Jacobi quadrature related operations

Low-level interface to Java VM very much like .C/.Call and friends. Allows creation of objects, calling methods and accessing fields.

RJDBC is an implementation of R's DBI interface using JDBC as a back-end. This allows R to connect to any DBMS that has a JDBC driver.

Converts R object into JSON objects and vice-versa

Functions and Datasets Required for ST370 class

Analysis of small scale infectious disease data using stochastic
Susceptible-Exposed-Infectious-Recovered (SEIR) models.
The R package  wraps functionality of an Java program,
i.e. a java virtual machine has to be installed on your computer.

Provides an interface to the C implementation of the random number generator with multiple independent streams developed by L'Ecuyer et al (2002). The main purpose of this package is to enable the use of this random number generator in parallel R applications.

A classification algorithm, based on a multi-chip, multi-SNP approach for Affymetrix SNP arrays. Using a large training sample where the genotype labels are known, this aglorithm will obtain more accurate classification results on new data. RLMM is based on a robust, linear model and uses the Mahalanobis distance for classification. The chip-to-chip non-biological variation is removed through normalization. This model-based algorithm captures the similarities across genotype groups and probes, as well as thousands other SNPs for accurate classification. NOTE: 100K-Xba only at for now.

Rapid, simulation-based exact (restricted) likelihood ratio tests for testing the presence of variance components/nonparametric terms with a convenient interface for models fit with nlme::lme(),lme4::lmer(), mgcv::gamm() and SemiPar:spm()

This package provides functions for using R with the
LSF cluster/grid queuing system.

This package provides methods to read and write MAT files.  It also makes it possible to communicate (evaluate code, send and retrieve objects etc.) with Matlab v6 or higher running locally or on a remote host.

Rmdr: R package of the MDR approach That is a nonparametric and genetic model-free alternative
to logistic regression for detecting and characterizing
nonlinear interactions among discrete genetic and
environmental attributes.

Functions for simple fixed and random effects meta-analysis
for two-sample comparisons and cumulative meta-analyses. Draws standard
summary plots, funnel plots, and computes summaries and tests for
association and heterogeneity

An interface between R and the metasim simulation engine.
Facilitates the use of the metasim engine to build and
run individual based population genetics simulations. The
simulation environment is documented in: Allan
Strand. Metasim 1.0: an individual-based environment for
simulating population genetics of complex population
dynamics. Mol. Ecol. Notes, 2:373-376, 2002. (Please
contact Allan Strand with comments, bug reports, etc).
This version represents a significant alteration of
function names that hopefully increases consistency and
reduces the chances of collisions with other packages
naming conventions.  For a spatially-explicit, but slower, package
with a similar interface, see kernelPop

Methods that simplify the setup of S3 generic functions and S3 methods.  Major effort has been made in making definition of methods as simple as possible with a minimum of maintainance for package developers.  For example, generic functions are created automatically, if missing, and name conflict are automatically solved, if possible.  The method setMethodS3() is a good start for those who in the future want to migrate to S4.  This is a cross-platform package implemented in pure R and is generating standard S3 methods.

Environment for teaching "Financial Engineering and Computational Finance"

Rmpi provides an interface (wrapper) to MPI APIs. It also
provides interactive R slave environment.

Database interface and MySQL driver for R.
This version complies with the database interface
definition as implemented in the package DBI 0.2-2.

This package provides an interface to Unidata's NetCDF library
functions (version 3) and furthermore access to Unidata's udunits calendar
conversions. The routines and the documentation follow the NetCDF and udunits
C interface, so the corresponding manuals can be consulted for more
detailed information.

A set of functions to filter time series based on
concepts from robust statistics.

functions for the determination of optimally
robust influence curves in case of normal
location with unknown scale

functions for the determination of optimally
robust influence curves in case of linear regression
with unknown scale and standard normal distributed errors
where the regressor is random.

"Essential" Robust Statistics.  The goal is to provide
tools allowing to analyze data with robust methods.  This includes
regression methodology including model selections and multivariate
statistics where we strive to cover the book "Robust Statistics,
Theory and Methods" by Maronna, Martin and Yohai; Wiley 2006.

A package of robust methods.

ROC graphs, sensitivity/specificity curves, lift charts,
and precision/recall plots are popular examples of trade-off
visualizations for specific pairs of performance measures. ROCR is a
flexible tool for creating cutoff-parametrized 2D performance curves
by freely combining two from over 25 performance measures (new
performance measures can be added using a standard interface).
Curves from different cross-validation or bootstrapping runs can be
averaged by different methods, and standard deviations, standard
errors or box plots can be used to visualize the variability across
the runs. The parametrization can be visualized by printing cutoff
values at the corresponding curve positions, or by coloring the
curve according to cutoff. All components of a performance plot can
be quickly adjusted using a flexible parameter dispatching
mechanism. Despite its flexibility, ROCR is easy to use, with only
three commands and reasonable default values for all optional
parameters.

An ODBC database interface

Methods and classes for object-oriented programming in R with or without references.  Large effort has been made on making definition of methods as simple as possible with a minimum of maintainance for package developers.  The package has been developed since 2001 and is now considered very stable.  This is a cross-platform package implemented in pure R that defines standard S3 classes without any tricks.

Optimally robust estimation using S4 classes and methods

Optimally robust estimation for regression-type
models using S4 classes and methods

Oracle database interface (DBI) driver for R.
This is a DBI-compliant Oracle driver based on the ProC/C++
embedded SQL.  It implements the DBI version 0.1-8 plus one
extension.

A workbook-style user interface to R through a web browser.
Provides convenient plotting, HTML GUI generation, and HTML
output routines. Can be used with R in standalone mode or with
a webserver to serve Rpad pages to other users.

rpanel provides a set of functions to build simple
GUI controls for R functions.  These are built on the tcltk package.
Uses could include changing a parameter on a graph by animating it
with a slider or a "doublebutton", up to more sophisticated control panels.

Recursive partitioning and regression trees

Performs permutation tests of rpart models.

Poor Man's Gui: create interactive R analysis sessions

Access PubChem data (compounds, substance, assays). Structural information
is provided in the form of SMILES strings. This package only provides access
to a subset of the precalculated data stored by PubChem. Bio-assay data can
be accessed to obtain descriptions as well as the actual data. It is also
possible to search for assay ID's by keyword. Currently the main limitation is
that only 1000 molecules can be downloaded at a time from the PubChem servers

Provides interface to PVM APIs, and examples and
documentation for its use.

Provide access to (virtually any) ArcGIS Geoprocessing
tool from within R by running Python geoprocessing scripts
without writing Python code or touching ArcGIS.
Requires ArcGIS &gt;=9.2, a suitable version of Python
(currently 2.4), and Windows.

Markov Chain Marginal Bootstrap for Quantile Regression. A resampling
method for inference in quantile regression.  Suitable for modest to large data sets.

The RQuantLib package makes selected parts of QuantLib
visible to the R user. Currently some basic option pricing
functions are included, as well as fixed-income functions that
can be used for interest rate curve construction and Bermuda
swaption pricing. Further software contributions are welcome.

The QuantLib project aims to provide a comprehensive software
framework for quantitative finance. The goal is to provide a
standard open source library for quantitative analysis,
modeling, trading, and risk management of financial
assets.

The Windows binary version is self-contained and does not require
a QuantLib (or Boost) installation. This version of RQuantLib for
Windows was built using QuantLib 0.8.1 and Boost 1.34.0.

Parts of RQuantLib use the Rcpp R/C++ interface class library.
See the RcppTemplate package on CRAN for more information on Rcpp.

Note that while RQuantLib's code is licensed under the GPL (v2 or
later), QuantLib itself is released under a somewhat less
restrictive Open Source license (see QuantLib-License.txt).

Robust Location and Scatter Estimation and Robust Multivariate Analysis with High Breakdown Point.

Random Recursive Partitiong and Rank-based proximities for data matching, missing data imputation and nonparametric classification and prediction

An R Server Page (RSP) is a document that contains both text in a format of choice (HTML, TeX, ...) as well as R source code within special tags.  An RSP file can be translated into a so called R servlet, which is an R script that outputs the final document when sourced.  This way documents in any format can be generated dynamically using R, e.g. automatic reports of statistical analysis.  Utilizing an internal cross-platform web server, this package provides dynamic help pages in HTML.  If other packages provide RSP help pages, these are automatically linked to in the RSP main menu.

The functions in this package are mostly designed to
reproduce the most commonly used functions in the LLNL software
Seismic Analysis Code (SAC, http://www.llnl.gov/sac/). This includes
reading standard binary SAC (and also SEGY) files, plotting
arrays of seismic recordings (the "SAC" plotting style), filtering
(high pass, lowpass, and bandpass IIR filters), integration,
differentiation, instrument deconvolution.

RSAGA provides access to geocomputing and terrain analysis functions of SAGA from within R by running the command line version of SAGA. In addition, several R functions for handling and manipulating ASCII grids are provided, including a flexible framework for applying local or focal functions to grids. SAGA is available under GPL via http://www.saga-gis.org/.

Links R to libsbml for SBML parsing and output,
provides an S4 SBML DOM, converts SBML to R graph objects, and more.

An R add-on package capable of carrying out parallel computation through a single function call from the R environment. It uses the high-performance ScaLAPACK library for the linear algebra computations.

Multiple interactive codes to analyze seismic data, via, spectrum analysis, wavelet transforms, particle motion, hodograms.

Rserve acts as a socket server (TCP/IP or local sockets)
which allows binary requests to be sent to R. Every
connection has a separate workspace and working
directory. Client-side implementations are available
for popular languages such as C/C++ and Java, allowing
any application to use facilities of R without the need of
linking to R code. Rserve supports remote connection,
user authentication and file transfer. A simple R client
is included in this package as well.

Provides interface to SPRNG 2.0 APIs, and examples and
documentation for its use.

Database Interface R driver for SQLite.
This package embeds the SQLite database engine in R and
provides an interface compliant with the DBI package.
The source for the SQLite engine (version 3.4.1) is included.

Unified object oriented interface for multiple independent streams of random numbers from different sources.

This package provides an interface for the package of nonlinear differential algebraic equation solvers that comprise SUNDIALS. ODEs are expressed as R functions or as compiled code.

A graphics device for R that uses the w3.org xml
standard for Scalable Vector Graphics.

A graphics device for R that uses the w3.org xml standard
for Scalable Vector Graphics.  This version supports
tooltips with 1 to 3 lines, hyperlinks, and line styles.

An R interface to the SYMPHONY MILP solver (version 5.1.7).

This package will read (and, as of version 1.1, write) TIFF format images and return them as a pixmap object. Because the resulting object can be very large for even modestly sized TIFF images, images can be reduced as they are read for improved performance.  This package is a wrapper around libtiff (www.libtiff.org), on which it depends (i.e. the libtiff shared library must be on your PATH for the binary to work, and tiffio.h must be on your system to build the package from source).  By using libtiff's highlevel TIFFReadRGBAImage function, this package inherently support a wide range of image formats and compression schemes.  This package also provides an implementation of the Ridler Autothresholding algorithm for easy generation of binary masks.

Algorithms for time series analysis from nonlinear dynamical systems theory originally made available by Rainer Hegger, Holger Kantz and Thomas Schreiber at the site http://www.mpipks-dresden.mpg.de/~tisean/ .  A related R package (tseriesChaos by Antonio, Fabio Di Narzo) contains rewritten versions of a few of the TISEAN algorithms.   The intention of the present package is to use the TISEAN routines from within R with no need of manual importing/exporting.  It is in a beta version state, though most of the functions should be usable.  Correspondence should be sent to either Marji Lines, lines@dss.uniud.it, or to the current maintainer of the package. This package only contains R interface code. It requires that you have the Tisean-3.0.1 algorithms available on your computer.

Created mainly for use with seismic tomography, this program
plots tomographic images, and allows one to interact and query
three-dimensional tomographic models.
Vertical cross-sectional cuts can be extracted by mouse click.
Geographic information can be added easily.

R functions implementing a standard Unit Testing framework, with additional code inspection and report generation tools

Interface to the UNU.RAN library for Universal Non-Uniform RANdom variate generators

This package provides utility classes and methods useful when programming in R and developing R packages.

R scripts for interactively analyzing downhole
seismic data and interpreting layered velocity models of
constant velocity layers accounting for refractions across
layer boundaries.

Simulation-based random variable object class

Rwave is a library of R functions which provide an
environment for the Time-Frequency analysis of 1-D signals (and
especially for the wavelet and Gabor transforms of noisy signals).
It was originally written for Splus by Rene Carmona, Bruno Torresani,
and Wen L. Hwang, first at the University of California at Irvine and
then at Princeton University.  Credit should also be given to Andrea
Wang whose functions on the dyadic wavelet transform are included.
Rwave is based on the book:
"Practical Time-Frequency Analysis: Gabor and Wavelet Transforms with
an Implementation in S", by Rene Carmona, Wen L. Hwang and Bruno
Torresani, Academic Press, 1998.

An R interface to Weka (Version 3.5.7).
Weka is a collection of machine learning algorithms for data mining
tasks written in Java, containing tools for data pre-processing,
classification, regression, clustering, association rules, and
visualization. Both the R interface and Weka itself are contained
in the RWeka package. For more information on Weka see
http://www.cs.waikato.ac.nz/~ml/weka/.

A plug in for using WinEdt as an editor for R

Provides a set of functions for performing digital signal processing

Identify and display TRACEs for a specified shrinkage path and determine the extent of shrinkage most likely, under normal distribution theory, to produce an optimal reduction in MSE Risk in estimates of regression (beta) coefficients.

An interface to the yacas computer algebra system.

Stats 20x functions.

Semiparametric empirical likelihood ratio
based test of changepoint with one-change or epidemic alternatives
with data-based model diagnostic

Implements a modified version of the Sampford sampling
algorithm. Given a quantity assigned to each unit in the
population, samples are drawn with probability proportional to the
product of the quantities of the units included in the sample.

Functions for drawing and calibrating samples.

Significance Analysis of Microarrays

Model-robust standard error estimators for cross-sectional,
time series and longitudinal data.

Software for the book Spectral Analysis for Physical Applications, Donald B. Percival and Andrew T. Walden, Cambridge University Press, 1993.

Data sets and sample lmer analyses corresponding
to the examples in Littel, Milliken, Stroup and Wolfinger
(1996), "SAS System for Mixed Models", SAS Institute.

This package provides functions for reading, listing
the contents of, and writing SAS xport format files.
The functions support reading and writing of either
individual data frames or sets of data frames.  Further,
a mechanism has been provided for customizing how
variables of different data types are stored.

This package estimates parameters of a Gaussian
copula, treating the univariate marginal distributions
as nuisance parameters as described in Hoff(2007). It also
provides a semiparametric imputation procedure for missing
multivariate data.

Calculates Tukey's scagnostics.  Scagnostics describe various measures of interest for pairs of variables, based on their appearance on a scatterplot.  They are useful tool for weeding out interesting or unusual scatterplots from a scatterplot matrix, without having to look at ever individual plot.

Simple Component Analysis often provides much more interpretable
components than Principal Components (PCA) without losing too much.

Calculating approximately unbiased (AU) p-values
from multiscale bootstrap probabilities.

Import and plot results from statistical catch-at-age
models, used in fisheries stock assessments.

Markov-chain Monte Carlo diagnostic plots, accompanying the 'scape'
package. The purpose of the package is to combine existing tools from the
'coda' and 'lattice' packages, and make it easy to adjust graphical details.
It can be useful for anyone using MCMC analysis, regardless of the
application.

Plots a three dimensional (3D) point cloud.

This package contains functions and datasets for math taught in school. A main focus is set to prime-calculation

A collection of functions that creates graphs with error
bars for data collected from one-way or higher factorial
designs.

SciViews GUI API
A series of packages to implement a complete reusable GUI API for R.


Calculate, per data frame row, a value that depends on information in a
relevant subset of rows and columns.  These functions create and refine
scope objects, which identify relevant rows on a per-row basis. Columns
can be aggregated within relevant scopes to aid identification of a row
of interest, from which a value in an arbitrary column can be selected.
Version 2 is optimized for fast execution.  Version 2.1 adds a formatting
command for display of scope objects in data.frames, etc; skim is enhanced.
Version 2.2 supports scope objects referenced as columns in the data frame.

Tools for the analysis of high-dimensional data developed/implemented
at the group "Statistical Complexity Reduction In Molecular Epidemiology" (SCRIME).
Main focus is on SNP data. But most of the functions can also be applied to other
types of categorical data.

dive profiles, decompression models and gas calculations
for scuba diving

Data from statistical agencies and other institutions are mostly confidential. This package can be used for the generation of save (micro)data, i.e. for the generation of public- and scientific-use files.

Companion package to the book 'Simulation and Inference for
Stochastic Differential Equations With R Examples',
ISBN 978-0-387-75838-1, Springer, NY.

Calculates parameters of the seawater carbonate system

Capable of deriving seasonal statistics, such as "normals", and
analysis of  seasonal data, such as departures. This package also has
graphics capabilities for representing seasonal data, including boxplots for
seasonal parameters, and bars for summed normals. There are many specific
functions related to climatology, including precipitation normals,
temperature normals, cumulative precipitation departures and precipitation
interarrivals. However, this package is designed to represent any
time-varying parameter with a discernible seasonal signal, such as found in
hydrology and ecology.

seewave provides functions for analysing, manipulating, displaying, editing and synthesizing time waves (particularly sound).  This package processes time analysis (oscillograms and envelopes), spectral content, resonance quality factor, cross correlation and autocorrelation, zero-crossing, dominant frequency, 2D and 3D spectrograms and many other analyses.

SegClust corresponds to the implementation of the statistical model described in : Picard et al., A segmentation/clustering model for the analysis of array CGH data. Biometrics, 63(3) 2007. Segmentation functions are also available (from Picard et al. A statistical approach for array CGH data analysis. BMC Bioinformatics. 2005 Feb 11;6:27).

Given a (generalized) linear model, segmented 'updates' the model by adding one or more
segmented relationships. Several variables with multiple breakpoints are allowed.

This package contains functions for fitting general linear structural
equation models (with observed and unobserved variables) by the method of
maximum likelihood using the RAM approach, and for fitting structural equations
in observed-variable models by two-stage least squares.

Functions for semiparametric regression analysis,
to complement the book:
Ruppert, D., Wand, M.P. and Carroll, R.J. (2003).
Semiparametric Regression. Cambridge University Press.

A tool for visualizing data

This package allows to perform sensitivity analyses within the R
environment. Implemented methods are : linear sensitivity analysis (SRC,
PCC, rank analysis), the screening method of Morris, the Sobol global
sensitivity indices (two methods of estimation), and the FAST method.

an R package for analysing sensory data

Collection of datasets from Sen & Srivastava:
Regression Analysis, Theory, Methods and Applications,
Springer.  Sources for individual data files are more fully
documented in the book.

Exploratory data analysis and data visualization
for biological sequence (DNA and protein) data. Include also
utilities for sequence data management under the ACNUC system.

This function estimates missing values sequentially from the gene that had least missing rate in microarray data

A program that computes the probability of crossing sequential boundaries in a
clinical trial. It implements the Armitage-McPherson and Rowe Algorithm using the method
described in Schoenfeld D. (2001)" A simple Algorithm for Designing Group Sequential Clinical
Trials" Biometrics 27: pp, 972-974

Infrastructure for seriation with an implementation of several
seriation/sequencing techniques to reorder matrices, dissimilarity
matrices, and dendrograms. Also contains some visualizations techniques
based on seriation.

Utility functions for interacting with R processes from
external programs. This package includes functions to
save and restore session information (including loaded packages,
and attached data objects), as well as functions to
evaluate strings containing R commands and return the
printed results or an execution transcript.

Set reproducible random number generator in R (and S). See ?setRNG.Intro for more details.

Data structures and basic operations for ordinary sets,
and generalizations such as fuzzy sets, multisets, and
fuzzy multisets.

Useful utilities ['goodies'] from Seminar fuer Statistik ETH
Zurich, many ported from S-plus times.

An Object-oriented Framework for Geostatistical Modeling in S+

Functions to read and write ESRI shapefiles

Plotting functions for creating graphical shapes
such as ellipses, circles, cylinders, arrows, ...
Support for the book "A guide to ecological modelling" by Karline Soetaert and Peter Herman (in preparation).
Includes demo(colorshapes)

Routines for the statistical analysis
of shapes. In particular, the package provides routines for
procrustes analysis, displaying shapes and principal components,
testing for mean shape difference, thin-plate spline transformation
grids and edge superimposition methods.

Derives per gene means and fits the wishart/inverse wishart conjugate
family to the per gene empirical covariance matrices.  Derives a Hotelling T2
statistic having an F-distribution using an empirical bayes variance.

This package takes data on organism isotopes and fits a Bayesian model to their dietary habits based upon a Gaussian likelihood with a Dirichlet prior on the mean.

This package contents a set of functions
to investigate with sigma2 parameter

The Direct Filter Approach (DFA) provides efficient estimates of
signals at the current boundary of time series in real-time. For
that purpose, one-sided ARMA-filters are computed by minimizing
customized error criteria. The DFA can be used for estimating
either the level or turning-points of a series, knowing that both
criteria are incongruent. In the context of real-time turning-
point detection, various risk-profiles can be operationalized,
which account for the speed and/or the reliability of the one-
sided filter.

A set of generally Matlab/Octave-compatible signal processing
functions. Includes filter generation utilities, filtering functions,
resampling routines, and visualization of filter models. It also
includes interpolation functions and some Matlab compatability
functions.

Besides a function for the calculation of similarity measures with
binary data (for instance presence/absence species data) the package
contains some simple wrapper functions for reshaping species lists into
matrices and vice versa and some other functions for further processing
of similarity data (Mantel-like permutation procedures) as well as some other useful stuff.

Simco is a package that (1) imports Structure output files into R and (2) carries out similarity coefficient calculations on them.

Object oriented framework to simulate ecological (and other) dynamic systems

Implementation of the SIMEX-Algorithm by Cook & Stefanski and MCSIMEX by Kchenhoff, Mwalili & Lesaffre

SimHap is a package for genetic association testing. It can perform single SNP and multi-locus (haplotype) association analyses for continuous Normal, binary, longitudinal and right-censored outcomes measured in population-based samples. SimHap uses estimation maximisation techniques for inferring haplotypic phase in individuals, and incorporates a multiple-imputation approach to deal with the uncertainty of imputed haplotypes in association testing.

Simple bootstrap routines

This package provides routines to perform SIN model selection
as described in Drton & Perlman (2004).  The selected models are
represented in the format of the 'ggm' package, which allows in
particular parameter estimation in the selected model.

Density, distribution function, quantile function and random generation for the skewed t distribution of Fernandez and Steel.

Miscellaneous Functions for analysis of gene expression data at SIRS-Lab GmbH

The package contains some simple functions for
exploratory microarray analysis.

This package provides methods of fitting bivariate lines in allometry using the major axis (MA) or standardised major axis (SMA), and for making inferences about such lines. The available methods of inference include confidence intervals and one-sample tests for slope and elevation, testing for a common slope or elevation amongst several allometric lines, constructing a confidence interval for a common slope or elevation, and testing for no shift along a common axis, amongst several samples.

provides functions for particle filtering, auxiliary particle filtering and sequential Monte Carlo algorithms

This is software linked to the book
'Applied Smoothing Techniques for Data Analysis:
The Kernel Approach with S-Plus Illustrations' Oxford University Press.

This package contains primarily a function to fit
a regression model with possibly right, left or interval
censored observations and with the error distrbution
expressed as a mixture of G-splines. Core part
of the computation is done in compiled C++ written
using the Scythe Statistical Libary Version 0.3.

Given independent and identically distributed observations X(1), ..., X(n) from a Generalized Pareto
distribution with shape parameter gamma in [-1,0], this package offers several estimates to compute estimates of gamma.
The estimates are based on the principle of replacing the order statistics by quantiles of a distribution
function based on a log--concave density function. This procedure is justified by the fact that the GPD density
is log--concave for gamma in [-1,0].

This package contains the datasets and a few functions
for use with the practicals outlined in Appendix A of the book
Statistical Models (Davison, 2003, Cambridge University Press).
The practicals themselves can be found at
http://statwww.epfl.ch/davison/SM/

A range of tools for social network analysis, including node and graph-level indices, structural distance and covariance methods, structural equivalence detection, p* modeling, random graph generation, and 2D/3D network visualization.

Functions for manipulating skew-normal and skew-t probability
distributions, and for fitting them to data, in the scalar and in the
multivariate case.

Snowball stemmers.

Extension of the snow package supporting fault tolerant and reproducible applications. It is written for the PVM communication layer.

Support for simple parallel computing in R.

This package carries out most common analysis when performing whole genome association studies. These analyses include descriptive statistics and exploratory analysis of missing values, calculation of Hardy-Weinberg equilibrium, analysis of association based on generalized linear models (either for quantitative or binary traits), and analysis of multiple SNPs (haplotype and epistasis analysis). Permutation test and related tests (sum statistic and truncated product) are also implemented.

This package implements asymptotic methods related to maximally selected statistics, with applications to SNP data.

Creates plots of p-values using single SNP and/or haplotype data. Main features of the package include options to display a linkage disequilibrium (LD) plot and the ability to plot multiple datasets simultaneously. Plots can be created using global and/or individual haplotype p-values along with single SNP p-values. Images are created as either PDF/EPS files.

This package is designed to analyze SNP data

Utilities and examples from the book
"Software for Data Analysis: Programming with R".

Self-Organizing Map (with application in gene clustering)

SWMS_2D interface

Basic functions for dealing with wav files and sound samples.

Set of function for sparse matrix algebra.
The package 'fields' v4.1 uses spam as a required package.
Differences with SparseM/Matrix are: (1) we only support
one sparse matrix format, (2) based on transparent and
simple structure(s) (3) S3 and S4 compatible.

Some functions wrapping the sparse logistic regression code by S. K. Shevade and S. S. Keerthi originally intended for microarray-based gene selection

Basic linear algebra for sparse matrices

Multiple cluster location and detection for 2D and 3D spatial point patterns (case event data). The methodology of this package is based on an original method that allows the detection of multiple clusters of any shape. A selection order and the distance from its nearest neighbour once pre-selected points have been taken into account are attributed at each point. This distance is weighted by the expected distance under the uniform distribution hypothesis. Potential clusters are located by modelling the multiple structural change of the distances on the selection order. Their presence is tested using the double maximum test and a Monte Carlo procedure. The main function of this R package is "clus".

Graphs, graph visualization and graph based summaries to be used as a tool in
spatial point pattern analysis. See package 'spatstat' for more info about spatial point patterns.

Functions that compute the spatial covariance matrix for the matern and power classes of spatial models, for data that arise on rectangular units.  This code can also be used for the change of support problem and for spatial data that arise on irregularly shaped regions like counties or zipcodes by laying a fine grid of rectangles and aggregating the integrals in a form of Riemann integration.

Edge-corrected kernel density estimation and binary kernel
regression estimation for multivariate spatial point process data

This package contains test and estimates of location, tests of independence, tests of sphericity, several estimates of shape and regression all based on spatial signs, symmetrized signs, ranks and signed ranks.

A package for analysing spatial data,
mainly Spatial Point Patterns, including multitype/marked points
and spatial covariates, in any two-dimensional spatial region.
Contains functions for plotting spatial data,
exploratory data analysis, model-fitting, simulation,
spatial sampling, model diagnostics,
and formal inference. Data types include point patterns,
line segment patterns, spatial windows, and
pixel images. Point process models can be
fitted to point pattern data. Cluster type models are fitted
by the method of minimum contrast. Very general Gibbs point
process models can be fitted to point pattern data
using a function ppm similar to lm or glm. Models may
include dependence on covariates, interpoint interaction
and dependence on marks. Fitted models can be simulated automatically.
Also provides facilities for formal inference (such as chi-squared
tests) and model diagnostics (including simulation
envelopes, residuals, residual plots and Q-Q plots).

spBayes fits Gaussian univariate and multivariate models with Markov chain Monte Carlo (MCMC).

Evaluation of control charts by means of
the zero-state, steady-state ARL (Average Run Length).
Setting up control charts for given in-control ARL
and plotting of the related figures. The control charts
under consideration are one- and two-sided EWMA and
CUSUM charts for monitoring the mean of normally
distributed independent data. Now, the ARL calculation
of EWMA-S^2 control charts is added.
Other charts and
parameters are in preparation. Further SPC areas will
be covered as well (sampling plans, capability indices ...).

A collection of functions to create spatial weights matrix
objects from polygon contiguities, from point patterns by distance and
tesselations, for summarising these objects, and for permitting their
use in spatial data analysis; a collection of tests for spatial
autocorrelation, including global Moran's I, Geary's C, Hubert/Mantel
general cross product statistic, Empirical Bayes estimates and Assuno/Reis
Index, Getis/Ord G and multicoloured join count statistics,
local Moran's I and Getis/Ord G, saddlepoint approximations  and exact
tests for global and local Moran's I; and functions for estimating
spatial simultaneous autoregressive (SAR) lag and error models,
weighted and unweighted SAR and CAR spatial regression models,
semi-parametric and Moran eigenvector spatial filtering, GM SAR error
models, and generalized spatial two stage least squares models.

Routines for creating, manipulating, and performing
Bayesian inference about Gaussian processes in
one and two dimensions using the Fourier basis approximation:
simulation and plotting of processes, calculation of
coefficient variances, calculation of process density,
coefficient proposals (for use in MCMC).  It uses R environments to
store GP objects as references/pointers.

Spectra organizer, visualization and data extraction
from within R

Implements stochastic proximity embedding as described by
Agrafiotis et al. in PNAS, 2002, 99, pg 15869 and J. Comput. Chem., 2003,24, pg 1215

Interpreted interface between GRASS 6 geographical
information system and R, based on starting R from within the GRASS
environment.

Functions for computing geographically weighted
regressions based on work by Chris
Brunsdon, Martin Charlton and Stewart Fortheringham,
http://ncg.nuim.ie/ncg/GWR/index.htm

This package carries out
spherical wavelet transform developed by Li (1999) and Oh (1999), and
implements wavelet thresholding approaches proposed by Oh and Li
(2004).

A package that provides classes and methods for spatial
data. The classes document where the spatial location information
resides, for 2D or 3D data. Utility functions are provided, e.g. for
plotting data as maps, spatial selection, as well as methods for
retrieving coordinates, for subsetting, print, summary, etc.

Spatial and Space-Time Point Pattern Analysis Functions

Currently there are many functions in S-PLUS that are
missing in R. To facilitate the conversion of S-PLUS packages to R
packages, this package provides some missing S-PLUS functionality
in R.

This group of functions implements algorithms required for design
and analysis of probability surveys such as those utilized by the U.S.
Environmental Protection Agency's Environmental Monitoring and Assessment
Program (EMAP).

Manipulate R data frames using SQL.

Transparently stores data frames & matrices into SQLite tables.

A package development and management system for distributed reproducible research

A set of functions to calculate sample size for two-sample difference in means tests.
Does adjustments for either nonadherence or variability that comes from using data to estimate parameters.

A glm-like formula language to define dynamic generalized linear models (state space models). Includes functions for Kalman filtering and smoothing. Read help(sspir) to get started.

R package for Computing the Spherical Smoothing Splines

Una libreria per utilizzare con semplicit
le tecniche di statistica inferenziale presenti sulla
calcolatrice scientifica grafica TI-83 Plus

Utilities for start-up messages

A Set of Tools for Administering SHared Repositories

This package offers different possibilities to make statistical analysis for Environmental Data.

read and write StatDataML files

Various statistical modeling functions including growth curve comparisons, limiting dilution analysis, mixed linear models, heteroscedastic regression, Tweedie family generalized linear models, the inverse-Gaussian distribution and Gauss guadrature.

An integrated set of tools for the representation, visualization, analysis and simulation of network data. This package For an introduction type: help(package='statnet')

L2 penalized logistic regression for both continuous and discrete predictors, with the forward stepwise variable selection procedure.

A stepwise approach to identifying recombination breakpoints in a sequence alignment.

This package implements the "shrinkage t" statistic
described in Opgen-Rhein and Strimmer (2007).  It also offers
a convenient interface to a number of other regularized t-type
statistics often used in high-dimensional case-control studies.

A consistently well behaved method of interpolation based on piecewise rational functions using Stineman's algorithm

stochasticGEM is a publicly available package that
implements Bayesian inference for partially observed stochastic
epidemics. The general epidemic model is used for estimating
the parameters governing the infectious and incubation period length,
and the parameters governing susceptibility. In real-life epidemics
the infection process is unobserved, and the data consists of the
times individuals are detected, usually via appearance of symptoms.
The stochasticGEM package fits several variants of the general
epidemic model, namely the stochastic SIR with Markovian and
nonMarkovian infectious periods. The estimation is based on Markov chain
Monte Carlo algorithm.

Learning and inference algorithms for a variety of probabilistic models

Functions with example data for creating, importing, attributing,
analyzing, and displaying stream networks represented as binary trees.  Capabilities
include upstream and downstream distance matrices, stochastic network generation,
segmentation of network into reaches, adding attributes to reaches with specified
statistical distributions, interpolating reach attributes from sparse data,
analyzing autocorrelation of reach attributes, and creating maps with legends
of attribute data.  Target applications include dynamic fish modeling.

Testing, monitoring and dating structural changes in (linear)
regression models. strucchange features tests/methods from
the generalized fluctuation test framework as well as from
the F test (Chow test) framework. This includes methods to
fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM,
recursive/moving estimates) and F statistics, respectively.
It is possible to monitor incoming data online using
fluctuation processes.
Finally, the breakpoints in regression models with structural
changes can be estimated together with confidence intervals.
Emphasis is always given to methods for visualizing the data.

The subplex algorithm for unconstrained optimization, developed by Tom Rowan.

A collection of functions which (i) assess the quality of variable subsets as surrogates for a full data set, in either an exploratory data analysis or in the context of a multivariate linear model, and (ii) search for subsets which are optimal under various criteria.

Generates, plays, and solves Sudoku puzzles.
The GUI playSudoku() needs package "tkrplot" if you are not on Windows.

Methodology for Supervised Grouping of Predictor Variables

Supervised principal components for regression and survival analsysis. Especially useful for high-dimnesional data, including microarray data.

Ten distributions supplementing those built into R. Inverse Gauss,
Kruskal-Wallis, Kendall's Tau, Friedman's chi squared, Spearman's rho, maximum F ratio,
the Pearson product moment correlation coefficiant, Johnson distributions, normal scores
and generalized hypergeometric distributions. In addition two random number generators
of George Marsaglia are included.

The package provides tests for comparing two survival
distributions, testing equality of two cumulative incidence
functions under competing risks and checking goodness of fit of
proportional rate models (proportional hazards, proportional
odds) for two samples.

Fits a proportional hazards model to time to event data by a Bayesian approach.
Right and interval censored data and a lognormal or gamma frailty term can be fitted.

A package implementing statistical methods for the surveillance of infectious diseases. This boils down to modelling and outbreak detection of univariate and multivariate time series of counts resulting from routinely collected public health surveillance data. Currently the package contains an implementation of the typically outbreak detection procedures such as Stroup et. al (1989), Farrington et. al (1996), Rossi et al (1999), Rogerson and Yamada (2001), a Bayesian approach and a detector based on generalized likelihood ratios (GLR). Furthermore, inference methods for the infectious disease model in Held et al (2005) are provided. The package contains several real-world datasets and the ability to simulate outbreak data.

Summary statistics, generalised linear models, and general maximum pseudolikelihood estimation for multistage stratified, cluster-sampled, unequally weighted survey samples. Variances by Taylor series linearisation or replicate weights. Post-stratification, calibration, and raking. Two-phase designs. Graphics.

Complex survey samples -- database interface, sparse matrices.

survival analysis: descriptive statistics, two-sample
tests, parametric accelerated failure models, Cox model. Delayed entry
(truncation) allowed for all models; interval censoring for parametric
models. Case-cohort designs.

Compute time-dependent ROC curve from censored survival
data using Kaplan-Meier (KM) or Nearest Neighbor Estimation
(NNE)  method of  Heagerty, Lumley & Pepe (Biometrics,
Vol 56 No 2, 2000, PP 337-344)

Estimation of survival function for recurrent event
data using Pea-Strawderman-Hollander, Whang-Chang
estimators and MLE estimation under a Gamma Frailty model.

2d and 3d space-varying coefficient models are fitted to regular grid data using either a full B-spline tensor product approach or a sequential approximation. The latter one is computationally more efficient. Resolution increment is enabled.

svcR implements a support vector machine technique for clustering

Computes the entire regularization path for the two-class svm classifier
with essentialy the same cost as a single SVM fit.

Ozone, NOx (= Sum of Nitrogenmonoxide and Nitrogendioxide), Nitrogenmonoxide, ambient temperature, dew point, wind speed and wind direction at 3 sites around lake of Lucerne in Central Switzerland in 30 min time resolution for year 2004.

Implements the synthetic control group method for comparative case studies
developed in Abadie and Gardeazabal (2003) and Abadie, Diamond, Hainmueller (2007).

This package contains functions for fitting simultaneous
systems of linear and nonlinear equations using Ordinary Least
Squares (OLS), Weighted Least Squares (WLS), Seemingly Unrelated
Regressions (SUR), Two-Stage Least Squares (2SLS), Weighted
Two-Stage Least Squares (W2SLS), and Three-Stage Least Squares (3SLS).

A series of widgets (themed controls, tktable, combobox, multi-column list, etc...) and various functions (under Windows: DDE exchange, access to the registry and icon manipulation) to supplement tcltk.

Computes the distribution of a linear combination of independent Student's t-variables (with small degrees of freedom, dff &lt;= 100) and/or standard Normal Z random variables.

TDM can be used to estimate individual pharmacokinetic parameters with one or more drug serum/plasma concentrations obtained from a single subject or multiple subjects using OpenBUGS (Bayesian inference Using Gibbs Sampling) interfaced through BRugs. Besides, it also can calculate a suggested dose with the target drug concentration (C -&gt;D) or calculate a predicted drug concentration with a given dose (D -&gt; C).

Clayton, D. and Jones, H.B. (1999), Transmission/disequilibrium tests for
extended marker haplotypes, Am. J. Hum. Gen., 65:1161-1169.

This package is a set of demonstration functions that can be used in a classroom to demonstrate statistical concepts, or on your own to better understand the concepts or the programming.

The package provides convenience functions for advance linear
algebra with tensors and computation with datasets of tensors
on a higher level abstraction. It includes Einstein and
Riemann summing conventions, dragging, co- and contravariate
indices, parallel computations on sequences of tensors.

The tensor product of two arrays is notionally an outer
product of the arrays collapsed in specific extents by
summing along the appropriate diagonals.

Zero-coupon yield curves and spread curves are important inputs for various financial models, e.g. pricing of securities, risk management, monetary policy issues. Since zero-coupon rates are rarely directly observable, they have to be estimated from market data. The literature broadly distinguishes between parametric and spline-based estimation methods for the zero-coupon yield curve. Our package consists of several widely-used approaches, i.e. the parametric Nelson and Siegel (1987) method with the Svensson (1994) extension, and the McCulloch (1975) cubic splines approach. Extensive summary statistics and plots are provided to compare the results of the different estimation methods.

Functions for writing code that is independent of the way time is represented. See ?tframe.Intro for more details.

Bayesian nonstationary, semiparametric nonlinear regression
and design by treed Gaussian processes with jumps to the limiting
linear model (LLM).  Special cases also implemented include Bayesian
linear models, CART, treed linear models, stationary separable and
isotropic Gaussian processes.  Provides 1-d and 2-d plotting functions
(with projection and slice capabilities) and tree drawing, designed for
visualization of tgp-class output.  Sensitivity analysis and
multi-resolution models are supported, and a limited set of experimental
design and adaptive sampling functions are also provided, including ALM,
ALC, and expected improvement.

Provides a function for drawing a progress bar using standard text output, and additional time-tracking routines that allow developers to track how long their processor-intensive calculations take.

TIMP is a problem solving environment for fitting separable nonlinear models to measurements arising in physics and chemistry experiments, and has been extensively applied to time-resolved spectroscopy and FLIM-FRET data.

Time series analysis and control program package TIMSAC.

GUI to analyze mass spectrometric data on the relative
abundance of two substances from a titration series.

This package provides function to run the TITE-CRM in phase I
trials and the calibration tools to plan a TITE-CRM design.

TK widget tools for rgl package

simple mechanism for placing R graphics in a Tk widget

Functions for two level normal models as described in Everson and Morris (2000). J. R. Statist. Soc. B, 62 prt 2, pp.399--412.

A framework for text mining applications within R.

Regression models for temporal process responses with
time-varying coefficient.

Stores objects in files on disk so that files are
automatically rewritten when objects are changed, and so
that objects are accessible but do not occupy memory until
they are accessed. Also tracks times when objects are created
and modified, and caches some basic characteristics of objects
to allow for fast summaries of objects.

Performs ex-post analysis of security transaction costs.

TRAMPR is an R package for matching terminal restriction
fragment length polymorphism (TRFLP) profiles between unknown
samples and a database of knowns.  TRAMPR facilitates analysis
of many unknown profiles at once, and provides tools for
working directly with electrophoresis output through to
generating summaries suitable for community analyses with R's
rich set of statistical functions.  TRAMPR also resolves the
issues of multiple TRFLP profiles within a species, and shared
TRFLP profiles across species.

Stem analysis functions for volume increment and carbon uptake assessment from tree-rings.

Classification and Regression Trees.

Provides the "r, q, p, and d" distribution functions for the triangle distribution

Trimmed k-means clustering.

A constrained two-dimensional Delaunay triangulation package

Data handling and estimation functions for animal movement estimation by light level and satellite data. Image summaries from MCMC simulations of point data, binned by time interval. Various convenience functions for creating generic summary images for periods defined by the location estimation and combining those in arbitrarily specified ways.

Functions for accessing and manipulating spatial data for animal tracking.
Filter for speed and create time spent plots from animal track data.

Goodness-of-fit tests and some adjusted exploratory tools allowing for left truncated data

local optimization using two derivatives and trust regions

Contains R functions and datasets detailed in the book
"Time Series Analysis with Applications in R (second edition)" by Jonathan Cryer and Kung-Sik Chan

TSdbi provides an common interface to time series databases.

Time series analysis based on dynamical systems theory

Routines for the analysis of nonlinear time series.
This work is largely inspired by the TISEAN project, by Rainer Hegger, Holger Kantz and Thomas Schreiber: http://www.mpipks-dresden.mpg.de/~tisean/

Package for time series analysis and computational finance

Extraction of Factors from Multivariate Time Series. See ?00tsfa-Intro for more details.

two-stage procedure for comparing hazard rate functions
which may or may not cross each other

TSMySQL provides a MySQL interface for TSdbi.

Provides methods for generics in the TSdbi package to
connect through a protocol for application database interface (PADI)
to a time series database (e.g. Fame).

Basic infrastructure and some algorithms for the traveling
salesperson problem (TSP). The package provides some simple algorithms and
an interface to Concorde, the currently fastest TSP solver. Concorde
itself is not included in the package and has to be obtained separately.

TSSQLite provides a SQLite interface for TSdbi.

Functions and data to construct technical trading rules with R.

Collection of tools to analyze music, handling wave files, transcription, ...

Package to mask common functions so that inputs in error are
explained and able to be corrected, prior to execution.
'assist' offers step-by-step assistance to correctly call
a function such as 'par'.  'eg' picks out Examples first and
foremost in help.  With 'deskcheck', execution is initiated
and the 'debug' flag utilized, to help deskcheck a function.

This package offers functions for propensity score estimating and
weighting, nonresponse weighting, and diagnosis of the weights

Maximum likelihood computations for Tweedie families.

Trees with extra splits

The package offers a fitting of smooth varying coefficients in an additiv two-way hazard model.
A one-way hazard model can also be fittied with supplementary functions. Nonparametric penalized spline (p-spline) fitting is proposed.
In the two-way case two alternativ models can be analized: In the first one a non periodic calendar time, like the calendar year, is considered
as a second time scale (additionaly to the survival time), and as a spline basis truncated polynomial functions are chosen.
In the second model a periodic time scale, like a season of year, is additionaly considered, and as a spline basis the b-splines
are selected. In the one-way case the user can choose between these two alternative penalized bases.

twslm is for normalization of cDNA microarray data with
the two-way semilinear model. Huber's and Tukey's bisquare
weight functions are available for robust estimation of the two-way
semilinear models.

TwslmSpikeWeight is for normalization of cDNA microarray data with
the two-way semilinear model(TW-SLM). It incorporates information from
control spots and data quality in the TW-SLM to improve normalization of
cDNA microarray data. Huber's and Tukey's bisquare
weight functions are available for robust estimation of the TW-SLM.

This package provides an R interface to the Unidata
udunits library routines, which can convert quantities between various units.
Units are indicated by human-readable strings, such as "m/s", "J",
"kg", or "in".  Routines for converting any quantity in known units
to other compatible units are provided.  Of particular use are the
time and calendar conversion routines.  Calendar dates are given
with units such as "days since 1900-01-01", for example.  Values with
this unit can be converted to normal, readable calendar dates.  This
will let you find that "32018 days since 1900-01-01" is actually
31 Aug 1987.  These routines follow the library's C interface, so consult that
section of Unidata's udunits manual for reference.  Here are some
example formatted units strings that can be used: "10 kilogram.meters/seconds2",
"10 kg-m/sec2", "10 kg m/s^2", "(PI radian)2", "degF", "degC", "100rpm",
"geopotential meters", "33 feet water".  Note that the udunits library
must already be installed on your machine for this package to work.
Change log: v1.3: added support for non-standard calendars in routine
'utCalendar'.

Universal Markov Chain Sampler

see title

The package implements an algorithm for nonparametric function estimation using Unbalanced Haar wavelets.

Computes a univeral numeric fingerprint of the data.

A collection of utilities for biodiversity data.
Includes the simulation of ecological drift under Hubbell's Unified
Neutral Theory of Biodiversity, and the calculation of various
diagnostics such as Preston curves.

Unit root and cointegration tests encountered in applied
econometric analysis are implemented.

Functions for sampling without replacement. (Simulated Urns).

This package contains several functions for analysing quarterly and monthly time series.
Unit root test: ADF, KPSS, HEGY, and CH, as well as graphics: Buys-Ballot and
seasonal cycles, among others, have been implemented to accomplish either an analytical or a
graphical analysis. Combined use of both enables the user to characterize the seasonality as
deterministic, stochastic or a mixture of them. An easy to use graphical user interface is also
provided to run some of the implemented functions.

A collection of datasets to accompany the
textbook "Using R for Introductory Statistics."

Identify and display the distribution of Local Treatment Differences (LTDs)and Local Average Treatment Effects (LATEs) on Outcomes within Clusters of patients chosen so as to be relatively well matched on baseline X-covariates.

Performs inference of a gaussian mixture model within a bayesian
framework using an optimal separable approximation to the posterior density.
The optimal posterior approximation is obtained using a variational approach.

This package allows interactive variogram diagnostics.

A set of methods for calculation of Value at Risk (VaR)

This package performs mixture model on the variance for the analysis of gene expression data.

Variable selection from random forests
using both backwards variable elimination (for the selection of
small sets of non-redundant variables) and selection based on the
importance spectrum (somewhat similar to scree plots; for the
selection of large, potentially highly-correlated variables).
Main applications in high-dimensional data
(e.g., microarray data, and other genomics and proteomics
applications). You can use rpvm instead of Rmpi if you want
but I've only tested with Rmpi.

Estimation, lag selection, diagnostic testing, forecasting, causality analysis, forecast error variance decomposition and impulse response functions of VAR models and estimation of SVAR/SVEC models.

Variational Bayesian Multinomial Probit Regression with Gaussian
Process Priors. (Neural Computation 18, 1790-1817 (2006))

Visualization techniques, data sets, summary and inference
procedures aimed particularly at categorical data. Special
emphasis is given to highly extensible grid graphics. The
package was inspired by the book "Visualizing Categorical Data"
by Michael Friendly.

The VDC system is an open source digital library system for
quantitative data. This is a package to support on-line analysis using
VDC and Zelig, accuracy, and R2HTML

Ordination methods, diversity analysis and other
functions for community and vegetation ecologists.

This package contains utilities for verification of discrete,continuous,  probabilistic forecasts and forecast expressed as parametric distributions.

This small library contains simple tools for
constructing and manipulating objects of class verify.
These are used when regression-testing R software.

Vector generalized linear and additive models, and
associated models (Reduced-Rank VGLMs, Quadratic RR-VGLMs,
Reduced-Rank VGAMs). This package fits many models and
distribution by maximum likelihood estimation (MLE) or
penalized MLE. Also fits constrained ordination models in
ecology.

This package is a new tool for the visualization of missing values. The tool can be used for exploring the data and the structure of the missing values. Depending on this structure, the tool can be helpful for identifying the mechanism generating the missings. A graphical user interface allows an easy handling of the implemented plot methods.

A violin plot is a combination of a box plot and a kernel density plot.

Functions, Classes & Methods for estimation, prediction, and
simulation (bootstrap) of VLMC -- Variable Length Markov Chain -- Models

Functions and datasets to support Venables and
Ripley, 'Modern Applied Statistics with S' (4th edition).

A collection of variance ratio tests

Makes available code necessary to reproduce figures and tables in recent papers on the WaveD method for wavelet deconvolution of noisy signals as presented in The WaveD Transform in R, Journal of Statistical Software Volume 21, No. 3, 2007.

This package contains functions for computing and
plotting discrete wavelet transforms (DWT) and maximal
overlap discrete wavelet transforms (MODWT), as well as
their inverses. Additionally, it contains functionality
for computing and plotting wavelet transform filters that
are used in the above decompositions as well as
multiresolution analyses.

Basic wavelet routines for time series (1D), image (2D)
and array (3D) analysis.  The code provided here is based on
wavelet methodology developed in Percival and Walden (2000);
Gencay, Selcuk and Whitcher (2001); the dual-tree complex wavelet
transform (CWT) from Kingsbury (1999, 2001) as implemented by
Selesnick; and Hilbert wavelet pairs (Selesnick 2001, 2002).  All
figures in chapters 4-7 of GSW (2001) are reproducible using this
package and R code available at the book website(s) below.

Software to perform 1-d and 2-d wavelet statistics and transforms

SOM networks for comparing patterns with peak shifts.

Compute spatial predictions from exact count and a covariate

Inferences about counterfactuals are essential for prediction,
answering what if questions, and estimating causal effects.
However, when the counterfactuals posed are too far from the data at
hand, conclusions drawn from well-specified statistical analyses
become based largely on speculation hidden in convenient modeling
assumptions that few would be willing to defend.  Unfortunately,
standard statistical approaches assume the veracity of the model
rather than revealing the degree of model-dependence, which makes this
problem hard to detect.  WhatIf offers easy-to-apply methods to
evaluate counterfactuals that do not require sensitivity testing over
specified classes of models.  If an analysis fails the tests offered
here, then we know that substantive inferences will be sensitive to at
least some modeling choices that are not based on empirical evidence,
no matter what method of inference one chooses to use.  WhatIf
implements the methods for evaluating counterfactuals discussed in
Gary King and Langche Zeng, 2006, "The Dangers of Extreme
Counterfactuals," Political Analysis 14 (2); and Gary
King and Langche Zeng, 2007, "When Can History Be Our Guide?  The
Pitfalls of Counterfactual Inference," International Studies
Quarterly 51 (March).

The german Wikibook "GNU R" introduces R to new users.
This package is a collection of functions and datas used in the german WikiBook "GNU R"

This package provides functions to perform fast variable selection based on the Wilcoxon rank sum test in the  cross-validation or Monte-Carlo cross-validation settings, for use in microarray-based binary classification.

Approach to the robustness via Weighted Likelihood.

Software to book Wavelet Methods for Time Series Analysis, Donald B. Percival and Andrew T. Walden, Cambridge University Press, 2000.

Estimates Poole and Rosenthal W-NOMINATE scores from roll call votes supplied though a 'rollcall' object from package 'pscl'.

Analyses individually geo-referenced multilocus genotypes for the inferences of genetic boundaries between populations. It is based on the wombling method that estimates the systemic function by looking for the local variation of the allele frequencies. The systemic function estimation is based on the local polynomial regression, and a binomial test assess the significance of boundaries. The method applies to codominant or dominant markers and allows for missing data.

An interface to WordNet using the Jawbone Java API to WordNet.

Function for writing a SNNS pattern file from a data.frame or matrix.

Book is "Linear Mixed Models: A Practical Guide Using Statistical Software" published in 2006 by Chapman Hall / CRC Press

Interface to the XGobi and XGvis programs for graphical
data analysis.

Read and write Excelfiles natively (v97-2003/BIFF8).

Parsing facilities for arbitrary XML files, DTDs and HTML, offering both
event and tree based parsers which are customizable with R functions.

This is a package that implements extreme regression
estimation as described in LeBlanc, Moon and Kooperberg,
Biostatistics, 7, 71-84, 2006.

Coerce data to LaTeX and HTML tables

Provide for uniform handling of R's different time-based data classes by extending zoo, maximizing native format information preservation and allowing for user level customization and extension, while simplifying cross-class interoperability.

Performs popular nearest neighbor routines for imputation

This collection of data exploration tools was developed at Yale University for the graphical exploration of complex multivariate data. Functions provided include \code{barcode()}, \code{gpairs()}, \code{corrgram()}, \code{whatis()}, \code{sparkline()}, \code{sparklines()}, and \code{sparkmat()}.

Model selection and variance estimation in Gaussian independence models

Zelig is an easy-to-use program that can estimate, and
help interpret the results of, an enormous range of
statistical models. It literally is "everyone's statistical
software" because Zelig's simple unified framework
incorporates everyone else's (R) code. We also hope it will
become "everyone's statistical software" for applications
and teaching, and so have designed Zelig so that anyone can
easily use it or add their programs to it.  Zelig also comes
with infrastructure that facilitates the use of any existing
method, such as by allowing multiply imputed data for any
model, and mimicking the program Clarify (for Stata) that
takes the raw output of existing statistical procedures and
translates them into quantities of direct interest.

This package fits  classical and zero-inflated count
data regression model as well as censored count data regression

Adapt and analyze GP, ZIP and ZIGP regression models

Statistical models and utilities for the analysis of word frequency distributions.
The utilities include functions for loading, manipulating and visualizing word frequency
data and vocabulary growth curves.  The package also implements several statistical
models for the distribution of word frequencies in a population.  (The name of this library
derives from the most famous word frequency distribution, Zipf's law.)

Calculate and plot scattering matrix coefficients for plane waves at interface.

An S3 class with methods for totally ordered indexed
observations. It is particularly aimed at irregular
time series of numeric vectors/matrices and factors.
zoo's key design goals are independence of a particular
index/date/time class and consistency with with ts and
base R by providing methods to extend standard generics.

